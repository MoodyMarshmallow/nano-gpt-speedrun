{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Stage 2 LR Sweep (baseline vs elementwise gate)\n", "\n", "Sweeps learning rate multipliers for baseline and elementwise gating (sigmoid, SDPA output) per Stage 2 of the plan. Uses 150 warmup steps and 1,000 total iterations. Runs 2 seeds per LR (default) and stops early if divergence is detected (NaN/inf or loss above a threshold).\n", "\n", "**Important**: This notebook launches `torchrun` subprocesses. Adjust `gpus_per_run` and `cuda_visible_devices` to match your node. Ensure data is at `data/fineweb10B/` (or set a symlink)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%cd /workspace/ese-3060-project\n", "\n", "# Config\n", "base_lr = 0.0036\n", "lr_multipliers = [1.0, 1.1, 1.2, 1.3]\n", "seeds = [1337, 2337]\n", "warmup_iters = 150\n", "num_iterations = 1000\n", "early_stop_patience = 2      # 0 disables; patience counted on val checks\n", "early_stop_min_delta = 0.0    # require this much improvement to reset patience\n", "\n", "gpus_per_run = 8            # set to your available GPU count (e.g., 1 or 8)\n", "cuda_visible_devices = None # e.g., \"0,1,2,3\" or leave None\n", "\n", "script_path = \"/workspace/ese-3060-project/train_gpt.py\"  # absolute path to avoid cwd issues\n", "torchrun_cmd = \"torchrun\"\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, subprocess, shlex, time, re, json, pathlib, signal\n", "\n", "def build_env(attn_gate, lr, seed):\n", "    env = os.environ.copy()\n", "    env.update({\n", "        \"ATTNGATE\": attn_gate,\n", "        \"GATEPOS\": \"sdpa\",\n", "        \"GATEACT\": \"sigmoid\",\n", "        \"LR\": str(lr),\n", "        \"SEED\": str(seed),\n", "        \"WARMUP_ITERS\": str(warmup_iters),\n", "        \"NUM_ITERATIONS\": str(num_iterations),\n", "        \"EARLY_STOP_PATIENCE\": str(early_stop_patience),\n", "        \"EARLY_STOP_MIN_DELTA\": str(early_stop_min_delta),\n", "    })\n", "    if cuda_visible_devices is not None:\n", "        env[\"CUDA_VISIBLE_DEVICES\"] = str(cuda_visible_devices)\n", "    return env\n", "\n", "def run_experiment(name, attn_gate, lr, seed):\n", "    cmd = [\n", "        torchrun_cmd,\n", "        \"--standalone\",\n", "        f\"--nproc_per_node={gpus_per_run}\",\n", "        script_path,\n", "    ]\n", "    env = build_env(attn_gate, lr, seed)\n", "    print(f\"\n>>> Launching {name}\n    attn_gate={attn_gate} lr={lr:.6f} seed={seed} nproc={gpus_per_run}\")\n", "    proc = subprocess.Popen(\n", "        cmd,\n", "        stdout=subprocess.PIPE,\n", "        stderr=subprocess.STDOUT,\n", "        text=True,\n", "        env=env,\n", "        preexec_fn=os.setsid,\n", "        bufsize=1,\n", "    )\n", "    lines = []\n", "    for line in proc.stdout:\n", "        line = line.rstrip()\n", "        print(line)\n", "        lines.append(line)\n", "    ret = proc.wait()\n", "    tail = lines[-10:]\n", "    return {\"name\": name, \"attn_gate\": attn_gate, \"lr\": lr, \"seed\": seed, \"returncode\": ret, \"tail\": tail}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build experiment list: baseline (none) and elementwise\n", "experiments = []\n", "for attn_gate in [\"none\", \"elementwise\"]:\n", "    for mult in lr_multipliers:\n", "        lr = base_lr * mult\n", "        for seed in seeds:\n", "            name = f\"{attn_gate}-lr{mult:.2f}-seed{seed}\"\n", "            experiments.append({\n", "                \"name\": name,\n", "                \"attn_gate\": attn_gate,\n", "                \"lr\": lr,\n", "                \"seed\": seed,\n", "            })\n", "print(f\"Prepared {len(experiments)} experiments:\")\n", "for exp in experiments:\n", "    print(\" -\", exp[\"name\"])\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run (commented out by default; remove the guard to execute)\n", "results = []\n", "if False:  # set to True to launch runs (expensive!)\n", "    for exp in experiments:\n", "        res = run_experiment(**exp)\n", "        results.append(res)\n", "    print(json.dumps(results, indent=2))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot averaged val loss curves per configuration (baseline vs elementwise), LR variants on same graph\n", "import glob\n", "import matplotlib.pyplot as plt\n", "\n", "log_dir = \"logs\"\n", "\n", "def parse_hparams(lines):\n", "    try:\n", "        start = lines.index(\"hyperparameters:\\n\")\n", "    except ValueError:\n", "        return None\n", "    blob = []\n", "    for ln in lines[start+1:]:\n", "        if ln.startswith(\"Running pytorch\"):\n", "            break\n", "        blob.append(ln)\n", "    try:\n", "        return json.loads(\"\".join(blob))\n", "    except Exception:\n", "        return None\n", "\n", "val_re = re.compile(r\"step:(\\d+)/(\\d+).*val_loss:([\\d\\.eE+-]+)\")\n", "\n", "def parse_log(path):\n", "    with open(path) as f:\n", "        lines = f.readlines()\n", "    hparams = parse_hparams(lines)\n", "    vals = []\n", "    for ln in lines:\n", "        m = val_re.search(ln)\n", "        if m:\n", "            step = int(m.group(1))\n", "            loss = float(m.group(3))\n", "            vals.append((step, loss))\n", "    return hparams, vals\n", "\n", "# collect logs matching our Stage 2 sweep filters\n", "grouped = {}\n", "for path in glob.glob(os.path.join(log_dir, \"*.txt\")):\n", "    hparams, vals = parse_log(path)\n", "    if not hparams or not vals:\n", "        continue\n", "    if hparams.get(\"gate_pos\") != \"sdpa\" or hparams.get(\"gate_act\") != \"sigmoid\":\n", "        continue\n", "    if hparams.get(\"warmup_iters\") != warmup_iters or hparams.get(\"num_iterations\") != num_iterations:\n", "        continue\n", "    if hparams.get(\"attn_gate\") not in (\"none\", \"elementwise\"):\n", "        continue\n", "    key = (hparams[\"attn_gate\"], float(hparams[\"learning_rate\"]))\n", "    grouped.setdefault(key, []).append(vals)\n", "\n", "if not grouped:\n", "    print(\"No matching logs found. Ensure runs completed and warmup/num_iterations match notebook settings.\")\n", "else:\n", "    attn_groups = sorted(set(k[0] for k in grouped.keys()))\n", "    for attn in attn_groups:\n", "        plt.figure(figsize=(8,5))\n", "        lrs = sorted(k[1] for k in grouped.keys() if k[0] == attn)\n", "        for lr in lrs:\n", "            runs = grouped[(attn, lr)]\n", "            # average per step across seeds\n", "            agg = {}\n", "            for run in runs:\n", "                for step, loss in run:\n", "                    agg.setdefault(step, []).append(loss)\n", "            steps = sorted(agg.keys())\n", "            mean_loss = [sum(agg[s])/len(agg[s]) for s in steps]\n", "            plt.plot(steps, mean_loss, label=f\"lr={lr:.5f} (n={len(runs)})\")\n", "        plt.title(f\"Val loss vs step \u2014 attn_gate={attn}\")\n", "        plt.xlabel(\"step\")\n", "        plt.ylabel(\"val_loss\")\n", "        plt.legend()\n", "        plt.grid(True, alpha=0.3)\n", "    plt.show()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}