{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Runs + EDA (Gating Screening)\n",
    "\n",
    "This notebook lets you:\n",
    "1. Launch the short Stage 1 screening runs (gating variants) with `torchrun`.\n",
    "2. Parse `experiments/results.csv` and `logs/*.txt`.\n",
    "3. Plot loss/time and step-time summaries.\n",
    "\n",
    "> **Safety:** Runs start only if you set `DRY_RUN = False` in the launch cell. Default is safe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0a) (Once per machine) Register and verify venv kernel inside the notebook\n",
    "If JupyterLab is already running but not using the venv, run the next cell to install the kernel spec, then switch to it via Kernel → Change Kernel → `ese3060 venv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /workspace/ese-3060-project/.venv/bin/activate\n",
    "python -m ipykernel install --user --name ese3060-venv --display-name \"ese3060 venv\"\n",
    "echo \"Kernel installed. Switch via Kernel -> Change Kernel -> 'ese3060 venv'\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "- Assumes repo at `/workspace/ese-3060-project` and data symlinked at `data/fineweb10B`.\n",
    "- Activate venv before starting JupyterLab: `source .venv/bin/activate`.\n",
    "- Change `PROJECT_ROOT` below if different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, shlex, json, glob, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "PROJECT_ROOT = os.environ.get(\"PROJ_ROOT\", \"/workspace/ese-3060-project\")\n",
    "SCRIPT = os.path.join(PROJECT_ROOT, \"train_gpt.py\")\n",
    "RESULTS = os.path.join(PROJECT_ROOT, \"experiments\", \"results.csv\")\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, \"logs\")\n",
    "PROJECT_ROOT, SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Launch runs (Stage 1 screening)\n",
    "Configs: baseline, headwise sigmoid, elementwise sigmoid, headwise ns_sigmoid, const sigmoid (all at SDPA position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime knobs\n",
    "NPROC = None           # set to int to override; otherwise auto-detect GPU count\n",
    "NUM_ITER = 1500        # screening iterations\n",
    "VAL_EVERY = 125        # eval cadence\n",
    "SEEDS = [1337, 1338]   # two seeds per config\n",
    "DRY_RUN = True         # flip to False to actually launch\n",
    "\n",
    "CONFIGS = {\n",
    "    \"baseline\":  {\"ATTNGATE\": \"none\",      \"GATEPOS\": \"sdpa\", \"GATEACT\": \"sigmoid\"},\n",
    "    \"head_sig\":  {\"ATTNGATE\": \"headwise\",  \"GATEPOS\": \"sdpa\", \"GATEACT\": \"sigmoid\"},\n",
    "    \"elem_sig\":  {\"ATTNGATE\": \"elementwise\",\"GATEPOS\": \"sdpa\", \"GATEACT\": \"sigmoid\"},\n",
    "    \"head_ns\":   {\"ATTNGATE\": \"headwise\",  \"GATEPOS\": \"sdpa\", \"GATEACT\": \"ns_sigmoid\"},\n",
    "    \"const_sig\": {\"ATTNGATE\": \"const\",     \"GATEPOS\": \"sdpa\", \"GATEACT\": \"sigmoid\"},\n",
    "}\n",
    "\n",
    "def detect_gpu_count():\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.cuda.device_count()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        out = subprocess.check_output([\"nvidia-smi\", \"--list-gpus\"], text=True)\n",
    "        return len([l for l in out.splitlines() if l.strip()])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "gpu_count = detect_gpu_count()\n",
    "effective_nproc = NPROC if NPROC is not None else gpu_count\n",
    "print(f\"Detected GPUs: {gpu_count}; using NPROC={effective_nproc}\")\n",
    "if effective_nproc is None or effective_nproc < 1:\n",
    "    raise SystemExit(\"No GPUs detected; set NPROC manually if using a custom setup.\")\n",
    "if effective_nproc > gpu_count:\n",
    "    raise SystemExit(f\"Requested NPROC={effective_nproc} exceeds visible GPUs={gpu_count}.\")\n",
    "\n",
    "def run_config(cfg_name, cfg_env, seed, nproc, num_iter, val_every, dry_run=True):\n",
    "    env = os.environ.copy()\n",
    "    env.update({\n",
    "        \"SEED\": str(seed),\n",
    "        \"NUM_ITER\": str(num_iter),\n",
    "        \"VAL_EVERY\": str(val_every),\n",
    "    })\n",
    "    env.update(cfg_env)\n",
    "    cmd = [\"torchrun\", \"--standalone\", f\"--nproc_per_node={nproc}\", SCRIPT]\n",
    "    print(f\"[{datetime.utcnow().isoformat()}Z] {cfg_name} seed={seed}\")\n",
    "    print(\"    env overrides:\", json.dumps({k: env[k] for k in ['ATTNGATE','GATEPOS','GATEACT','SEED','NUM_ITER','VAL_EVERY']}, indent=2))\n",
    "    if dry_run:\n",
    "        return\n",
    "    result = subprocess.run(cmd, env=env)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Run failed: {cfg_name}, seed {seed}, rc={result.returncode}\")\n",
    "\n",
    "for cfg_name, cfg_env in CONFIGS.items():\n",
    "    for seed in SEEDS:\n",
    "        run_config(cfg_name, cfg_env, seed, effective_nproc, NUM_ITER, VAL_EVERY, dry_run=DRY_RUN)\n",
    "\n",
    "print(\"Done (DRY_RUN=\" + str(DRY_RUN) + \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load results and logs\n",
    "Run after some jobs complete. If `experiments/results.csv` doesn’t exist yet, create an empty DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(RESULTS):\n",
    "    df = pd.read_csv(RESULTS)\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "df.head() if not df.empty else df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to Stage 1 runs (short num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1 = df[df.get(\"num_iterations\", pd.Series([0]*len(df))) <= 2000].copy()\n",
    "stage1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate metrics by config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stage1.empty:\n",
    "    group_cols = [\"attn_gate\", \"gate_act\", \"gate_pos\"]\n",
    "    agg = stage1.groupby(group_cols).agg(\n",
    "        runs=(\"run_id\", \"count\"),\n",
    "        mean_final_val=(\"final_val_loss\", \"mean\"),\n",
    "        std_final_val=(\"final_val_loss\", \"std\"),\n",
    "        mean_best_val=(\"best_val_loss\", \"mean\"),\n",
    "        mean_ms_step=(\"ms_per_step\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg\n",
    "else:\n",
    "    agg = pd.DataFrame()\n",
    "    agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots: best val loss and step time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not agg.empty:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.bar(agg.index, agg[\"mean_best_val\"], yerr=agg[\"std_final_val\"], capsize=4)\n",
    "    ax.set_xticks(agg.index)\n",
    "    ax.set_xticklabels(agg[[\"attn_gate\",\"gate_act\"]].agg(' / '.join, axis=1), rotation=30, ha='right')\n",
    "    ax.set_ylabel(\"Best val loss (mean ± sd)\")\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No data to plot yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not agg.empty:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.bar(agg.index, agg[\"mean_ms_step\"])\n",
    "    ax.set_xticks(agg.index)\n",
    "    ax.set_xticklabels(agg[[\"attn_gate\",\"gate_act\"]].agg(' / '.join, axis=1), rotation=30, ha='right')\n",
    "    ax.set_ylabel(\"ms/step (mean)\")\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No data to plot yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse val_loss curves from logs and plot val_loss vs train_time\n",
    "Uses the first run per config for a quick visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_RE = re.compile(r\"step:(\\d+)/(\\d+) val_loss:([0-9.]+) train_time:(\\d+)ms\")\n",
    "\n",
    "def parse_log(path):\n",
    "    rows = []\n",
    "    for line in open(path):\n",
    "        m = VAL_RE.search(line)\n",
    "        if m:\n",
    "            step = int(m.group(1)); total = int(m.group(2))\n",
    "            vloss = float(m.group(3)); t_ms = int(m.group(4))\n",
    "            rows.append((step, total, vloss, t_ms))\n",
    "    return rows\n",
    "\n",
    "log_rows = []\n",
    "for path in glob.glob(os.path.join(LOG_DIR, \"*.txt\")):\n",
    "    rid = os.path.basename(path).replace('.txt','')\n",
    "    rows = parse_log(path)\n",
    "    for step, total, vloss, t_ms in rows:\n",
    "        log_rows.append({\"run_id\": rid, \"step\": step, \"total\": total, \"val_loss\": vloss, \"train_time_ms\": t_ms})\n",
    "\n",
    "log_df = pd.DataFrame(log_rows)\n",
    "log_df.head() if not log_df.empty else log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not log_df.empty and not df.empty:\n",
    "    curves = log_df.merge(df[[\"run_id\",\"attn_gate\",\"gate_act\",\"gate_pos\"]], on=\"run_id\", how=\"left\")\n",
    "    # pick first run per config for plotting\n",
    "    first_runs = curves.groupby([\"attn_gate\",\"gate_act\",\"gate_pos\"])['run_id'].transform('min') == curves['run_id']\n",
    "    plot_df = curves[first_runs]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    for (gate, act, pos), sub in plot_df.groupby([\"attn_gate\",\"gate_act\",\"gate_pos\"]):\n",
    "        ax.plot(sub[\"train_time_ms\"]/1000.0, sub[\"val_loss\"], label=f\"{gate}/{act}\")\n",
    "    ax.set_xlabel(\"Train time (s)\")\n",
    "    ax.set_ylabel(\"Val loss\")\n",
    "    ax.set_title(\"Val loss vs train time (first run per config)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No curves to plot yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
