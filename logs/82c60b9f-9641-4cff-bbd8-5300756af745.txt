====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import json
import dataclasses
import subprocess
import csv
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

def _apply_gate_act(logits: torch.Tensor, kind: str) -> torch.Tensor:
    if kind == "sigmoid":
        return torch.sigmoid(logits)
    if kind == "ns_sigmoid":
        return 0.5 + 0.5 * torch.sigmoid(logits)
    raise ValueError(f"unknown gate_act={kind}")

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.attn_gate = getattr(config, "attn_gate", "none")
        self.gate_pos = getattr(config, "gate_pos", "sdpa")
        self.gate_act = getattr(config, "gate_act", "sigmoid")
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.attn_gate == "headwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_head, bias=False)
            self.gate_param = None
        elif self.attn_gate == "elementwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_embd, bias=False)
            self.gate_param = None
        elif self.attn_gate == "const":
            self.c_gate = None
            self.gate_param = nn.Parameter(torch.zeros(self.n_head, self.head_dim))
        else:
            self.c_gate = None
            self.gate_param = None

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if self.attn_gate != "none" and self.gate_pos == "value":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            v = v * gate
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2) # (B, T, n_head, head_dim)
        if self.attn_gate != "none" and self.gate_pos == "sdpa":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            y = y * gate
        y = y.contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attn_gate : str = "none"
    gate_pos : str = "sdpa"
    gate_act : str = "sigmoid"

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    seed : int = 1337
    attn_gate : str = "none" # none|headwise|elementwise|const
    gate_pos : str = "sdpa" # sdpa|value
    gate_act : str = "sigmoid" # sigmoid|ns_sigmoid
    early_stop_patience : int = 0 # 0 disables early stopping
    early_stop_min_delta : float = 0.0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

def apply_env_overrides():
    # environment-variable overrides allow quick sweeps without editing code
    args.learning_rate = float(os.environ.get("LR", args.learning_rate))
    args.seed = int(os.environ.get("SEED", args.seed))
    args.warmup_iters = int(os.environ.get("WARMUP_ITERS", args.warmup_iters))
    args.num_iterations = int(os.environ.get("NUM_ITERATIONS", args.num_iterations))
    args.early_stop_patience = int(os.environ.get("EARLY_STOP_PATIENCE", args.early_stop_patience))
    args.early_stop_min_delta = float(os.environ.get("EARLY_STOP_MIN_DELTA", args.early_stop_min_delta))
    args.attn_gate = os.environ.get("ATTNGATE", args.attn_gate)
    args.gate_pos = os.environ.get("GATEPOS", args.gate_pos)
    args.gate_act = os.environ.get("GATEACT", args.gate_act)
    args.num_iterations = int(os.environ.get("NUM_ITER", args.num_iterations))
    args.val_loss_every = int(os.environ.get("VAL_EVERY", args.val_loss_every))

def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

apply_env_overrides()
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
np.random.seed(args.seed)

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
git_commit = get_git_commit() if master_process else "unknown"

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    attn_gate=args.attn_gate,
    gate_pos=args.gate_pos,
    gate_act=args.gate_act,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        f.write(f"git_commit: {git_commit}\n")
        f.write(f"seed: {args.seed}\n")
        f.write("hyperparameters:\n")
        f.write(json.dumps(dataclasses.asdict(args), indent=2))
        f.write("\n")
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
best_val_loss = float("inf")
final_val_loss = None
no_improve_count = 0
early_stop_reason = None
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        if not torch.isfinite(val_loss):
            early_stop_reason = "non-finite val_loss"
            final_val_loss = float("nan")
            val_loss_item = float("nan")
        else:
            val_loss_item = val_loss.item()
            final_val_loss = val_loss_item
            if val_loss_item < best_val_loss - args.early_stop_min_delta:
                best_val_loss = val_loss_item
                no_improve_count = 0
            else:
                no_improve_count += 1
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if early_stop_reason is None and args.early_stop_patience > 0 and no_improve_count >= args.early_stop_patience:
                early_stop_reason = f"early_stop patience={args.early_stop_patience} min_delta={args.early_stop_min_delta}"
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process in a subdir to keep logs tidy
        ckpt_dir = os.path.join(logdir, "checkpoints")
        os.makedirs(ckpt_dir, exist_ok=True)
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, os.path.join(ckpt_dir, f"state_step{step:06d}.pt"))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")
    if early_stop_reason is not None:
        break

if master_process:
    if early_stop_reason is not None:
        print(f"stopped early: {early_stop_reason}")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    timed_steps_final = max(args.num_iterations - 9, 1)
    ms_per_step = training_time_ms / timed_steps_final
    os.makedirs("experiments", exist_ok=True)
    results_path = os.path.join("experiments", "results.csv")
    fieldnames = [
        "run_id",
        "date",
        "git_commit",
        "seed",
        "attn_gate",
        "gate_pos",
        "gate_act",
        "learning_rate",
        "batch_size",
        "device_batch_size",
        "sequence_length",
        "num_iterations",
        "warmdown_iters",
        "final_val_loss",
        "best_val_loss",
        "train_time_ms",
        "ms_per_step",
        "gpu_name",
        "n_gpus",
        "runpod_instance",
        "notes",
    ]
    final_loss_value = final_val_loss if final_val_loss is not None else float("nan")
    best_loss_value = best_val_loss if best_val_loss < float("inf") else float("nan")
    row = {
        "run_id": run_id,
        "date": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        "git_commit": git_commit,
        "seed": args.seed,
        "attn_gate": args.attn_gate,
        "gate_pos": args.gate_pos,
        "gate_act": args.gate_act,
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "device_batch_size": args.device_batch_size,
        "sequence_length": args.sequence_length,
        "num_iterations": args.num_iterations,
        "warmdown_iters": args.warmdown_iters,
        "final_val_loss": final_loss_value,
        "best_val_loss": best_loss_value,
        "train_time_ms": training_time_ms,
        "ms_per_step": ms_per_step,
        "gpu_name": torch.cuda.get_device_name(ddp_local_rank),
        "n_gpus": ddp_world_size,
        "runpod_instance": os.environ.get("RUNPOD_INSTANCE_TYPE", "unknown"),
        "notes": "",
    }
    write_header = not os.path.exists(results_path)
    with open(results_path, "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerow(row)
====================================================================================================
git_commit: bf6d1ff4b5e4ce7280e822ab33e90c4fb250e604
seed: 2337
hyperparameters:
{
  "input_bin": "data/fineweb10B/fineweb_train_*.bin",
  "input_val_bin": "data/fineweb10B/fineweb_val_*.bin",
  "batch_size": 512,
  "device_batch_size": 64,
  "sequence_length": 1024,
  "num_iterations": 800,
  "learning_rate": 0.00396,
  "warmup_iters": 150,
  "warmdown_iters": 1450,
  "weight_decay": 0,
  "seed": 2337,
  "attn_gate": "none",
  "gate_pos": "sdpa",
  "gate_act": "sigmoid",
  "early_stop_patience": 2,
  "early_stop_min_delta": 0.0,
  "val_loss_every": 125,
  "val_tokens": 10485760,
  "save_every": 0
}
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Sun Dec  7 16:58:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:00:07.0 Off |                    0 |
| N/A   46C    P0            115W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:00:08.0 Off |                    0 |
| N/A   48C    P0            118W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:00:09.0 Off |                    0 |
| N/A   43C    P0            108W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000000:00:0A.0 Off |                    0 |
| N/A   45C    P0            128W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  |   00000000:00:0B.0 Off |                    0 |
| N/A   48C    P0            115W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  |   00000000:00:0C.0 Off |                    0 |
| N/A   45C    P0            143W /  300W |    2180MiB /  81920MiB |     17%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  |   00000000:00:0D.0 Off |                    0 |
| N/A   47C    P0            118W /  300W |    2180MiB /  81920MiB |     19%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  |   00000000:00:0E.0 Off |                    0 |
| N/A   47C    P0            131W /  300W |    2180MiB /  81920MiB |      8%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/800 val_loss:16.0277 train_time:244ms step_avg:nanms
step:1/800 train_loss:16.0220 train_time:52083ms step_avg:nanms
step:2/800 train_loss:15.8972 train_time:54127ms step_avg:nanms
step:3/800 train_loss:15.5678 train_time:54523ms step_avg:nanms
step:4/800 train_loss:14.8788 train_time:54921ms step_avg:nanms
step:5/800 train_loss:13.5403 train_time:55317ms step_avg:nanms
step:6/800 train_loss:11.7819 train_time:55709ms step_avg:nanms
step:7/800 train_loss:10.2234 train_time:56103ms step_avg:nanms
step:8/800 train_loss:9.6851 train_time:56496ms step_avg:nanms
step:9/800 train_loss:9.4711 train_time:56890ms step_avg:nanms
step:10/800 train_loss:9.3005 train_time:57281ms step_avg:nanms
step:11/800 train_loss:9.1185 train_time:377ms step_avg:nanms
step:12/800 train_loss:8.9686 train_time:769ms step_avg:nanms
step:13/800 train_loss:8.7189 train_time:1160ms step_avg:386.65ms
step:14/800 train_loss:8.5875 train_time:1551ms step_avg:387.80ms
step:15/800 train_loss:8.4244 train_time:1943ms step_avg:388.56ms
step:16/800 train_loss:8.2523 train_time:2334ms step_avg:388.97ms
step:17/800 train_loss:8.0968 train_time:2726ms step_avg:389.37ms
step:18/800 train_loss:7.9617 train_time:3119ms step_avg:389.92ms
step:19/800 train_loss:7.7220 train_time:3512ms step_avg:390.18ms
step:20/800 train_loss:7.6237 train_time:3903ms step_avg:390.25ms
step:21/800 train_loss:7.2579 train_time:4295ms step_avg:390.42ms
step:22/800 train_loss:7.5334 train_time:4685ms step_avg:390.44ms
step:23/800 train_loss:7.6808 train_time:5076ms step_avg:390.47ms
step:24/800 train_loss:7.3633 train_time:5468ms step_avg:390.56ms
step:25/800 train_loss:7.4411 train_time:5861ms step_avg:390.76ms
step:26/800 train_loss:7.2144 train_time:6252ms step_avg:390.76ms
step:27/800 train_loss:7.1122 train_time:6644ms step_avg:390.82ms
step:28/800 train_loss:7.2468 train_time:7037ms step_avg:390.93ms
step:29/800 train_loss:6.9210 train_time:7429ms step_avg:390.99ms
step:30/800 train_loss:7.1375 train_time:7825ms step_avg:391.23ms
step:31/800 train_loss:6.9821 train_time:8222ms step_avg:391.52ms
step:32/800 train_loss:6.8971 train_time:8613ms step_avg:391.51ms
step:33/800 train_loss:6.6844 train_time:9005ms step_avg:391.52ms
step:34/800 train_loss:7.0793 train_time:9398ms step_avg:391.58ms
step:35/800 train_loss:6.8617 train_time:9789ms step_avg:391.58ms
step:36/800 train_loss:7.0588 train_time:10182ms step_avg:391.63ms
step:37/800 train_loss:6.9403 train_time:10576ms step_avg:391.69ms
step:38/800 train_loss:6.7904 train_time:10967ms step_avg:391.69ms
step:39/800 train_loss:6.6577 train_time:11361ms step_avg:391.77ms
step:40/800 train_loss:6.7471 train_time:11753ms step_avg:391.77ms
step:41/800 train_loss:6.6094 train_time:12144ms step_avg:391.75ms
step:42/800 train_loss:6.6533 train_time:12539ms step_avg:391.84ms
step:43/800 train_loss:6.4719 train_time:12932ms step_avg:391.89ms
step:44/800 train_loss:6.5708 train_time:13327ms step_avg:391.96ms
step:45/800 train_loss:6.5453 train_time:13723ms step_avg:392.09ms
step:46/800 train_loss:6.7472 train_time:14117ms step_avg:392.13ms
step:47/800 train_loss:6.5421 train_time:14511ms step_avg:392.18ms
step:48/800 train_loss:6.3713 train_time:14904ms step_avg:392.21ms
step:49/800 train_loss:6.6192 train_time:15297ms step_avg:392.24ms
step:50/800 train_loss:6.4710 train_time:15691ms step_avg:392.28ms
step:51/800 train_loss:6.6192 train_time:16086ms step_avg:392.33ms
step:52/800 train_loss:6.4706 train_time:16479ms step_avg:392.35ms
step:53/800 train_loss:6.3008 train_time:16872ms step_avg:392.38ms
step:54/800 train_loss:6.4302 train_time:17267ms step_avg:392.42ms
step:55/800 train_loss:6.3554 train_time:17661ms step_avg:392.47ms
step:56/800 train_loss:6.6580 train_time:18055ms step_avg:392.49ms
step:57/800 train_loss:6.3380 train_time:18447ms step_avg:392.49ms
step:58/800 train_loss:6.2077 train_time:18840ms step_avg:392.50ms
step:59/800 train_loss:6.3904 train_time:19234ms step_avg:392.54ms
step:60/800 train_loss:6.2930 train_time:19627ms step_avg:392.55ms
step:61/800 train_loss:6.4057 train_time:20024ms step_avg:392.63ms
step:62/800 train_loss:6.1994 train_time:20418ms step_avg:392.64ms
step:63/800 train_loss:6.2782 train_time:20812ms step_avg:392.68ms
step:64/800 train_loss:6.2343 train_time:21206ms step_avg:392.71ms
step:65/800 train_loss:6.7320 train_time:21600ms step_avg:392.72ms
step:66/800 train_loss:6.0670 train_time:21995ms step_avg:392.77ms
step:67/800 train_loss:6.2262 train_time:22390ms step_avg:392.81ms
step:68/800 train_loss:6.0828 train_time:22783ms step_avg:392.81ms
step:69/800 train_loss:6.4009 train_time:23176ms step_avg:392.81ms
step:70/800 train_loss:6.0086 train_time:23571ms step_avg:392.85ms
step:71/800 train_loss:6.0436 train_time:23964ms step_avg:392.85ms
step:72/800 train_loss:6.2635 train_time:24359ms step_avg:392.88ms
step:73/800 train_loss:6.1763 train_time:24753ms step_avg:392.90ms
step:74/800 train_loss:6.0696 train_time:25148ms step_avg:392.93ms
step:75/800 train_loss:6.1730 train_time:25542ms step_avg:392.96ms
step:76/800 train_loss:6.1148 train_time:25936ms step_avg:392.97ms
step:77/800 train_loss:6.1001 train_time:26330ms step_avg:392.98ms
step:78/800 train_loss:6.1670 train_time:26727ms step_avg:393.05ms
step:79/800 train_loss:6.2155 train_time:27128ms step_avg:393.17ms
step:80/800 train_loss:6.0696 train_time:27527ms step_avg:393.24ms
step:81/800 train_loss:6.1647 train_time:27924ms step_avg:393.30ms
step:82/800 train_loss:5.8970 train_time:28319ms step_avg:393.32ms
step:83/800 train_loss:6.0833 train_time:28713ms step_avg:393.33ms
step:84/800 train_loss:6.0536 train_time:29108ms step_avg:393.35ms
step:85/800 train_loss:5.9904 train_time:29502ms step_avg:393.36ms
step:86/800 train_loss:5.8580 train_time:29899ms step_avg:393.41ms
step:87/800 train_loss:6.0675 train_time:30292ms step_avg:393.41ms
step:88/800 train_loss:5.9734 train_time:30687ms step_avg:393.43ms
step:89/800 train_loss:6.0576 train_time:31081ms step_avg:393.42ms
step:90/800 train_loss:6.0269 train_time:31474ms step_avg:393.43ms
step:91/800 train_loss:5.9272 train_time:31869ms step_avg:393.45ms
step:92/800 train_loss:5.9100 train_time:32265ms step_avg:393.47ms
step:93/800 train_loss:6.0124 train_time:32660ms step_avg:393.49ms
step:94/800 train_loss:5.8786 train_time:33054ms step_avg:393.50ms
step:95/800 train_loss:5.8485 train_time:33448ms step_avg:393.50ms
step:96/800 train_loss:5.8547 train_time:33843ms step_avg:393.53ms
step:97/800 train_loss:5.7628 train_time:34239ms step_avg:393.55ms
step:98/800 train_loss:5.8590 train_time:34633ms step_avg:393.56ms
step:99/800 train_loss:5.7505 train_time:35029ms step_avg:393.58ms
step:100/800 train_loss:5.8937 train_time:35427ms step_avg:393.63ms
step:101/800 train_loss:5.8446 train_time:35825ms step_avg:393.68ms
step:102/800 train_loss:5.7320 train_time:36220ms step_avg:393.70ms
step:103/800 train_loss:5.8452 train_time:36614ms step_avg:393.70ms
step:104/800 train_loss:5.8220 train_time:37010ms step_avg:393.72ms
step:105/800 train_loss:5.6206 train_time:37403ms step_avg:393.72ms
step:106/800 train_loss:5.7510 train_time:37798ms step_avg:393.73ms
step:107/800 train_loss:6.0016 train_time:38193ms step_avg:393.74ms
step:108/800 train_loss:5.7407 train_time:38588ms step_avg:393.75ms
step:109/800 train_loss:5.4692 train_time:38983ms step_avg:393.77ms
step:110/800 train_loss:5.7022 train_time:39378ms step_avg:393.78ms
step:111/800 train_loss:5.6504 train_time:39772ms step_avg:393.78ms
step:112/800 train_loss:5.6268 train_time:40165ms step_avg:393.78ms
step:113/800 train_loss:5.7106 train_time:40561ms step_avg:393.79ms
step:114/800 train_loss:5.6364 train_time:40955ms step_avg:393.80ms
step:115/800 train_loss:5.4887 train_time:41352ms step_avg:393.83ms
step:116/800 train_loss:5.6843 train_time:41745ms step_avg:393.82ms
step:117/800 train_loss:5.5069 train_time:42140ms step_avg:393.83ms
step:118/800 train_loss:5.5060 train_time:42534ms step_avg:393.83ms
step:119/800 train_loss:5.5968 train_time:42930ms step_avg:393.85ms
step:120/800 train_loss:5.6233 train_time:43327ms step_avg:393.89ms
step:121/800 train_loss:5.5327 train_time:43725ms step_avg:393.92ms
step:122/800 train_loss:5.4169 train_time:44124ms step_avg:393.96ms
step:123/800 train_loss:5.5218 train_time:44517ms step_avg:393.96ms
step:124/800 train_loss:5.3830 train_time:44913ms step_avg:393.97ms
step:125/800 train_loss:5.6880 train_time:45307ms step_avg:393.97ms
step:125/800 val_loss:5.5101 train_time:45322ms step_avg:394.11ms
step:126/800 train_loss:5.5256 train_time:45707ms step_avg:394.03ms
step:127/800 train_loss:5.5056 train_time:46102ms step_avg:394.03ms
step:128/800 train_loss:5.5799 train_time:46496ms step_avg:394.04ms
step:129/800 train_loss:5.4168 train_time:46892ms step_avg:394.05ms
step:130/800 train_loss:5.6832 train_time:47287ms step_avg:394.06ms
step:131/800 train_loss:5.4673 train_time:47683ms step_avg:394.07ms
step:132/800 train_loss:5.4780 train_time:48078ms step_avg:394.09ms
step:133/800 train_loss:5.3971 train_time:48474ms step_avg:394.10ms
step:134/800 train_loss:5.4423 train_time:48868ms step_avg:394.09ms
step:135/800 train_loss:5.3911 train_time:49264ms step_avg:394.11ms
step:136/800 train_loss:5.4370 train_time:49658ms step_avg:394.11ms
step:137/800 train_loss:5.2328 train_time:50053ms step_avg:394.12ms
step:138/800 train_loss:5.3942 train_time:50449ms step_avg:394.13ms
step:139/800 train_loss:5.3704 train_time:50844ms step_avg:394.14ms
step:140/800 train_loss:5.3706 train_time:51240ms step_avg:394.16ms
step:141/800 train_loss:5.3891 train_time:51634ms step_avg:394.15ms
step:142/800 train_loss:5.2936 train_time:52032ms step_avg:394.18ms
step:143/800 train_loss:5.3713 train_time:52430ms step_avg:394.21ms
step:144/800 train_loss:5.1615 train_time:52828ms step_avg:394.24ms
step:145/800 train_loss:5.3530 train_time:53223ms step_avg:394.24ms
step:146/800 train_loss:5.2776 train_time:53615ms step_avg:394.23ms
step:147/800 train_loss:5.1911 train_time:54012ms step_avg:394.25ms
step:148/800 train_loss:5.3153 train_time:54407ms step_avg:394.26ms
step:149/800 train_loss:5.2710 train_time:54804ms step_avg:394.27ms
step:150/800 train_loss:5.3453 train_time:55198ms step_avg:394.27ms
step:151/800 train_loss:5.3631 train_time:55592ms step_avg:394.27ms
step:152/800 train_loss:5.2269 train_time:55987ms step_avg:394.28ms
step:153/800 train_loss:5.2135 train_time:56382ms step_avg:394.28ms
step:154/800 train_loss:5.2777 train_time:56775ms step_avg:394.27ms
step:155/800 train_loss:5.2047 train_time:57171ms step_avg:394.28ms
step:156/800 train_loss:5.1832 train_time:57566ms step_avg:394.29ms
step:157/800 train_loss:5.1839 train_time:57960ms step_avg:394.29ms
step:158/800 train_loss:5.3209 train_time:58354ms step_avg:394.28ms
step:159/800 train_loss:5.0911 train_time:58749ms step_avg:394.29ms
step:160/800 train_loss:5.1562 train_time:59144ms step_avg:394.29ms
step:161/800 train_loss:5.0069 train_time:59537ms step_avg:394.29ms
step:162/800 train_loss:5.1550 train_time:59933ms step_avg:394.30ms
step:163/800 train_loss:5.1904 train_time:60330ms step_avg:394.31ms
step:164/800 train_loss:5.1845 train_time:60727ms step_avg:394.33ms
step:165/800 train_loss:4.9938 train_time:61122ms step_avg:394.34ms
step:166/800 train_loss:5.1050 train_time:61517ms step_avg:394.34ms
step:167/800 train_loss:5.2705 train_time:61911ms step_avg:394.34ms
step:168/800 train_loss:5.0396 train_time:62307ms step_avg:394.35ms
step:169/800 train_loss:5.1265 train_time:62701ms step_avg:394.35ms
step:170/800 train_loss:4.9833 train_time:63095ms step_avg:394.34ms
step:171/800 train_loss:4.9236 train_time:63490ms step_avg:394.35ms
step:172/800 train_loss:5.0233 train_time:63885ms step_avg:394.35ms
step:173/800 train_loss:4.9996 train_time:64281ms step_avg:394.36ms
step:174/800 train_loss:5.0620 train_time:64677ms step_avg:394.37ms
step:175/800 train_loss:5.2010 train_time:65072ms step_avg:394.38ms
step:176/800 train_loss:5.0849 train_time:65465ms step_avg:394.37ms
step:177/800 train_loss:4.9161 train_time:65861ms step_avg:394.38ms
step:178/800 train_loss:4.8898 train_time:66256ms step_avg:394.38ms
step:179/800 train_loss:4.9315 train_time:66652ms step_avg:394.39ms
step:180/800 train_loss:4.9760 train_time:67046ms step_avg:394.39ms
step:181/800 train_loss:4.9596 train_time:67442ms step_avg:394.40ms
step:182/800 train_loss:5.0747 train_time:67838ms step_avg:394.41ms
step:183/800 train_loss:4.9562 train_time:68233ms step_avg:394.41ms
step:184/800 train_loss:4.8919 train_time:68631ms step_avg:394.43ms
step:185/800 train_loss:4.9133 train_time:69029ms step_avg:394.45ms
step:186/800 train_loss:5.0425 train_time:69423ms step_avg:394.45ms
step:187/800 train_loss:4.9269 train_time:69818ms step_avg:394.45ms
step:188/800 train_loss:5.1743 train_time:70212ms step_avg:394.45ms
step:189/800 train_loss:4.9562 train_time:70716ms step_avg:395.06ms
step:190/800 train_loss:4.8688 train_time:71250ms step_avg:395.83ms
step:191/800 train_loss:5.0318 train_time:71644ms step_avg:395.82ms
step:192/800 train_loss:4.8631 train_time:72039ms step_avg:395.82ms
step:193/800 train_loss:4.7870 train_time:72433ms step_avg:395.81ms
step:194/800 train_loss:4.9941 train_time:72840ms step_avg:395.87ms
step:195/800 train_loss:4.9311 train_time:73235ms step_avg:395.86ms
step:196/800 train_loss:5.1212 train_time:73629ms step_avg:395.86ms
step:197/800 train_loss:5.0046 train_time:74027ms step_avg:395.87ms
step:198/800 train_loss:4.8362 train_time:74421ms step_avg:395.86ms
step:199/800 train_loss:4.8835 train_time:74816ms step_avg:395.85ms
step:200/800 train_loss:4.7735 train_time:75210ms step_avg:395.84ms
step:201/800 train_loss:4.8546 train_time:75606ms step_avg:395.84ms
step:202/800 train_loss:4.7759 train_time:76000ms step_avg:395.83ms
step:203/800 train_loss:5.0201 train_time:76394ms step_avg:395.82ms
step:204/800 train_loss:4.8984 train_time:76789ms step_avg:395.82ms
step:205/800 train_loss:4.8789 train_time:77184ms step_avg:395.82ms
step:206/800 train_loss:5.0296 train_time:77580ms step_avg:395.82ms
step:207/800 train_loss:4.6937 train_time:77974ms step_avg:395.81ms
step:208/800 train_loss:4.8411 train_time:78369ms step_avg:395.80ms
step:209/800 train_loss:4.8013 train_time:78764ms step_avg:395.80ms
step:210/800 train_loss:4.9640 train_time:79158ms step_avg:395.79ms
step:211/800 train_loss:4.8832 train_time:79553ms step_avg:395.79ms
step:212/800 train_loss:4.7752 train_time:79947ms step_avg:395.78ms
step:213/800 train_loss:4.9092 train_time:80341ms step_avg:395.77ms
step:214/800 train_loss:4.7370 train_time:80736ms step_avg:395.77ms
step:215/800 train_loss:4.8280 train_time:81130ms step_avg:395.76ms
step:216/800 train_loss:4.6811 train_time:81529ms step_avg:395.77ms
step:217/800 train_loss:4.8131 train_time:81924ms step_avg:395.77ms
step:218/800 train_loss:4.7931 train_time:82319ms step_avg:395.77ms
step:219/800 train_loss:4.7572 train_time:82714ms step_avg:395.76ms
step:220/800 train_loss:4.7704 train_time:83109ms step_avg:395.76ms
step:221/800 train_loss:4.7963 train_time:83503ms step_avg:395.75ms
step:222/800 train_loss:4.8408 train_time:83896ms step_avg:395.74ms
step:223/800 train_loss:4.7785 train_time:84292ms step_avg:395.74ms
step:224/800 train_loss:4.7805 train_time:84687ms step_avg:395.74ms
step:225/800 train_loss:4.9086 train_time:85083ms step_avg:395.73ms
step:226/800 train_loss:4.6469 train_time:85477ms step_avg:395.73ms
step:227/800 train_loss:4.6786 train_time:85871ms step_avg:395.72ms
step:228/800 train_loss:4.6620 train_time:86269ms step_avg:395.73ms
step:229/800 train_loss:4.8199 train_time:86664ms step_avg:395.73ms
step:230/800 train_loss:4.6591 train_time:87059ms step_avg:395.72ms
step:231/800 train_loss:4.8053 train_time:87454ms step_avg:395.72ms
step:232/800 train_loss:4.6684 train_time:87848ms step_avg:395.71ms
step:233/800 train_loss:4.6327 train_time:88244ms step_avg:395.71ms
step:234/800 train_loss:4.8294 train_time:88638ms step_avg:395.71ms
step:235/800 train_loss:4.6762 train_time:89033ms step_avg:395.70ms
step:236/800 train_loss:4.5890 train_time:89430ms step_avg:395.71ms
step:237/800 train_loss:4.8560 train_time:89825ms step_avg:395.70ms
step:238/800 train_loss:4.7399 train_time:90218ms step_avg:395.69ms
step:239/800 train_loss:4.6515 train_time:90616ms step_avg:395.70ms
step:240/800 train_loss:4.7884 train_time:91013ms step_avg:395.71ms
step:241/800 train_loss:4.7738 train_time:91409ms step_avg:395.71ms
step:242/800 train_loss:4.6727 train_time:91804ms step_avg:395.70ms
step:243/800 train_loss:4.8481 train_time:92198ms step_avg:395.70ms
step:244/800 train_loss:4.6669 train_time:92595ms step_avg:395.70ms
step:245/800 train_loss:4.6841 train_time:92990ms step_avg:395.70ms
step:246/800 train_loss:4.7606 train_time:93386ms step_avg:395.70ms
step:247/800 train_loss:4.7054 train_time:93783ms step_avg:395.71ms
step:248/800 train_loss:4.6576 train_time:94179ms step_avg:395.71ms
step:249/800 train_loss:4.8255 train_time:94576ms step_avg:395.72ms
step:250/800 train_loss:4.5588 train_time:94971ms step_avg:395.71ms
step:250/800 val_loss:4.6673 train_time:94987ms step_avg:395.78ms
step:251/800 train_loss:4.5959 train_time:95372ms step_avg:395.73ms
step:252/800 train_loss:4.7289 train_time:95767ms step_avg:395.73ms
step:253/800 train_loss:4.7294 train_time:96161ms step_avg:395.72ms
step:254/800 train_loss:4.5930 train_time:96559ms step_avg:395.73ms
step:255/800 train_loss:4.6113 train_time:96954ms step_avg:395.73ms
step:256/800 train_loss:4.7541 train_time:97349ms step_avg:395.73ms
step:257/800 train_loss:4.6937 train_time:97746ms step_avg:395.73ms
step:258/800 train_loss:4.6611 train_time:98145ms step_avg:395.75ms
step:259/800 train_loss:4.5931 train_time:98540ms step_avg:395.74ms
step:260/800 train_loss:4.6068 train_time:98935ms step_avg:395.74ms
step:261/800 train_loss:4.6851 train_time:99329ms step_avg:395.73ms
step:262/800 train_loss:4.6834 train_time:99723ms step_avg:395.73ms
step:263/800 train_loss:4.5886 train_time:100119ms step_avg:395.73ms
step:264/800 train_loss:4.5338 train_time:100514ms step_avg:395.72ms
step:265/800 train_loss:4.5905 train_time:100907ms step_avg:395.71ms
step:266/800 train_loss:4.4409 train_time:101303ms step_avg:395.71ms
step:267/800 train_loss:4.5061 train_time:101697ms step_avg:395.71ms
step:268/800 train_loss:4.5497 train_time:102091ms step_avg:395.70ms
step:269/800 train_loss:4.5081 train_time:102486ms step_avg:395.70ms
step:270/800 train_loss:4.4588 train_time:102882ms step_avg:395.70ms
step:271/800 train_loss:4.6914 train_time:103277ms step_avg:395.70ms
step:272/800 train_loss:4.6158 train_time:103673ms step_avg:395.70ms
step:273/800 train_loss:4.4775 train_time:104069ms step_avg:395.70ms
step:274/800 train_loss:4.5300 train_time:104463ms step_avg:395.69ms
step:275/800 train_loss:4.6447 train_time:104858ms step_avg:395.69ms
step:276/800 train_loss:4.6485 train_time:105252ms step_avg:395.69ms
step:277/800 train_loss:4.8538 train_time:105647ms step_avg:395.68ms
step:278/800 train_loss:4.6030 train_time:106045ms step_avg:395.69ms
step:279/800 train_loss:4.7225 train_time:106438ms step_avg:395.68ms
step:280/800 train_loss:4.5706 train_time:106832ms step_avg:395.68ms
step:281/800 train_loss:4.6477 train_time:107228ms step_avg:395.68ms
step:282/800 train_loss:4.5320 train_time:107623ms step_avg:395.67ms
step:283/800 train_loss:4.6313 train_time:108018ms step_avg:395.67ms
step:284/800 train_loss:4.4662 train_time:108412ms step_avg:395.66ms
step:285/800 train_loss:4.6328 train_time:108807ms step_avg:395.66ms
step:286/800 train_loss:4.6183 train_time:109203ms step_avg:395.66ms
step:287/800 train_loss:4.6557 train_time:109597ms step_avg:395.66ms
step:288/800 train_loss:4.4993 train_time:109994ms step_avg:395.66ms
step:289/800 train_loss:4.5708 train_time:110389ms step_avg:395.66ms
step:290/800 train_loss:4.4312 train_time:110786ms step_avg:395.66ms
step:291/800 train_loss:4.4323 train_time:111181ms step_avg:395.66ms
step:292/800 train_loss:4.5428 train_time:111576ms step_avg:395.66ms
step:293/800 train_loss:4.4313 train_time:111971ms step_avg:395.66ms
step:294/800 train_loss:4.4789 train_time:112365ms step_avg:395.65ms
step:295/800 train_loss:4.5002 train_time:112760ms step_avg:395.65ms
step:296/800 train_loss:4.3678 train_time:113155ms step_avg:395.65ms
step:297/800 train_loss:4.3679 train_time:113552ms step_avg:395.65ms
step:298/800 train_loss:4.3897 train_time:113948ms step_avg:395.65ms
step:299/800 train_loss:4.4956 train_time:114346ms step_avg:395.66ms
step:300/800 train_loss:4.3681 train_time:114743ms step_avg:395.67ms
step:301/800 train_loss:4.5432 train_time:115138ms step_avg:395.66ms
step:302/800 train_loss:4.5161 train_time:115534ms step_avg:395.66ms
step:303/800 train_loss:4.4427 train_time:115928ms step_avg:395.66ms
step:304/800 train_loss:4.5094 train_time:116324ms step_avg:395.66ms
step:305/800 train_loss:4.4930 train_time:116718ms step_avg:395.66ms
step:306/800 train_loss:4.9615 train_time:117112ms step_avg:395.65ms
step:307/800 train_loss:4.4393 train_time:117506ms step_avg:395.64ms
step:308/800 train_loss:4.3580 train_time:117901ms step_avg:395.64ms
step:309/800 train_loss:4.5344 train_time:118296ms step_avg:395.64ms
step:310/800 train_loss:4.3393 train_time:118690ms step_avg:395.63ms
step:311/800 train_loss:4.5813 train_time:119084ms step_avg:395.63ms
step:312/800 train_loss:4.4692 train_time:119481ms step_avg:395.63ms
step:313/800 train_loss:4.3804 train_time:119874ms step_avg:395.62ms
step:314/800 train_loss:4.5087 train_time:120270ms step_avg:395.62ms
step:315/800 train_loss:4.6152 train_time:120664ms step_avg:395.62ms
step:316/800 train_loss:4.4618 train_time:121058ms step_avg:395.62ms
step:317/800 train_loss:4.3294 train_time:121451ms step_avg:395.61ms
step:318/800 train_loss:4.3741 train_time:121847ms step_avg:395.61ms
step:319/800 train_loss:4.3956 train_time:122246ms step_avg:395.62ms
step:320/800 train_loss:4.3531 train_time:122646ms step_avg:395.63ms
step:321/800 train_loss:4.4518 train_time:123039ms step_avg:395.63ms
step:322/800 train_loss:4.4374 train_time:123433ms step_avg:395.62ms
step:323/800 train_loss:4.3955 train_time:123829ms step_avg:395.62ms
step:324/800 train_loss:4.4791 train_time:124223ms step_avg:395.61ms
step:325/800 train_loss:4.4589 train_time:124617ms step_avg:395.61ms
step:326/800 train_loss:4.5261 train_time:125011ms step_avg:395.60ms
step:327/800 train_loss:4.3747 train_time:125406ms step_avg:395.60ms
step:328/800 train_loss:4.8428 train_time:125801ms step_avg:395.60ms
step:329/800 train_loss:4.5426 train_time:126194ms step_avg:395.59ms
step:330/800 train_loss:4.3046 train_time:126588ms step_avg:395.59ms
step:331/800 train_loss:4.2540 train_time:126984ms step_avg:395.59ms
step:332/800 train_loss:4.4453 train_time:127378ms step_avg:395.58ms
step:333/800 train_loss:4.3606 train_time:127772ms step_avg:395.58ms
step:334/800 train_loss:4.3531 train_time:128167ms step_avg:395.58ms
step:335/800 train_loss:4.3064 train_time:128561ms step_avg:395.57ms
step:336/800 train_loss:4.4802 train_time:128955ms step_avg:395.57ms
step:337/800 train_loss:4.4210 train_time:129350ms step_avg:395.57ms
step:338/800 train_loss:4.9280 train_time:129747ms step_avg:395.57ms
step:339/800 train_loss:4.3991 train_time:130146ms step_avg:395.58ms
step:340/800 train_loss:4.3626 train_time:130540ms step_avg:395.58ms
step:341/800 train_loss:4.3609 train_time:130933ms step_avg:395.57ms
step:342/800 train_loss:4.2945 train_time:131331ms step_avg:395.58ms
step:343/800 train_loss:4.2691 train_time:131726ms step_avg:395.57ms
step:344/800 train_loss:4.3237 train_time:132120ms step_avg:395.57ms
step:345/800 train_loss:4.4431 train_time:132514ms step_avg:395.56ms
step:346/800 train_loss:4.3102 train_time:132909ms step_avg:395.56ms
step:347/800 train_loss:4.2345 train_time:133305ms step_avg:395.56ms
step:348/800 train_loss:4.2771 train_time:133700ms step_avg:395.56ms
step:349/800 train_loss:4.3118 train_time:134096ms step_avg:395.56ms
step:350/800 train_loss:4.2557 train_time:134490ms step_avg:395.56ms
step:351/800 train_loss:3.9507 train_time:134887ms step_avg:395.56ms
step:352/800 train_loss:4.2342 train_time:135282ms step_avg:395.56ms
step:353/800 train_loss:4.5800 train_time:135676ms step_avg:395.56ms
step:354/800 train_loss:4.0937 train_time:136069ms step_avg:395.55ms
step:355/800 train_loss:4.3525 train_time:136466ms step_avg:395.55ms
step:356/800 train_loss:4.2302 train_time:136860ms step_avg:395.55ms
step:357/800 train_loss:4.3317 train_time:137252ms step_avg:395.54ms
step:358/800 train_loss:4.3152 train_time:137649ms step_avg:395.54ms
step:359/800 train_loss:4.2724 train_time:138046ms step_avg:395.55ms
step:360/800 train_loss:4.3597 train_time:138444ms step_avg:395.56ms
step:361/800 train_loss:3.9473 train_time:138838ms step_avg:395.55ms
step:362/800 train_loss:4.4592 train_time:139235ms step_avg:395.55ms
step:363/800 train_loss:4.3483 train_time:139628ms step_avg:395.55ms
step:364/800 train_loss:4.2597 train_time:140023ms step_avg:395.55ms
step:365/800 train_loss:4.1781 train_time:140418ms step_avg:395.54ms
step:366/800 train_loss:4.3414 train_time:140812ms step_avg:395.54ms
step:367/800 train_loss:4.2904 train_time:141215ms step_avg:395.56ms
step:368/800 train_loss:4.2675 train_time:141609ms step_avg:395.56ms
step:369/800 train_loss:4.2663 train_time:142005ms step_avg:395.56ms
step:370/800 train_loss:4.1572 train_time:142399ms step_avg:395.55ms
step:371/800 train_loss:4.3134 train_time:142794ms step_avg:395.55ms
step:372/800 train_loss:4.2089 train_time:143189ms step_avg:395.55ms
step:373/800 train_loss:4.1070 train_time:143584ms step_avg:395.55ms
step:374/800 train_loss:4.3179 train_time:143980ms step_avg:395.55ms
step:375/800 train_loss:4.2476 train_time:144374ms step_avg:395.54ms
step:375/800 val_loss:4.2507 train_time:144388ms step_avg:395.58ms
step:376/800 train_loss:4.2221 train_time:144772ms step_avg:395.55ms
step:377/800 train_loss:4.2800 train_time:145167ms step_avg:395.55ms
step:378/800 train_loss:4.1884 train_time:145679ms step_avg:395.87ms
step:379/800 train_loss:4.2452 train_time:146078ms step_avg:395.88ms
step:380/800 train_loss:4.3028 train_time:146603ms step_avg:396.22ms
step:381/800 train_loss:4.3531 train_time:146996ms step_avg:396.22ms
step:382/800 train_loss:4.2684 train_time:147392ms step_avg:396.21ms
step:383/800 train_loss:4.2465 train_time:147788ms step_avg:396.21ms
step:384/800 train_loss:4.1851 train_time:148181ms step_avg:396.21ms
step:385/800 train_loss:4.2746 train_time:148576ms step_avg:396.20ms
step:386/800 train_loss:4.1881 train_time:148969ms step_avg:396.20ms
step:387/800 train_loss:4.3053 train_time:149365ms step_avg:396.19ms
step:388/800 train_loss:4.4889 train_time:149764ms step_avg:396.20ms
step:389/800 train_loss:4.2084 train_time:150162ms step_avg:396.21ms
step:390/800 train_loss:4.1849 train_time:150556ms step_avg:396.20ms
step:391/800 train_loss:4.2929 train_time:150951ms step_avg:396.20ms
step:392/800 train_loss:4.2066 train_time:151346ms step_avg:396.19ms
step:393/800 train_loss:4.3160 train_time:151749ms step_avg:396.21ms
step:394/800 train_loss:4.1441 train_time:152142ms step_avg:396.20ms
step:395/800 train_loss:4.2838 train_time:152536ms step_avg:396.20ms
step:396/800 train_loss:4.0427 train_time:152930ms step_avg:396.19ms
step:397/800 train_loss:4.2266 train_time:153324ms step_avg:396.19ms
step:398/800 train_loss:4.2948 train_time:153719ms step_avg:396.18ms
step:399/800 train_loss:4.2770 train_time:154115ms step_avg:396.18ms
step:400/800 train_loss:4.1840 train_time:154509ms step_avg:396.18ms
step:401/800 train_loss:4.2283 train_time:154903ms step_avg:396.17ms
step:402/800 train_loss:4.2893 train_time:155297ms step_avg:396.17ms
step:403/800 train_loss:4.2464 train_time:155692ms step_avg:396.16ms
step:404/800 train_loss:4.3401 train_time:156086ms step_avg:396.16ms
step:405/800 train_loss:4.0976 train_time:156480ms step_avg:396.15ms
step:406/800 train_loss:4.1828 train_time:156873ms step_avg:396.14ms
step:407/800 train_loss:4.4542 train_time:157267ms step_avg:396.14ms
step:408/800 train_loss:4.2037 train_time:157666ms step_avg:396.15ms
step:409/800 train_loss:4.2107 train_time:158065ms step_avg:396.15ms
step:410/800 train_loss:4.2530 train_time:158463ms step_avg:396.16ms
step:411/800 train_loss:4.1291 train_time:158857ms step_avg:396.15ms
step:412/800 train_loss:4.1569 train_time:159253ms step_avg:396.15ms
step:413/800 train_loss:4.5531 train_time:159648ms step_avg:396.15ms
step:414/800 train_loss:4.0137 train_time:160043ms step_avg:396.15ms
step:415/800 train_loss:4.4047 train_time:160438ms step_avg:396.14ms
step:416/800 train_loss:4.1523 train_time:160834ms step_avg:396.14ms
step:417/800 train_loss:4.1531 train_time:161228ms step_avg:396.14ms
step:418/800 train_loss:4.3411 train_time:161624ms step_avg:396.14ms
step:419/800 train_loss:4.0692 train_time:162017ms step_avg:396.13ms
step:420/800 train_loss:4.1791 train_time:162412ms step_avg:396.13ms
step:421/800 train_loss:4.1289 train_time:162807ms step_avg:396.12ms
step:422/800 train_loss:4.0272 train_time:163202ms step_avg:396.12ms
step:423/800 train_loss:4.1526 train_time:163596ms step_avg:396.12ms
step:424/800 train_loss:4.2510 train_time:163990ms step_avg:396.11ms
step:425/800 train_loss:4.0241 train_time:164383ms step_avg:396.10ms
step:426/800 train_loss:4.2031 train_time:164779ms step_avg:396.10ms
step:427/800 train_loss:4.0794 train_time:165173ms step_avg:396.10ms
step:428/800 train_loss:4.2818 train_time:165570ms step_avg:396.10ms
step:429/800 train_loss:4.2038 train_time:165967ms step_avg:396.10ms
step:430/800 train_loss:4.1335 train_time:166364ms step_avg:396.11ms
step:431/800 train_loss:4.1052 train_time:166762ms step_avg:396.11ms
step:432/800 train_loss:4.0188 train_time:167156ms step_avg:396.10ms
step:433/800 train_loss:4.1413 train_time:167550ms step_avg:396.10ms
step:434/800 train_loss:4.2141 train_time:167947ms step_avg:396.10ms
step:435/800 train_loss:4.1407 train_time:168340ms step_avg:396.09ms
step:436/800 train_loss:4.1922 train_time:168735ms step_avg:396.09ms
step:437/800 train_loss:4.2016 train_time:169129ms step_avg:396.09ms
step:438/800 train_loss:4.0704 train_time:169524ms step_avg:396.08ms
step:439/800 train_loss:4.1011 train_time:169918ms step_avg:396.08ms
step:440/800 train_loss:4.0714 train_time:170313ms step_avg:396.08ms
step:441/800 train_loss:4.2543 train_time:170707ms step_avg:396.07ms
step:442/800 train_loss:4.1427 train_time:171102ms step_avg:396.07ms
step:443/800 train_loss:4.1274 train_time:171496ms step_avg:396.06ms
step:444/800 train_loss:4.0135 train_time:171891ms step_avg:396.06ms
step:445/800 train_loss:4.2758 train_time:172285ms step_avg:396.06ms
step:446/800 train_loss:4.2074 train_time:172680ms step_avg:396.06ms
step:447/800 train_loss:4.2045 train_time:173073ms step_avg:396.05ms
step:448/800 train_loss:4.1175 train_time:173467ms step_avg:396.04ms
step:449/800 train_loss:4.2160 train_time:173863ms step_avg:396.04ms
step:450/800 train_loss:4.0366 train_time:174256ms step_avg:396.04ms
step:451/800 train_loss:4.0854 train_time:174650ms step_avg:396.03ms
step:452/800 train_loss:3.9545 train_time:175045ms step_avg:396.03ms
step:453/800 train_loss:4.0693 train_time:175439ms step_avg:396.02ms
step:454/800 train_loss:4.0504 train_time:175832ms step_avg:396.02ms
step:455/800 train_loss:4.0062 train_time:176227ms step_avg:396.02ms
step:456/800 train_loss:4.2221 train_time:176620ms step_avg:396.01ms
step:457/800 train_loss:4.0860 train_time:177016ms step_avg:396.01ms
step:458/800 train_loss:4.1587 train_time:177410ms step_avg:396.01ms
step:459/800 train_loss:4.2016 train_time:177805ms step_avg:396.00ms
step:460/800 train_loss:3.9951 train_time:178200ms step_avg:396.00ms
step:461/800 train_loss:4.1702 train_time:178593ms step_avg:395.99ms
step:462/800 train_loss:4.0580 train_time:178988ms step_avg:395.99ms
step:463/800 train_loss:4.0700 train_time:179384ms step_avg:395.99ms
step:464/800 train_loss:4.1416 train_time:179777ms step_avg:395.98ms
step:465/800 train_loss:4.0788 train_time:180171ms step_avg:395.98ms
step:466/800 train_loss:4.0897 train_time:180567ms step_avg:395.98ms
step:467/800 train_loss:4.1906 train_time:180964ms step_avg:395.98ms
step:468/800 train_loss:4.2001 train_time:181356ms step_avg:395.97ms
step:469/800 train_loss:4.1692 train_time:181752ms step_avg:395.97ms
step:470/800 train_loss:4.0655 train_time:182145ms step_avg:395.97ms
step:471/800 train_loss:4.1491 train_time:182540ms step_avg:395.97ms
step:472/800 train_loss:4.1980 train_time:182935ms step_avg:395.96ms
step:473/800 train_loss:4.1238 train_time:183329ms step_avg:395.96ms
step:474/800 train_loss:4.0848 train_time:183723ms step_avg:395.95ms
step:475/800 train_loss:3.9428 train_time:184119ms step_avg:395.95ms
step:476/800 train_loss:4.3702 train_time:184512ms step_avg:395.95ms
step:477/800 train_loss:4.1387 train_time:184908ms step_avg:395.95ms
step:478/800 train_loss:3.9336 train_time:185302ms step_avg:395.94ms
step:479/800 train_loss:4.1587 train_time:185697ms step_avg:395.94ms
step:480/800 train_loss:4.1296 train_time:186092ms step_avg:395.94ms
step:481/800 train_loss:4.2683 train_time:186486ms step_avg:395.94ms
step:482/800 train_loss:4.0789 train_time:186880ms step_avg:395.93ms
step:483/800 train_loss:3.8906 train_time:187274ms step_avg:395.93ms
step:484/800 train_loss:4.1735 train_time:187669ms step_avg:395.93ms
step:485/800 train_loss:4.0195 train_time:188066ms step_avg:395.93ms
step:486/800 train_loss:4.0379 train_time:188463ms step_avg:395.93ms
step:487/800 train_loss:3.9619 train_time:188855ms step_avg:395.92ms
step:488/800 train_loss:4.0200 train_time:189250ms step_avg:395.92ms
step:489/800 train_loss:4.2244 train_time:189646ms step_avg:395.92ms
step:490/800 train_loss:4.0724 train_time:190042ms step_avg:395.92ms
step:491/800 train_loss:3.9600 train_time:190436ms step_avg:395.92ms
step:492/800 train_loss:3.9751 train_time:190830ms step_avg:395.91ms
step:493/800 train_loss:4.0852 train_time:191225ms step_avg:395.91ms
step:494/800 train_loss:3.9407 train_time:191618ms step_avg:395.91ms
step:495/800 train_loss:4.0788 train_time:192012ms step_avg:395.90ms
step:496/800 train_loss:4.0045 train_time:192406ms step_avg:395.90ms
step:497/800 train_loss:3.8989 train_time:192799ms step_avg:395.89ms
step:498/800 train_loss:4.0793 train_time:193193ms step_avg:395.89ms
step:499/800 train_loss:4.1611 train_time:193588ms step_avg:395.88ms
step:500/800 train_loss:4.1983 train_time:193981ms step_avg:395.88ms
step:500/800 val_loss:4.0633 train_time:193996ms step_avg:395.91ms
step:501/800 train_loss:4.1003 train_time:194380ms step_avg:395.89ms
step:502/800 train_loss:4.1464 train_time:194774ms step_avg:395.88ms
step:503/800 train_loss:4.0890 train_time:195168ms step_avg:395.88ms
step:504/800 train_loss:4.1334 train_time:195563ms step_avg:395.88ms
step:505/800 train_loss:4.0907 train_time:195957ms step_avg:395.87ms
step:506/800 train_loss:4.1855 train_time:196350ms step_avg:395.87ms
step:507/800 train_loss:3.9831 train_time:196745ms step_avg:395.87ms
step:508/800 train_loss:4.1139 train_time:197141ms step_avg:395.86ms
step:509/800 train_loss:4.1922 train_time:197534ms step_avg:395.86ms
step:510/800 train_loss:4.1272 train_time:197929ms step_avg:395.86ms
step:511/800 train_loss:3.9396 train_time:198322ms step_avg:395.85ms
step:512/800 train_loss:4.1371 train_time:198717ms step_avg:395.85ms
step:513/800 train_loss:4.0711 train_time:199112ms step_avg:395.85ms
step:514/800 train_loss:4.0363 train_time:199508ms step_avg:395.85ms
step:515/800 train_loss:4.0952 train_time:199901ms step_avg:395.84ms
step:516/800 train_loss:4.0999 train_time:200296ms step_avg:395.84ms
step:517/800 train_loss:4.4182 train_time:200689ms step_avg:395.84ms
step:518/800 train_loss:4.0160 train_time:201083ms step_avg:395.83ms
step:519/800 train_loss:4.1441 train_time:201480ms step_avg:395.84ms
step:520/800 train_loss:4.0488 train_time:201875ms step_avg:395.83ms
step:521/800 train_loss:4.0358 train_time:202269ms step_avg:395.83ms
step:522/800 train_loss:3.9803 train_time:202662ms step_avg:395.82ms
step:523/800 train_loss:4.0008 train_time:203056ms step_avg:395.82ms
step:524/800 train_loss:4.6089 train_time:203450ms step_avg:395.82ms
step:525/800 train_loss:4.1027 train_time:203844ms step_avg:395.81ms
step:526/800 train_loss:4.0448 train_time:204238ms step_avg:395.81ms
step:527/800 train_loss:4.0427 train_time:204632ms step_avg:395.81ms
step:528/800 train_loss:3.9950 train_time:205026ms step_avg:395.80ms
step:529/800 train_loss:3.9732 train_time:205420ms step_avg:395.80ms
step:530/800 train_loss:4.1769 train_time:205816ms step_avg:395.80ms
step:531/800 train_loss:3.9926 train_time:206209ms step_avg:395.79ms
step:532/800 train_loss:4.2703 train_time:206603ms step_avg:395.79ms
step:533/800 train_loss:4.0753 train_time:206998ms step_avg:395.79ms
step:534/800 train_loss:4.0052 train_time:207392ms step_avg:395.79ms
step:535/800 train_loss:4.0262 train_time:207786ms step_avg:395.78ms
step:536/800 train_loss:3.9607 train_time:208183ms step_avg:395.79ms
step:537/800 train_loss:4.0820 train_time:208582ms step_avg:395.79ms
step:538/800 train_loss:4.0783 train_time:208975ms step_avg:395.79ms
step:539/800 train_loss:3.9883 train_time:209370ms step_avg:395.78ms
step:540/800 train_loss:4.4666 train_time:209763ms step_avg:395.78ms
step:541/800 train_loss:4.0089 train_time:210158ms step_avg:395.78ms
step:542/800 train_loss:4.1266 train_time:210551ms step_avg:395.77ms
step:543/800 train_loss:3.9604 train_time:210946ms step_avg:395.77ms
step:544/800 train_loss:3.9354 train_time:211340ms step_avg:395.77ms
step:545/800 train_loss:4.0208 train_time:211735ms step_avg:395.77ms
step:546/800 train_loss:3.9422 train_time:212129ms step_avg:395.76ms
step:547/800 train_loss:3.9874 train_time:212523ms step_avg:395.76ms
step:548/800 train_loss:3.9970 train_time:212917ms step_avg:395.76ms
step:549/800 train_loss:3.9782 train_time:213311ms step_avg:395.75ms
step:550/800 train_loss:4.0653 train_time:213706ms step_avg:395.75ms
step:551/800 train_loss:3.9403 train_time:214101ms step_avg:395.75ms
step:552/800 train_loss:3.9674 train_time:214494ms step_avg:395.74ms
step:553/800 train_loss:4.2882 train_time:214888ms step_avg:395.74ms
step:554/800 train_loss:4.0892 train_time:215285ms step_avg:395.74ms
step:555/800 train_loss:4.0552 train_time:215682ms step_avg:395.75ms
step:556/800 train_loss:4.0182 train_time:216074ms step_avg:395.74ms
step:557/800 train_loss:4.0342 train_time:216471ms step_avg:395.74ms
step:558/800 train_loss:3.6911 train_time:216865ms step_avg:395.74ms
step:559/800 train_loss:3.9477 train_time:217260ms step_avg:395.74ms
step:560/800 train_loss:3.9931 train_time:217653ms step_avg:395.73ms
step:561/800 train_loss:4.0363 train_time:218048ms step_avg:395.73ms
step:562/800 train_loss:3.9503 train_time:218441ms step_avg:395.73ms
step:563/800 train_loss:3.8921 train_time:218837ms step_avg:395.73ms
step:564/800 train_loss:4.0959 train_time:219231ms step_avg:395.72ms
step:565/800 train_loss:3.9082 train_time:219627ms step_avg:395.72ms
step:566/800 train_loss:4.0326 train_time:220020ms step_avg:395.72ms
step:567/800 train_loss:3.9811 train_time:220537ms step_avg:395.94ms
step:568/800 train_loss:3.9271 train_time:220932ms step_avg:395.94ms
step:569/800 train_loss:4.0280 train_time:221327ms step_avg:395.93ms
step:570/800 train_loss:3.9998 train_time:221861ms step_avg:396.18ms
step:571/800 train_loss:4.0255 train_time:222253ms step_avg:396.17ms
step:572/800 train_loss:4.1117 train_time:222647ms step_avg:396.17ms
step:573/800 train_loss:4.0396 train_time:223042ms step_avg:396.17ms
step:574/800 train_loss:4.0518 train_time:223438ms step_avg:396.17ms
step:575/800 train_loss:4.1167 train_time:223832ms step_avg:396.16ms
step:576/800 train_loss:4.0729 train_time:224227ms step_avg:396.16ms
step:577/800 train_loss:4.0761 train_time:224620ms step_avg:396.16ms
step:578/800 train_loss:4.0238 train_time:225014ms step_avg:396.15ms
step:579/800 train_loss:3.9909 train_time:225411ms step_avg:396.15ms
step:580/800 train_loss:3.9951 train_time:225806ms step_avg:396.15ms
step:581/800 train_loss:3.9413 train_time:226200ms step_avg:396.15ms
step:582/800 train_loss:3.9694 train_time:226595ms step_avg:396.14ms
step:583/800 train_loss:4.1979 train_time:226987ms step_avg:396.14ms
step:584/800 train_loss:3.9646 train_time:227383ms step_avg:396.14ms
step:585/800 train_loss:3.9239 train_time:227779ms step_avg:396.14ms
step:586/800 train_loss:4.1132 train_time:228173ms step_avg:396.13ms
step:587/800 train_loss:3.8679 train_time:228566ms step_avg:396.13ms
step:588/800 train_loss:4.0019 train_time:228960ms step_avg:396.13ms
step:589/800 train_loss:3.9993 train_time:229354ms step_avg:396.12ms
step:590/800 train_loss:4.3401 train_time:229749ms step_avg:396.12ms
step:591/800 train_loss:4.1183 train_time:230142ms step_avg:396.11ms
step:592/800 train_loss:3.8615 train_time:230537ms step_avg:396.11ms
step:593/800 train_loss:3.8745 train_time:230931ms step_avg:396.11ms
step:594/800 train_loss:3.8685 train_time:231325ms step_avg:396.10ms
step:595/800 train_loss:3.9066 train_time:231720ms step_avg:396.10ms
step:596/800 train_loss:4.2727 train_time:232115ms step_avg:396.10ms
step:597/800 train_loss:3.9879 train_time:232509ms step_avg:396.10ms
step:598/800 train_loss:3.9289 train_time:232902ms step_avg:396.09ms
step:599/800 train_loss:3.9938 train_time:233296ms step_avg:396.09ms
step:600/800 train_loss:3.8147 train_time:233690ms step_avg:396.08ms
step:601/800 train_loss:3.9331 train_time:234084ms step_avg:396.08ms
step:602/800 train_loss:3.9649 train_time:234482ms step_avg:396.08ms
step:603/800 train_loss:3.9790 train_time:234875ms step_avg:396.08ms
step:604/800 train_loss:4.1136 train_time:235271ms step_avg:396.08ms
step:605/800 train_loss:3.9770 train_time:235665ms step_avg:396.08ms
step:606/800 train_loss:3.9520 train_time:236060ms step_avg:396.07ms
step:607/800 train_loss:3.8854 train_time:236454ms step_avg:396.07ms
step:608/800 train_loss:4.1344 train_time:236848ms step_avg:396.07ms
step:609/800 train_loss:3.9787 train_time:237244ms step_avg:396.07ms
step:610/800 train_loss:3.9471 train_time:237638ms step_avg:396.06ms
step:611/800 train_loss:4.0592 train_time:238032ms step_avg:396.06ms
step:612/800 train_loss:3.9651 train_time:238425ms step_avg:396.05ms
step:613/800 train_loss:3.9341 train_time:238819ms step_avg:396.05ms
step:614/800 train_loss:4.0994 train_time:239213ms step_avg:396.05ms
step:615/800 train_loss:4.0611 train_time:239607ms step_avg:396.04ms
step:616/800 train_loss:4.0255 train_time:240000ms step_avg:396.04ms
step:617/800 train_loss:3.9460 train_time:240393ms step_avg:396.03ms
step:618/800 train_loss:3.8984 train_time:240789ms step_avg:396.03ms
step:619/800 train_loss:4.0054 train_time:241184ms step_avg:396.03ms
step:620/800 train_loss:3.9181 train_time:241580ms step_avg:396.03ms
step:621/800 train_loss:3.9225 train_time:241974ms step_avg:396.03ms
step:622/800 train_loss:4.2200 train_time:242369ms step_avg:396.03ms
step:623/800 train_loss:3.9247 train_time:242763ms step_avg:396.03ms
step:624/800 train_loss:3.9534 train_time:243156ms step_avg:396.02ms
step:625/800 train_loss:4.0367 train_time:243554ms step_avg:396.02ms
step:625/800 val_loss:3.9613 train_time:243568ms step_avg:396.05ms
step:626/800 train_loss:4.0621 train_time:243952ms step_avg:396.03ms
step:627/800 train_loss:4.0818 train_time:244345ms step_avg:396.02ms
step:628/800 train_loss:4.0550 train_time:244739ms step_avg:396.02ms
step:629/800 train_loss:4.1095 train_time:245133ms step_avg:396.01ms
step:630/800 train_loss:3.9229 train_time:245528ms step_avg:396.01ms
step:631/800 train_loss:4.0492 train_time:245922ms step_avg:396.01ms
step:632/800 train_loss:4.0954 train_time:246318ms step_avg:396.01ms
step:633/800 train_loss:3.9931 train_time:246712ms step_avg:396.01ms
step:634/800 train_loss:3.9167 train_time:247107ms step_avg:396.00ms
step:635/800 train_loss:4.0131 train_time:247502ms step_avg:396.00ms
step:636/800 train_loss:4.2682 train_time:247897ms step_avg:396.00ms
step:637/800 train_loss:3.8636 train_time:248291ms step_avg:396.00ms
step:638/800 train_loss:3.6704 train_time:248690ms step_avg:396.00ms
step:639/800 train_loss:3.9152 train_time:249083ms step_avg:396.00ms
step:640/800 train_loss:3.9444 train_time:249475ms step_avg:395.99ms
step:641/800 train_loss:3.9179 train_time:249868ms step_avg:395.99ms
step:642/800 train_loss:3.9121 train_time:250264ms step_avg:395.99ms
step:643/800 train_loss:3.9578 train_time:250659ms step_avg:395.99ms
step:644/800 train_loss:3.9756 train_time:251052ms step_avg:395.98ms
step:645/800 train_loss:3.8918 train_time:251446ms step_avg:395.98ms
step:646/800 train_loss:4.1137 train_time:251840ms step_avg:395.98ms
step:647/800 train_loss:4.0036 train_time:252235ms step_avg:395.97ms
step:648/800 train_loss:4.0037 train_time:252629ms step_avg:395.97ms
step:649/800 train_loss:4.0171 train_time:253023ms step_avg:395.97ms
step:650/800 train_loss:4.0864 train_time:253417ms step_avg:395.96ms
step:651/800 train_loss:3.9486 train_time:253811ms step_avg:395.96ms
step:652/800 train_loss:4.0890 train_time:254206ms step_avg:395.96ms
step:653/800 train_loss:3.9206 train_time:254601ms step_avg:395.96ms
step:654/800 train_loss:3.9967 train_time:254994ms step_avg:395.95ms
step:655/800 train_loss:3.7554 train_time:255392ms step_avg:395.96ms
step:656/800 train_loss:3.9068 train_time:255789ms step_avg:395.96ms
step:657/800 train_loss:3.9159 train_time:256183ms step_avg:395.96ms
step:658/800 train_loss:3.8490 train_time:256576ms step_avg:395.95ms
step:659/800 train_loss:4.0256 train_time:256972ms step_avg:395.95ms
step:660/800 train_loss:3.9261 train_time:257366ms step_avg:395.95ms
step:661/800 train_loss:4.0075 train_time:257760ms step_avg:395.94ms
step:662/800 train_loss:4.0815 train_time:258153ms step_avg:395.94ms
step:663/800 train_loss:3.9924 train_time:258548ms step_avg:395.94ms
step:664/800 train_loss:3.8756 train_time:258941ms step_avg:395.93ms
step:665/800 train_loss:3.9550 train_time:259336ms step_avg:395.93ms
step:666/800 train_loss:3.8263 train_time:259728ms step_avg:395.93ms
step:667/800 train_loss:4.1226 train_time:260125ms step_avg:395.93ms
step:668/800 train_loss:3.9663 train_time:260519ms step_avg:395.93ms
step:669/800 train_loss:3.9530 train_time:260915ms step_avg:395.93ms
step:670/800 train_loss:3.8065 train_time:261309ms step_avg:395.92ms
step:671/800 train_loss:3.9224 train_time:261703ms step_avg:395.92ms
step:672/800 train_loss:3.8857 train_time:262098ms step_avg:395.92ms
step:673/800 train_loss:3.9095 train_time:262493ms step_avg:395.92ms
step:674/800 train_loss:4.1908 train_time:262890ms step_avg:395.92ms
step:675/800 train_loss:3.9824 train_time:263283ms step_avg:395.91ms
step:676/800 train_loss:4.0514 train_time:263678ms step_avg:395.91ms
step:677/800 train_loss:3.8141 train_time:264072ms step_avg:395.91ms
step:678/800 train_loss:3.9205 train_time:264464ms step_avg:395.90ms
step:679/800 train_loss:3.8702 train_time:264859ms step_avg:395.90ms
step:680/800 train_loss:4.0125 train_time:265255ms step_avg:395.90ms
step:681/800 train_loss:3.9194 train_time:265647ms step_avg:395.90ms
step:682/800 train_loss:3.9513 train_time:266042ms step_avg:395.90ms
step:683/800 train_loss:4.0151 train_time:266436ms step_avg:395.89ms
step:684/800 train_loss:4.0613 train_time:266830ms step_avg:395.89ms
step:685/800 train_loss:3.9625 train_time:267223ms step_avg:395.89ms
step:686/800 train_loss:4.0385 train_time:267617ms step_avg:395.88ms
step:687/800 train_loss:3.9625 train_time:268011ms step_avg:395.88ms
step:688/800 train_loss:4.0079 train_time:268406ms step_avg:395.88ms
step:689/800 train_loss:3.6187 train_time:268800ms step_avg:395.88ms
step:690/800 train_loss:3.7452 train_time:269193ms step_avg:395.87ms
step:691/800 train_loss:3.8830 train_time:269590ms step_avg:395.87ms
step:692/800 train_loss:3.7703 train_time:269988ms step_avg:395.88ms
step:693/800 train_loss:3.9824 train_time:270382ms step_avg:395.87ms
step:694/800 train_loss:3.9973 train_time:270775ms step_avg:395.87ms
step:695/800 train_loss:3.8841 train_time:271170ms step_avg:395.87ms
step:696/800 train_loss:3.8736 train_time:271566ms step_avg:395.87ms
step:697/800 train_loss:4.1732 train_time:271961ms step_avg:395.87ms
step:698/800 train_loss:3.9356 train_time:272355ms step_avg:395.86ms
step:699/800 train_loss:3.9679 train_time:272747ms step_avg:395.86ms
step:700/800 train_loss:4.1376 train_time:273141ms step_avg:395.86ms
step:701/800 train_loss:3.9097 train_time:273537ms step_avg:395.86ms
step:702/800 train_loss:3.8572 train_time:273931ms step_avg:395.85ms
step:703/800 train_loss:3.8607 train_time:274324ms step_avg:395.85ms
step:704/800 train_loss:3.8022 train_time:274718ms step_avg:395.85ms
step:705/800 train_loss:3.8922 train_time:275112ms step_avg:395.84ms
step:706/800 train_loss:3.8855 train_time:275506ms step_avg:395.84ms
step:707/800 train_loss:3.9088 train_time:275900ms step_avg:395.84ms
step:708/800 train_loss:3.9745 train_time:276295ms step_avg:395.84ms
step:709/800 train_loss:3.9183 train_time:276691ms step_avg:395.84ms
step:710/800 train_loss:3.9026 train_time:277088ms step_avg:395.84ms
step:711/800 train_loss:3.8752 train_time:277482ms step_avg:395.84ms
step:712/800 train_loss:3.9223 train_time:277876ms step_avg:395.83ms
step:713/800 train_loss:3.9825 train_time:278270ms step_avg:395.83ms
step:714/800 train_loss:3.9880 train_time:278662ms step_avg:395.83ms
step:715/800 train_loss:3.8957 train_time:279056ms step_avg:395.82ms
step:716/800 train_loss:3.9091 train_time:279450ms step_avg:395.82ms
step:717/800 train_loss:3.9226 train_time:279843ms step_avg:395.82ms
step:718/800 train_loss:4.0525 train_time:280236ms step_avg:395.81ms
step:719/800 train_loss:3.9300 train_time:280631ms step_avg:395.81ms
step:720/800 train_loss:3.9898 train_time:281025ms step_avg:395.81ms
step:721/800 train_loss:4.1642 train_time:281420ms step_avg:395.81ms
step:722/800 train_loss:3.7921 train_time:281814ms step_avg:395.81ms
step:723/800 train_loss:4.0504 train_time:282209ms step_avg:395.80ms
step:724/800 train_loss:4.1173 train_time:282602ms step_avg:395.80ms
step:725/800 train_loss:3.8861 train_time:283000ms step_avg:395.80ms
step:726/800 train_loss:3.9789 train_time:283394ms step_avg:395.80ms
step:727/800 train_loss:3.8816 train_time:283791ms step_avg:395.80ms
step:728/800 train_loss:3.8784 train_time:284190ms step_avg:395.81ms
step:729/800 train_loss:4.0563 train_time:284590ms step_avg:395.81ms
step:730/800 train_loss:4.0135 train_time:284983ms step_avg:395.81ms
step:731/800 train_loss:4.0162 train_time:285376ms step_avg:395.81ms
step:732/800 train_loss:3.8960 train_time:285772ms step_avg:395.81ms
step:733/800 train_loss:3.9220 train_time:286166ms step_avg:395.80ms
step:734/800 train_loss:4.1569 train_time:286561ms step_avg:395.80ms
step:735/800 train_loss:3.8752 train_time:286956ms step_avg:395.80ms
step:736/800 train_loss:3.9531 train_time:287352ms step_avg:395.80ms
step:737/800 train_loss:4.0768 train_time:287745ms step_avg:395.80ms
step:738/800 train_loss:3.9824 train_time:288139ms step_avg:395.80ms
step:739/800 train_loss:3.9304 train_time:288534ms step_avg:395.79ms
step:740/800 train_loss:3.8352 train_time:288927ms step_avg:395.79ms
step:741/800 train_loss:4.4762 train_time:289321ms step_avg:395.79ms
step:742/800 train_loss:3.8398 train_time:289716ms step_avg:395.79ms
step:743/800 train_loss:3.9192 train_time:290111ms step_avg:395.79ms
step:744/800 train_loss:3.9126 train_time:290505ms step_avg:395.78ms
step:745/800 train_loss:3.9715 train_time:290899ms step_avg:395.78ms
step:746/800 train_loss:3.9462 train_time:291293ms step_avg:395.78ms
step:747/800 train_loss:3.9307 train_time:291691ms step_avg:395.78ms
step:748/800 train_loss:3.9619 train_time:292088ms step_avg:395.78ms
step:749/800 train_loss:3.8815 train_time:292482ms step_avg:395.78ms
step:750/800 train_loss:3.9043 train_time:292878ms step_avg:395.78ms
step:750/800 val_loss:3.9053 train_time:292892ms step_avg:395.80ms
step:751/800 train_loss:3.9439 train_time:293276ms step_avg:395.78ms
step:752/800 train_loss:3.8882 train_time:293669ms step_avg:395.78ms
step:753/800 train_loss:3.9322 train_time:294063ms step_avg:395.78ms
step:754/800 train_loss:3.9541 train_time:294459ms step_avg:395.78ms
step:755/800 train_loss:3.9174 train_time:294854ms step_avg:395.78ms
step:756/800 train_loss:4.0036 train_time:295911ms step_avg:396.66ms
step:757/800 train_loss:3.8373 train_time:296309ms step_avg:396.66ms
step:758/800 train_loss:4.0593 train_time:296702ms step_avg:396.66ms
step:759/800 train_loss:3.9708 train_time:297100ms step_avg:396.66ms
step:760/800 train_loss:3.9072 train_time:297624ms step_avg:396.83ms
step:761/800 train_loss:4.0043 train_time:298019ms step_avg:396.83ms
step:762/800 train_loss:3.7317 train_time:298412ms step_avg:396.82ms
step:763/800 train_loss:3.8952 train_time:298805ms step_avg:396.82ms
step:764/800 train_loss:3.9981 train_time:299201ms step_avg:396.82ms
step:765/800 train_loss:3.6439 train_time:299599ms step_avg:396.82ms
step:766/800 train_loss:4.0836 train_time:299996ms step_avg:396.82ms
step:767/800 train_loss:3.9322 train_time:300391ms step_avg:396.82ms
step:768/800 train_loss:3.8871 train_time:300785ms step_avg:396.81ms
step:769/800 train_loss:3.9127 train_time:301177ms step_avg:396.81ms
step:770/800 train_loss:3.9332 train_time:301572ms step_avg:396.80ms
step:771/800 train_loss:3.9954 train_time:301966ms step_avg:396.80ms
step:772/800 train_loss:4.2126 train_time:302359ms step_avg:396.80ms
step:773/800 train_loss:3.7906 train_time:302753ms step_avg:396.79ms
step:774/800 train_loss:3.9983 train_time:303147ms step_avg:396.79ms
step:775/800 train_loss:3.9767 train_time:303542ms step_avg:396.79ms
step:776/800 train_loss:3.9382 train_time:303935ms step_avg:396.78ms
step:777/800 train_loss:3.7460 train_time:304328ms step_avg:396.78ms
step:778/800 train_loss:3.7452 train_time:304721ms step_avg:396.77ms
step:779/800 train_loss:3.8141 train_time:305116ms step_avg:396.77ms
step:780/800 train_loss:3.9027 train_time:305511ms step_avg:396.77ms
step:781/800 train_loss:3.9415 train_time:305906ms step_avg:396.77ms
step:782/800 train_loss:3.9968 train_time:306301ms step_avg:396.76ms
step:783/800 train_loss:3.8925 train_time:306699ms step_avg:396.76ms
step:784/800 train_loss:3.9218 train_time:307096ms step_avg:396.77ms
step:785/800 train_loss:3.9054 train_time:307489ms step_avg:396.76ms
step:786/800 train_loss:3.8988 train_time:307885ms step_avg:396.76ms
step:787/800 train_loss:3.8000 train_time:308280ms step_avg:396.76ms
step:788/800 train_loss:4.0532 train_time:308675ms step_avg:396.75ms
step:789/800 train_loss:3.8459 train_time:309068ms step_avg:396.75ms
step:790/800 train_loss:3.9156 train_time:309463ms step_avg:396.75ms
step:791/800 train_loss:3.9732 train_time:309855ms step_avg:396.74ms
step:792/800 train_loss:4.1055 train_time:310249ms step_avg:396.74ms
step:793/800 train_loss:4.1083 train_time:310643ms step_avg:396.73ms
step:794/800 train_loss:3.8417 train_time:311037ms step_avg:396.73ms
step:795/800 train_loss:3.9461 train_time:311432ms step_avg:396.73ms
step:796/800 train_loss:3.9809 train_time:311824ms step_avg:396.72ms
step:797/800 train_loss:4.0979 train_time:312221ms step_avg:396.72ms
step:798/800 train_loss:3.8582 train_time:312615ms step_avg:396.72ms
step:799/800 train_loss:4.0071 train_time:313009ms step_avg:396.72ms
step:800/800 train_loss:3.9118 train_time:313403ms step_avg:396.71ms
step:800/800 val_loss:3.8971 train_time:313422ms step_avg:396.74ms
