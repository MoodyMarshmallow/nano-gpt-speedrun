====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import json
import dataclasses
import subprocess
import csv
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

def _apply_gate_act(logits: torch.Tensor, kind: str) -> torch.Tensor:
    if kind == "sigmoid":
        return torch.sigmoid(logits)
    if kind == "ns_sigmoid":
        return 0.5 + 0.5 * torch.sigmoid(logits)
    raise ValueError(f"unknown gate_act={kind}")

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.attn_gate = getattr(config, "attn_gate", "none")
        self.gate_pos = getattr(config, "gate_pos", "sdpa")
        self.gate_act = getattr(config, "gate_act", "sigmoid")
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.attn_gate == "headwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_head, bias=False)
            self.gate_param = None
        elif self.attn_gate == "elementwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_embd, bias=False)
            self.gate_param = None
        elif self.attn_gate == "const":
            self.c_gate = None
            self.gate_param = nn.Parameter(torch.zeros(self.n_head, self.head_dim))
        else:
            self.c_gate = None
            self.gate_param = None

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if self.attn_gate != "none" and self.gate_pos == "value":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            v = v * gate
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2) # (B, T, n_head, head_dim)
        if self.attn_gate != "none" and self.gate_pos == "sdpa":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            y = y * gate
        y = y.contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attn_gate : str = "none"
    gate_pos : str = "sdpa"
    gate_act : str = "sigmoid"

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    seed : int = 1337
    attn_gate : str = "none" # none|headwise|elementwise|const
    gate_pos : str = "sdpa" # sdpa|value
    gate_act : str = "sigmoid" # sigmoid|ns_sigmoid
    early_stop_patience : int = 0 # 0 disables early stopping
    early_stop_min_delta : float = 0.0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

def apply_env_overrides():
    # environment-variable overrides allow quick sweeps without editing code
    args.learning_rate = float(os.environ.get("LR", args.learning_rate))
    args.seed = int(os.environ.get("SEED", args.seed))
    args.warmup_iters = int(os.environ.get("WARMUP_ITERS", args.warmup_iters))
    args.num_iterations = int(os.environ.get("NUM_ITERATIONS", args.num_iterations))
    args.early_stop_patience = int(os.environ.get("EARLY_STOP_PATIENCE", args.early_stop_patience))
    args.early_stop_min_delta = float(os.environ.get("EARLY_STOP_MIN_DELTA", args.early_stop_min_delta))
    args.attn_gate = os.environ.get("ATTNGATE", args.attn_gate)
    args.gate_pos = os.environ.get("GATEPOS", args.gate_pos)
    args.gate_act = os.environ.get("GATEACT", args.gate_act)
    args.num_iterations = int(os.environ.get("NUM_ITER", args.num_iterations))
    args.val_loss_every = int(os.environ.get("VAL_EVERY", args.val_loss_every))

def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

apply_env_overrides()
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
np.random.seed(args.seed)

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
git_commit = get_git_commit() if master_process else "unknown"

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    attn_gate=args.attn_gate,
    gate_pos=args.gate_pos,
    gate_act=args.gate_act,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        f.write(f"git_commit: {git_commit}\n")
        f.write(f"seed: {args.seed}\n")
        f.write("hyperparameters:\n")
        f.write(json.dumps(dataclasses.asdict(args), indent=2))
        f.write("\n")
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
best_val_loss = float("inf")
final_val_loss = None
no_improve_count = 0
early_stop_reason = None
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        if not torch.isfinite(val_loss):
            early_stop_reason = "non-finite val_loss"
            final_val_loss = float("nan")
            val_loss_item = float("nan")
        else:
            val_loss_item = val_loss.item()
            final_val_loss = val_loss_item
            if val_loss_item < best_val_loss - args.early_stop_min_delta:
                best_val_loss = val_loss_item
                no_improve_count = 0
            else:
                no_improve_count += 1
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if early_stop_reason is None and args.early_stop_patience > 0 and no_improve_count >= args.early_stop_patience:
                early_stop_reason = f"early_stop patience={args.early_stop_patience} min_delta={args.early_stop_min_delta}"
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process in a subdir to keep logs tidy
        ckpt_dir = os.path.join(logdir, "checkpoints")
        os.makedirs(ckpt_dir, exist_ok=True)
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, os.path.join(ckpt_dir, f"state_step{step:06d}.pt"))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")
    if early_stop_reason is not None:
        break

if master_process:
    if early_stop_reason is not None:
        print(f"stopped early: {early_stop_reason}")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    timed_steps_final = max(args.num_iterations - 9, 1)
    ms_per_step = training_time_ms / timed_steps_final
    os.makedirs("experiments", exist_ok=True)
    results_path = os.path.join("experiments", "results.csv")
    fieldnames = [
        "run_id",
        "date",
        "git_commit",
        "seed",
        "attn_gate",
        "gate_pos",
        "gate_act",
        "learning_rate",
        "batch_size",
        "device_batch_size",
        "sequence_length",
        "num_iterations",
        "warmdown_iters",
        "final_val_loss",
        "best_val_loss",
        "train_time_ms",
        "ms_per_step",
        "gpu_name",
        "n_gpus",
        "runpod_instance",
        "notes",
    ]
    final_loss_value = final_val_loss if final_val_loss is not None else float("nan")
    best_loss_value = best_val_loss if best_val_loss < float("inf") else float("nan")
    row = {
        "run_id": run_id,
        "date": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        "git_commit": git_commit,
        "seed": args.seed,
        "attn_gate": args.attn_gate,
        "gate_pos": args.gate_pos,
        "gate_act": args.gate_act,
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "device_batch_size": args.device_batch_size,
        "sequence_length": args.sequence_length,
        "num_iterations": args.num_iterations,
        "warmdown_iters": args.warmdown_iters,
        "final_val_loss": final_loss_value,
        "best_val_loss": best_loss_value,
        "train_time_ms": training_time_ms,
        "ms_per_step": ms_per_step,
        "gpu_name": torch.cuda.get_device_name(ddp_local_rank),
        "n_gpus": ddp_world_size,
        "runpod_instance": os.environ.get("RUNPOD_INSTANCE_TYPE", "unknown"),
        "notes": "",
    }
    write_header = not os.path.exists(results_path)
    with open(results_path, "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerow(row)
====================================================================================================
git_commit: bf6d1ff4b5e4ce7280e822ab33e90c4fb250e604
seed: 2337
hyperparameters:
{
  "input_bin": "data/fineweb10B/fineweb_train_*.bin",
  "input_val_bin": "data/fineweb10B/fineweb_val_*.bin",
  "batch_size": 512,
  "device_batch_size": 64,
  "sequence_length": 1024,
  "num_iterations": 800,
  "learning_rate": 0.00432,
  "warmup_iters": 150,
  "warmdown_iters": 1450,
  "weight_decay": 0,
  "seed": 2337,
  "attn_gate": "elementwise",
  "gate_pos": "sdpa",
  "gate_act": "sigmoid",
  "early_stop_patience": 2,
  "early_stop_min_delta": 0.0,
  "val_loss_every": 125,
  "val_tokens": 10485760,
  "save_every": 0
}
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Sun Dec  7 18:18:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:00:07.0 Off |                    0 |
| N/A   47C    P0            115W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:00:08.0 Off |                    0 |
| N/A   48C    P0            119W /  300W |    2276MiB /  81920MiB |     19%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0            139W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000000:00:0A.0 Off |                    0 |
| N/A   45C    P0            142W /  300W |    2276MiB /  81920MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  |   00000000:00:0B.0 Off |                    0 |
| N/A   47C    P0            114W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  |   00000000:00:0C.0 Off |                    0 |
| N/A   45C    P0            140W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0            119W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  |   00000000:00:0E.0 Off |                    0 |
| N/A   47C    P0            126W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/800 val_loss:16.0092 train_time:249ms step_avg:nanms
step:1/800 train_loss:16.0082 train_time:44052ms step_avg:nanms
step:2/800 train_loss:15.8682 train_time:44869ms step_avg:nanms
step:3/800 train_loss:15.5036 train_time:45284ms step_avg:nanms
step:4/800 train_loss:14.7536 train_time:45698ms step_avg:nanms
step:5/800 train_loss:13.3143 train_time:46113ms step_avg:nanms
step:6/800 train_loss:11.5518 train_time:46528ms step_avg:nanms
step:7/800 train_loss:10.1300 train_time:46947ms step_avg:nanms
step:8/800 train_loss:9.7139 train_time:47364ms step_avg:nanms
step:9/800 train_loss:9.5278 train_time:47779ms step_avg:nanms
step:10/800 train_loss:9.3434 train_time:48196ms step_avg:nanms
step:11/800 train_loss:9.1394 train_time:401ms step_avg:nanms
step:12/800 train_loss:8.9608 train_time:816ms step_avg:nanms
step:13/800 train_loss:8.6867 train_time:1229ms step_avg:409.83ms
step:14/800 train_loss:8.5392 train_time:1645ms step_avg:411.35ms
step:15/800 train_loss:8.3514 train_time:2070ms step_avg:414.06ms
step:16/800 train_loss:8.1544 train_time:2485ms step_avg:414.11ms
step:17/800 train_loss:7.9970 train_time:2898ms step_avg:414.05ms
step:18/800 train_loss:7.8571 train_time:3311ms step_avg:413.86ms
step:19/800 train_loss:7.6234 train_time:3727ms step_avg:414.07ms
step:20/800 train_loss:7.5313 train_time:4140ms step_avg:414.03ms
step:21/800 train_loss:7.1564 train_time:4555ms step_avg:414.12ms
step:22/800 train_loss:7.4456 train_time:4970ms step_avg:414.20ms
step:23/800 train_loss:7.6108 train_time:5384ms step_avg:414.15ms
step:24/800 train_loss:7.2701 train_time:5802ms step_avg:414.44ms
step:25/800 train_loss:7.3526 train_time:6216ms step_avg:414.43ms
step:26/800 train_loss:7.1098 train_time:6630ms step_avg:414.41ms
step:27/800 train_loss:7.0002 train_time:7046ms step_avg:414.49ms
step:28/800 train_loss:7.1402 train_time:7462ms step_avg:414.53ms
step:29/800 train_loss:6.7909 train_time:7877ms step_avg:414.59ms
step:30/800 train_loss:7.0145 train_time:8295ms step_avg:414.73ms
step:31/800 train_loss:6.8655 train_time:8710ms step_avg:414.77ms
step:32/800 train_loss:6.7887 train_time:9127ms step_avg:414.86ms
step:33/800 train_loss:6.5691 train_time:9544ms step_avg:414.96ms
step:34/800 train_loss:6.9975 train_time:9960ms step_avg:414.99ms
step:35/800 train_loss:6.7665 train_time:10377ms step_avg:415.10ms
step:36/800 train_loss:6.9596 train_time:10794ms step_avg:415.17ms
step:37/800 train_loss:6.8396 train_time:11210ms step_avg:415.20ms
step:38/800 train_loss:6.7118 train_time:11626ms step_avg:415.21ms
step:39/800 train_loss:6.5697 train_time:12043ms step_avg:415.29ms
step:40/800 train_loss:6.6666 train_time:12459ms step_avg:415.32ms
step:41/800 train_loss:6.5367 train_time:12876ms step_avg:415.36ms
step:42/800 train_loss:6.5709 train_time:13291ms step_avg:415.34ms
step:43/800 train_loss:6.4000 train_time:13707ms step_avg:415.37ms
step:44/800 train_loss:6.4957 train_time:14125ms step_avg:415.44ms
step:45/800 train_loss:6.4778 train_time:14543ms step_avg:415.50ms
step:46/800 train_loss:6.6876 train_time:14960ms step_avg:415.56ms
step:47/800 train_loss:6.4847 train_time:15378ms step_avg:415.61ms
step:48/800 train_loss:6.3164 train_time:15794ms step_avg:415.63ms
step:49/800 train_loss:6.5613 train_time:16211ms step_avg:415.67ms
step:50/800 train_loss:6.4206 train_time:16628ms step_avg:415.69ms
step:51/800 train_loss:6.5732 train_time:17045ms step_avg:415.74ms
step:52/800 train_loss:6.4207 train_time:17463ms step_avg:415.80ms
step:53/800 train_loss:6.2552 train_time:17881ms step_avg:415.83ms
step:54/800 train_loss:6.3834 train_time:18298ms step_avg:415.87ms
step:55/800 train_loss:6.3138 train_time:18715ms step_avg:415.90ms
step:56/800 train_loss:6.6227 train_time:19133ms step_avg:415.94ms
step:57/800 train_loss:6.2904 train_time:19549ms step_avg:415.94ms
step:58/800 train_loss:6.1636 train_time:19967ms step_avg:415.98ms
step:59/800 train_loss:6.3455 train_time:20385ms step_avg:416.03ms
step:60/800 train_loss:6.2527 train_time:20803ms step_avg:416.05ms
step:61/800 train_loss:6.3657 train_time:21220ms step_avg:416.09ms
step:62/800 train_loss:6.1661 train_time:21640ms step_avg:416.16ms
step:63/800 train_loss:6.2493 train_time:22057ms step_avg:416.18ms
step:64/800 train_loss:6.2007 train_time:22475ms step_avg:416.20ms
step:65/800 train_loss:6.6117 train_time:22892ms step_avg:416.21ms
step:66/800 train_loss:6.0382 train_time:23309ms step_avg:416.24ms
step:67/800 train_loss:6.2044 train_time:23727ms step_avg:416.27ms
step:68/800 train_loss:6.0605 train_time:24145ms step_avg:416.30ms
step:69/800 train_loss:6.3857 train_time:24563ms step_avg:416.33ms
step:70/800 train_loss:5.9866 train_time:24982ms step_avg:416.37ms
step:71/800 train_loss:6.0382 train_time:25401ms step_avg:416.42ms
step:72/800 train_loss:6.2401 train_time:25819ms step_avg:416.43ms
step:73/800 train_loss:6.1519 train_time:26239ms step_avg:416.49ms
step:74/800 train_loss:6.0537 train_time:26657ms step_avg:416.51ms
step:75/800 train_loss:6.1541 train_time:27075ms step_avg:416.54ms
step:76/800 train_loss:6.0913 train_time:27494ms step_avg:416.57ms
step:77/800 train_loss:6.1001 train_time:27913ms step_avg:416.61ms
step:78/800 train_loss:6.1587 train_time:28331ms step_avg:416.64ms
step:79/800 train_loss:6.2407 train_time:28751ms step_avg:416.68ms
step:80/800 train_loss:6.0566 train_time:29170ms step_avg:416.71ms
step:81/800 train_loss:6.1618 train_time:29588ms step_avg:416.73ms
step:82/800 train_loss:5.8821 train_time:30006ms step_avg:416.76ms
step:83/800 train_loss:6.0739 train_time:30426ms step_avg:416.79ms
step:84/800 train_loss:6.0535 train_time:30853ms step_avg:416.93ms
step:85/800 train_loss:5.9807 train_time:31271ms step_avg:416.95ms
step:86/800 train_loss:5.8430 train_time:31690ms step_avg:416.98ms
step:87/800 train_loss:6.0532 train_time:32110ms step_avg:417.02ms
step:88/800 train_loss:5.9549 train_time:32528ms step_avg:417.02ms
step:89/800 train_loss:6.0363 train_time:32946ms step_avg:417.04ms
step:90/800 train_loss:6.0163 train_time:33365ms step_avg:417.06ms
step:91/800 train_loss:5.9123 train_time:33783ms step_avg:417.08ms
step:92/800 train_loss:5.9086 train_time:34199ms step_avg:417.06ms
step:93/800 train_loss:6.0190 train_time:34618ms step_avg:417.08ms
step:94/800 train_loss:5.8796 train_time:35040ms step_avg:417.14ms
step:95/800 train_loss:5.8464 train_time:35458ms step_avg:417.15ms
step:96/800 train_loss:5.8600 train_time:35875ms step_avg:417.15ms
step:97/800 train_loss:5.7703 train_time:36293ms step_avg:417.16ms
step:98/800 train_loss:5.8552 train_time:36711ms step_avg:417.17ms
step:99/800 train_loss:5.7521 train_time:37131ms step_avg:417.20ms
step:100/800 train_loss:5.8919 train_time:37551ms step_avg:417.23ms
step:101/800 train_loss:5.8373 train_time:37970ms step_avg:417.25ms
step:102/800 train_loss:5.7225 train_time:38386ms step_avg:417.24ms
step:103/800 train_loss:5.8468 train_time:38806ms step_avg:417.27ms
step:104/800 train_loss:5.8126 train_time:39224ms step_avg:417.27ms
step:105/800 train_loss:5.6202 train_time:39641ms step_avg:417.28ms
step:106/800 train_loss:5.7512 train_time:40061ms step_avg:417.30ms
step:107/800 train_loss:5.9698 train_time:40478ms step_avg:417.29ms
step:108/800 train_loss:5.7466 train_time:40894ms step_avg:417.29ms
step:109/800 train_loss:5.4648 train_time:41314ms step_avg:417.31ms
step:110/800 train_loss:5.7060 train_time:41732ms step_avg:417.32ms
step:111/800 train_loss:5.6553 train_time:42149ms step_avg:417.32ms
step:112/800 train_loss:5.6289 train_time:42569ms step_avg:417.34ms
step:113/800 train_loss:5.7296 train_time:42986ms step_avg:417.34ms
step:114/800 train_loss:5.6579 train_time:43403ms step_avg:417.34ms
step:115/800 train_loss:5.4943 train_time:43823ms step_avg:417.37ms
step:116/800 train_loss:5.6842 train_time:44245ms step_avg:417.40ms
step:117/800 train_loss:5.5071 train_time:44663ms step_avg:417.42ms
step:118/800 train_loss:5.5039 train_time:45081ms step_avg:417.42ms
step:119/800 train_loss:5.5956 train_time:45500ms step_avg:417.43ms
step:120/800 train_loss:5.6353 train_time:45917ms step_avg:417.42ms
step:121/800 train_loss:5.5422 train_time:46335ms step_avg:417.43ms
step:122/800 train_loss:5.4242 train_time:46752ms step_avg:417.43ms
step:123/800 train_loss:5.5070 train_time:47171ms step_avg:417.44ms
step:124/800 train_loss:5.3569 train_time:47591ms step_avg:417.47ms
step:125/800 train_loss:5.6790 train_time:48008ms step_avg:417.46ms
step:125/800 val_loss:5.5042 train_time:48021ms step_avg:417.57ms
step:126/800 train_loss:5.5211 train_time:48430ms step_avg:417.50ms
step:127/800 train_loss:5.4907 train_time:48850ms step_avg:417.52ms
step:128/800 train_loss:5.5650 train_time:49269ms step_avg:417.54ms
step:129/800 train_loss:5.4139 train_time:49687ms step_avg:417.54ms
step:130/800 train_loss:5.6877 train_time:50106ms step_avg:417.55ms
step:131/800 train_loss:5.4742 train_time:50525ms step_avg:417.56ms
step:132/800 train_loss:5.4819 train_time:50943ms step_avg:417.57ms
step:133/800 train_loss:5.4056 train_time:51361ms step_avg:417.57ms
step:134/800 train_loss:5.4387 train_time:51779ms step_avg:417.57ms
step:135/800 train_loss:5.3868 train_time:52198ms step_avg:417.59ms
step:136/800 train_loss:5.4398 train_time:52616ms step_avg:417.59ms
step:137/800 train_loss:5.2336 train_time:53036ms step_avg:417.61ms
step:138/800 train_loss:5.3874 train_time:53453ms step_avg:417.60ms
step:139/800 train_loss:5.3715 train_time:53871ms step_avg:417.60ms
step:140/800 train_loss:5.3735 train_time:54288ms step_avg:417.60ms
step:141/800 train_loss:5.3995 train_time:54710ms step_avg:417.63ms
step:142/800 train_loss:5.2954 train_time:55128ms step_avg:417.64ms
step:143/800 train_loss:5.3798 train_time:55545ms step_avg:417.64ms
step:144/800 train_loss:5.1734 train_time:55962ms step_avg:417.63ms
step:145/800 train_loss:5.3471 train_time:56380ms step_avg:417.63ms
step:146/800 train_loss:5.2714 train_time:56797ms step_avg:417.63ms
step:147/800 train_loss:5.1776 train_time:57215ms step_avg:417.63ms
step:148/800 train_loss:5.3138 train_time:57634ms step_avg:417.64ms
step:149/800 train_loss:5.2802 train_time:58052ms step_avg:417.64ms
step:150/800 train_loss:5.3436 train_time:58471ms step_avg:417.65ms
step:151/800 train_loss:5.3514 train_time:58889ms step_avg:417.65ms
step:152/800 train_loss:5.2361 train_time:59309ms step_avg:417.67ms
step:153/800 train_loss:5.2082 train_time:59726ms step_avg:417.66ms
step:154/800 train_loss:5.2750 train_time:60145ms step_avg:417.67ms
step:155/800 train_loss:5.2040 train_time:60563ms step_avg:417.67ms
step:156/800 train_loss:5.1794 train_time:60981ms step_avg:417.68ms
step:157/800 train_loss:5.1834 train_time:61399ms step_avg:417.68ms
step:158/800 train_loss:5.3165 train_time:61817ms step_avg:417.68ms
step:159/800 train_loss:5.0882 train_time:62237ms step_avg:417.70ms
step:160/800 train_loss:5.1480 train_time:62655ms step_avg:417.70ms
step:161/800 train_loss:4.9981 train_time:63075ms step_avg:417.71ms
step:162/800 train_loss:5.1513 train_time:63494ms step_avg:417.72ms
step:163/800 train_loss:5.1867 train_time:63915ms step_avg:417.75ms
step:164/800 train_loss:5.1765 train_time:64334ms step_avg:417.75ms
step:165/800 train_loss:4.9836 train_time:64752ms step_avg:417.75ms
step:166/800 train_loss:5.1029 train_time:65169ms step_avg:417.75ms
step:167/800 train_loss:5.2622 train_time:65586ms step_avg:417.74ms
step:168/800 train_loss:5.0371 train_time:66006ms step_avg:417.76ms
step:169/800 train_loss:5.1276 train_time:66423ms step_avg:417.75ms
step:170/800 train_loss:4.9780 train_time:66842ms step_avg:417.76ms
step:171/800 train_loss:4.9156 train_time:67261ms step_avg:417.77ms
step:172/800 train_loss:5.0292 train_time:67679ms step_avg:417.77ms
step:173/800 train_loss:4.9992 train_time:68097ms step_avg:417.77ms
step:174/800 train_loss:5.0548 train_time:68515ms step_avg:417.78ms
step:175/800 train_loss:5.1987 train_time:68934ms step_avg:417.78ms
step:176/800 train_loss:5.0797 train_time:69351ms step_avg:417.78ms
step:177/800 train_loss:4.9157 train_time:69771ms step_avg:417.79ms
step:178/800 train_loss:4.8899 train_time:70187ms step_avg:417.78ms
step:179/800 train_loss:4.9300 train_time:70604ms step_avg:417.78ms
step:180/800 train_loss:4.9713 train_time:71023ms step_avg:417.79ms
step:181/800 train_loss:4.9487 train_time:71442ms step_avg:417.79ms
step:182/800 train_loss:5.0745 train_time:71860ms step_avg:417.79ms
step:183/800 train_loss:4.9528 train_time:72279ms step_avg:417.80ms
step:184/800 train_loss:4.8828 train_time:72696ms step_avg:417.80ms
step:185/800 train_loss:4.9086 train_time:73116ms step_avg:417.80ms
step:186/800 train_loss:5.0334 train_time:73534ms step_avg:417.81ms
step:187/800 train_loss:4.9202 train_time:73953ms step_avg:417.81ms
step:188/800 train_loss:5.1728 train_time:74371ms step_avg:417.81ms
step:189/800 train_loss:4.9535 train_time:74895ms step_avg:418.41ms
step:190/800 train_loss:4.8653 train_time:75446ms step_avg:419.15ms
step:191/800 train_loss:5.0269 train_time:75864ms step_avg:419.14ms
step:192/800 train_loss:4.8600 train_time:76284ms step_avg:419.14ms
step:193/800 train_loss:4.7846 train_time:76701ms step_avg:419.13ms
step:194/800 train_loss:4.9925 train_time:77120ms step_avg:419.13ms
step:195/800 train_loss:4.9236 train_time:77537ms step_avg:419.12ms
step:196/800 train_loss:5.1173 train_time:77954ms step_avg:419.11ms
step:197/800 train_loss:4.9989 train_time:78373ms step_avg:419.10ms
step:198/800 train_loss:4.8284 train_time:78794ms step_avg:419.12ms
step:199/800 train_loss:4.8762 train_time:79212ms step_avg:419.11ms
step:200/800 train_loss:4.7678 train_time:79630ms step_avg:419.11ms
step:201/800 train_loss:4.8532 train_time:80048ms step_avg:419.10ms
step:202/800 train_loss:4.7808 train_time:80465ms step_avg:419.09ms
step:203/800 train_loss:5.0101 train_time:80882ms step_avg:419.08ms
step:204/800 train_loss:4.8957 train_time:81301ms step_avg:419.08ms
step:205/800 train_loss:4.8691 train_time:81719ms step_avg:419.07ms
step:206/800 train_loss:5.0230 train_time:82139ms step_avg:419.07ms
step:207/800 train_loss:4.6963 train_time:82557ms step_avg:419.07ms
step:208/800 train_loss:4.8446 train_time:82975ms step_avg:419.07ms
step:209/800 train_loss:4.8004 train_time:83395ms step_avg:419.07ms
step:210/800 train_loss:4.9683 train_time:83814ms step_avg:419.07ms
step:211/800 train_loss:4.8816 train_time:84232ms step_avg:419.06ms
step:212/800 train_loss:4.7683 train_time:84649ms step_avg:419.06ms
step:213/800 train_loss:4.9286 train_time:85066ms step_avg:419.04ms
step:214/800 train_loss:4.7387 train_time:85484ms step_avg:419.04ms
step:215/800 train_loss:4.8277 train_time:85902ms step_avg:419.04ms
step:216/800 train_loss:4.6817 train_time:86319ms step_avg:419.03ms
step:217/800 train_loss:4.8219 train_time:86738ms step_avg:419.02ms
step:218/800 train_loss:4.7938 train_time:87155ms step_avg:419.01ms
step:219/800 train_loss:4.7598 train_time:87574ms step_avg:419.01ms
step:220/800 train_loss:4.7627 train_time:87993ms step_avg:419.01ms
step:221/800 train_loss:4.8000 train_time:88413ms step_avg:419.02ms
step:222/800 train_loss:4.8445 train_time:88832ms step_avg:419.02ms
step:223/800 train_loss:4.7712 train_time:89251ms step_avg:419.02ms
step:224/800 train_loss:4.7882 train_time:89669ms step_avg:419.02ms
step:225/800 train_loss:4.9115 train_time:90088ms step_avg:419.01ms
step:226/800 train_loss:4.6506 train_time:90509ms step_avg:419.02ms
step:227/800 train_loss:4.6754 train_time:90927ms step_avg:419.02ms
step:228/800 train_loss:4.6636 train_time:91346ms step_avg:419.02ms
step:229/800 train_loss:4.8241 train_time:91765ms step_avg:419.02ms
step:230/800 train_loss:4.6569 train_time:92184ms step_avg:419.02ms
step:231/800 train_loss:4.8088 train_time:92603ms step_avg:419.02ms
step:232/800 train_loss:4.6703 train_time:93021ms step_avg:419.01ms
step:233/800 train_loss:4.6329 train_time:93439ms step_avg:419.01ms
step:234/800 train_loss:4.8422 train_time:93855ms step_avg:419.00ms
step:235/800 train_loss:4.6714 train_time:94275ms step_avg:419.00ms
step:236/800 train_loss:4.6074 train_time:94695ms step_avg:419.00ms
step:237/800 train_loss:4.8577 train_time:95114ms step_avg:419.01ms
step:238/800 train_loss:4.7470 train_time:95532ms step_avg:419.00ms
step:239/800 train_loss:4.6576 train_time:95950ms step_avg:418.99ms
step:240/800 train_loss:4.7976 train_time:96368ms step_avg:418.99ms
step:241/800 train_loss:4.7845 train_time:96786ms step_avg:418.99ms
step:242/800 train_loss:4.6846 train_time:97203ms step_avg:418.98ms
step:243/800 train_loss:4.8516 train_time:97620ms step_avg:418.97ms
step:244/800 train_loss:4.6712 train_time:98039ms step_avg:418.97ms
step:245/800 train_loss:4.6841 train_time:98459ms step_avg:418.97ms
step:246/800 train_loss:4.7548 train_time:98879ms step_avg:418.98ms
step:247/800 train_loss:4.7093 train_time:99297ms step_avg:418.98ms
step:248/800 train_loss:4.6628 train_time:99714ms step_avg:418.97ms
step:249/800 train_loss:4.8267 train_time:100134ms step_avg:418.97ms
step:250/800 train_loss:4.5611 train_time:100551ms step_avg:418.96ms
step:250/800 val_loss:4.6748 train_time:100565ms step_avg:419.02ms
step:251/800 train_loss:4.6081 train_time:100973ms step_avg:418.97ms
step:252/800 train_loss:4.7373 train_time:101391ms step_avg:418.97ms
step:253/800 train_loss:4.7411 train_time:101809ms step_avg:418.97ms
step:254/800 train_loss:4.6057 train_time:102227ms step_avg:418.96ms
step:255/800 train_loss:4.6238 train_time:102647ms step_avg:418.97ms
step:256/800 train_loss:4.7695 train_time:103063ms step_avg:418.95ms
step:257/800 train_loss:4.7107 train_time:103483ms step_avg:418.96ms
step:258/800 train_loss:4.6697 train_time:103902ms step_avg:418.96ms
step:259/800 train_loss:4.5993 train_time:104321ms step_avg:418.96ms
step:260/800 train_loss:4.6170 train_time:104740ms step_avg:418.96ms
step:261/800 train_loss:4.6909 train_time:105159ms step_avg:418.96ms
step:262/800 train_loss:4.7030 train_time:105579ms step_avg:418.96ms
step:263/800 train_loss:4.6011 train_time:105998ms step_avg:418.96ms
step:264/800 train_loss:4.5415 train_time:106416ms step_avg:418.96ms
step:265/800 train_loss:4.5977 train_time:106835ms step_avg:418.96ms
step:266/800 train_loss:4.4567 train_time:107253ms step_avg:418.96ms
step:267/800 train_loss:4.5120 train_time:107670ms step_avg:418.95ms
step:268/800 train_loss:4.5563 train_time:108089ms step_avg:418.95ms
step:269/800 train_loss:4.5109 train_time:108507ms step_avg:418.95ms
step:270/800 train_loss:4.4694 train_time:108926ms step_avg:418.95ms
step:271/800 train_loss:4.7062 train_time:109344ms step_avg:418.94ms
step:272/800 train_loss:4.6275 train_time:109763ms step_avg:418.94ms
step:273/800 train_loss:4.4864 train_time:110184ms step_avg:418.95ms
step:274/800 train_loss:4.5364 train_time:110602ms step_avg:418.95ms
step:275/800 train_loss:4.6587 train_time:111021ms step_avg:418.95ms
step:276/800 train_loss:4.6753 train_time:111438ms step_avg:418.94ms
step:277/800 train_loss:4.8769 train_time:111857ms step_avg:418.94ms
step:278/800 train_loss:4.6179 train_time:112273ms step_avg:418.93ms
step:279/800 train_loss:4.7460 train_time:112692ms step_avg:418.93ms
step:280/800 train_loss:4.5921 train_time:113113ms step_avg:418.94ms
step:281/800 train_loss:4.6568 train_time:113532ms step_avg:418.94ms
step:282/800 train_loss:4.5548 train_time:113951ms step_avg:418.94ms
step:283/800 train_loss:4.6540 train_time:114370ms step_avg:418.94ms
step:284/800 train_loss:4.4835 train_time:114791ms step_avg:418.94ms
step:285/800 train_loss:4.6518 train_time:115209ms step_avg:418.94ms
step:286/800 train_loss:4.6419 train_time:115626ms step_avg:418.93ms
step:287/800 train_loss:4.6850 train_time:116046ms step_avg:418.94ms
step:288/800 train_loss:4.5336 train_time:116463ms step_avg:418.93ms
step:289/800 train_loss:4.6017 train_time:116885ms step_avg:418.94ms
step:290/800 train_loss:4.4629 train_time:117305ms step_avg:418.95ms
step:291/800 train_loss:4.4570 train_time:117723ms step_avg:418.94ms
step:292/800 train_loss:4.5571 train_time:118141ms step_avg:418.94ms
step:293/800 train_loss:4.4603 train_time:118560ms step_avg:418.94ms
step:294/800 train_loss:4.5212 train_time:118977ms step_avg:418.93ms
step:295/800 train_loss:4.5271 train_time:119395ms step_avg:418.93ms
step:296/800 train_loss:4.4011 train_time:119813ms step_avg:418.93ms
step:297/800 train_loss:4.3861 train_time:120232ms step_avg:418.93ms
step:298/800 train_loss:4.4171 train_time:120650ms step_avg:418.92ms
step:299/800 train_loss:4.5253 train_time:121067ms step_avg:418.92ms
step:300/800 train_loss:4.3995 train_time:121485ms step_avg:418.92ms
step:301/800 train_loss:4.5846 train_time:121903ms step_avg:418.91ms
step:302/800 train_loss:4.5620 train_time:122320ms step_avg:418.90ms
step:303/800 train_loss:4.4728 train_time:122739ms step_avg:418.91ms
step:304/800 train_loss:4.5540 train_time:123159ms step_avg:418.91ms
step:305/800 train_loss:4.5288 train_time:123577ms step_avg:418.90ms
step:306/800 train_loss:5.0081 train_time:123997ms step_avg:418.91ms
step:307/800 train_loss:4.4805 train_time:124416ms step_avg:418.91ms
step:308/800 train_loss:4.3901 train_time:124832ms step_avg:418.90ms
step:309/800 train_loss:4.5712 train_time:125252ms step_avg:418.90ms
step:310/800 train_loss:4.3697 train_time:125670ms step_avg:418.90ms
step:311/800 train_loss:4.6137 train_time:126088ms step_avg:418.90ms
step:312/800 train_loss:4.5116 train_time:126507ms step_avg:418.90ms
step:313/800 train_loss:4.4202 train_time:126925ms step_avg:418.89ms
step:314/800 train_loss:4.5473 train_time:127342ms step_avg:418.89ms
step:315/800 train_loss:4.6862 train_time:127759ms step_avg:418.88ms
step:316/800 train_loss:4.5145 train_time:128177ms step_avg:418.88ms
step:317/800 train_loss:4.3994 train_time:128595ms step_avg:418.88ms
step:318/800 train_loss:4.4079 train_time:129012ms step_avg:418.87ms
step:319/800 train_loss:4.4346 train_time:129430ms step_avg:418.87ms
step:320/800 train_loss:4.3862 train_time:129848ms step_avg:418.86ms
step:321/800 train_loss:4.4826 train_time:130266ms step_avg:418.86ms
step:322/800 train_loss:4.4851 train_time:130687ms step_avg:418.87ms
step:323/800 train_loss:4.4468 train_time:131105ms step_avg:418.87ms
step:324/800 train_loss:4.5252 train_time:131523ms step_avg:418.86ms
step:325/800 train_loss:4.4974 train_time:131943ms step_avg:418.87ms
step:326/800 train_loss:4.5749 train_time:132360ms step_avg:418.86ms
step:327/800 train_loss:4.4256 train_time:132777ms step_avg:418.85ms
step:328/800 train_loss:4.8969 train_time:133195ms step_avg:418.85ms
step:329/800 train_loss:4.5880 train_time:133613ms step_avg:418.85ms
step:330/800 train_loss:4.3536 train_time:134030ms step_avg:418.84ms
step:331/800 train_loss:4.3181 train_time:134449ms step_avg:418.84ms
step:332/800 train_loss:4.4921 train_time:134868ms step_avg:418.84ms
step:333/800 train_loss:4.4080 train_time:135286ms step_avg:418.84ms
step:334/800 train_loss:4.3940 train_time:135704ms step_avg:418.84ms
step:335/800 train_loss:4.3487 train_time:136123ms step_avg:418.84ms
step:336/800 train_loss:4.5373 train_time:136540ms step_avg:418.84ms
step:337/800 train_loss:4.4683 train_time:136960ms step_avg:418.84ms
step:338/800 train_loss:5.0171 train_time:137378ms step_avg:418.83ms
step:339/800 train_loss:4.4473 train_time:137795ms step_avg:418.83ms
step:340/800 train_loss:4.4114 train_time:138211ms step_avg:418.82ms
step:341/800 train_loss:4.4033 train_time:138631ms step_avg:418.82ms
step:342/800 train_loss:4.3411 train_time:139048ms step_avg:418.82ms
step:343/800 train_loss:4.3132 train_time:139465ms step_avg:418.81ms
step:344/800 train_loss:4.3756 train_time:139887ms step_avg:418.82ms
step:345/800 train_loss:4.4763 train_time:140306ms step_avg:418.82ms
step:346/800 train_loss:4.3406 train_time:140723ms step_avg:418.82ms
step:347/800 train_loss:4.2870 train_time:141142ms step_avg:418.82ms
step:348/800 train_loss:4.3325 train_time:141561ms step_avg:418.82ms
step:349/800 train_loss:4.3378 train_time:141980ms step_avg:418.82ms
step:350/800 train_loss:4.2823 train_time:142397ms step_avg:418.82ms
step:351/800 train_loss:3.9766 train_time:142816ms step_avg:418.81ms
step:352/800 train_loss:4.2575 train_time:143235ms step_avg:418.81ms
step:353/800 train_loss:4.6257 train_time:143652ms step_avg:418.81ms
step:354/800 train_loss:4.1338 train_time:144071ms step_avg:418.81ms
step:355/800 train_loss:4.3852 train_time:144490ms step_avg:418.81ms
step:356/800 train_loss:4.2796 train_time:144908ms step_avg:418.81ms
step:357/800 train_loss:4.3756 train_time:145328ms step_avg:418.81ms
step:358/800 train_loss:4.3791 train_time:145745ms step_avg:418.81ms
step:359/800 train_loss:4.3029 train_time:146163ms step_avg:418.80ms
step:360/800 train_loss:4.5589 train_time:146584ms step_avg:418.81ms
step:361/800 train_loss:3.9971 train_time:147002ms step_avg:418.81ms
step:362/800 train_loss:4.4943 train_time:147421ms step_avg:418.81ms
step:363/800 train_loss:4.3925 train_time:147847ms step_avg:418.83ms
step:364/800 train_loss:4.2861 train_time:148265ms step_avg:418.83ms
step:365/800 train_loss:4.2148 train_time:148686ms step_avg:418.83ms
step:366/800 train_loss:4.3828 train_time:149106ms step_avg:418.84ms
step:367/800 train_loss:4.3168 train_time:149524ms step_avg:418.83ms
step:368/800 train_loss:4.2978 train_time:149943ms step_avg:418.83ms
step:369/800 train_loss:4.2934 train_time:150363ms step_avg:418.84ms
step:370/800 train_loss:4.1884 train_time:150784ms step_avg:418.84ms
step:371/800 train_loss:4.3315 train_time:151201ms step_avg:418.84ms
step:372/800 train_loss:4.2514 train_time:151618ms step_avg:418.83ms
step:373/800 train_loss:4.1414 train_time:152037ms step_avg:418.83ms
step:374/800 train_loss:4.3409 train_time:152457ms step_avg:418.84ms
step:375/800 train_loss:4.2743 train_time:152875ms step_avg:418.83ms
step:375/800 val_loss:4.2802 train_time:152888ms step_avg:418.87ms
step:376/800 train_loss:4.2519 train_time:153298ms step_avg:418.85ms
step:377/800 train_loss:4.3154 train_time:153716ms step_avg:418.84ms
step:378/800 train_loss:4.2144 train_time:154236ms step_avg:419.12ms
step:379/800 train_loss:4.2761 train_time:154658ms step_avg:419.13ms
step:380/800 train_loss:4.3273 train_time:155205ms step_avg:419.47ms
step:381/800 train_loss:4.3755 train_time:155622ms step_avg:419.47ms
step:382/800 train_loss:4.2977 train_time:156041ms step_avg:419.47ms
step:383/800 train_loss:4.2752 train_time:156459ms step_avg:419.46ms
step:384/800 train_loss:4.1999 train_time:156876ms step_avg:419.45ms
step:385/800 train_loss:4.2890 train_time:157294ms step_avg:419.45ms
step:386/800 train_loss:4.2068 train_time:157711ms step_avg:419.45ms
step:387/800 train_loss:4.3317 train_time:158130ms step_avg:419.44ms
step:388/800 train_loss:4.5296 train_time:158549ms step_avg:419.44ms
step:389/800 train_loss:4.2241 train_time:158966ms step_avg:419.43ms
step:390/800 train_loss:4.2003 train_time:159384ms step_avg:419.43ms
step:391/800 train_loss:4.3167 train_time:159802ms step_avg:419.43ms
step:392/800 train_loss:4.2284 train_time:160218ms step_avg:419.42ms
step:393/800 train_loss:4.3376 train_time:160637ms step_avg:419.42ms
step:394/800 train_loss:4.1622 train_time:161057ms step_avg:419.42ms
step:395/800 train_loss:4.3024 train_time:161473ms step_avg:419.41ms
step:396/800 train_loss:4.0660 train_time:161891ms step_avg:419.41ms
step:397/800 train_loss:4.2459 train_time:162309ms step_avg:419.40ms
step:398/800 train_loss:4.3211 train_time:162725ms step_avg:419.40ms
step:399/800 train_loss:4.2896 train_time:163144ms step_avg:419.39ms
step:400/800 train_loss:4.2010 train_time:163563ms step_avg:419.39ms
step:401/800 train_loss:4.2608 train_time:163981ms step_avg:419.39ms
step:402/800 train_loss:4.3058 train_time:164398ms step_avg:419.38ms
step:403/800 train_loss:4.2621 train_time:164815ms step_avg:419.38ms
step:404/800 train_loss:4.3635 train_time:165232ms step_avg:419.37ms
step:405/800 train_loss:4.1309 train_time:165653ms step_avg:419.37ms
step:406/800 train_loss:4.1927 train_time:166072ms step_avg:419.37ms
step:407/800 train_loss:4.4760 train_time:166491ms step_avg:419.37ms
step:408/800 train_loss:4.2236 train_time:166910ms step_avg:419.37ms
step:409/800 train_loss:4.2226 train_time:167327ms step_avg:419.37ms
step:410/800 train_loss:4.2685 train_time:167746ms step_avg:419.36ms
step:411/800 train_loss:4.1467 train_time:168164ms step_avg:419.36ms
step:412/800 train_loss:4.1703 train_time:168581ms step_avg:419.36ms
step:413/800 train_loss:4.5840 train_time:169000ms step_avg:419.36ms
step:414/800 train_loss:4.0358 train_time:169419ms step_avg:419.35ms
step:415/800 train_loss:4.4103 train_time:169837ms step_avg:419.35ms
step:416/800 train_loss:4.1660 train_time:170256ms step_avg:419.35ms
step:417/800 train_loss:4.1689 train_time:170673ms step_avg:419.34ms
step:418/800 train_loss:4.3566 train_time:171091ms step_avg:419.34ms
step:419/800 train_loss:4.0861 train_time:171508ms step_avg:419.34ms
step:420/800 train_loss:4.1845 train_time:171926ms step_avg:419.33ms
step:421/800 train_loss:4.1378 train_time:172344ms step_avg:419.33ms
step:422/800 train_loss:4.0419 train_time:172761ms step_avg:419.32ms
step:423/800 train_loss:4.1610 train_time:173178ms step_avg:419.32ms
step:424/800 train_loss:4.2619 train_time:173597ms step_avg:419.32ms
step:425/800 train_loss:4.0394 train_time:174014ms step_avg:419.31ms
step:426/800 train_loss:4.2093 train_time:174433ms step_avg:419.31ms
step:427/800 train_loss:4.0933 train_time:174853ms step_avg:419.31ms
step:428/800 train_loss:4.2939 train_time:175271ms step_avg:419.31ms
step:429/800 train_loss:4.2137 train_time:175688ms step_avg:419.30ms
step:430/800 train_loss:4.1410 train_time:176107ms step_avg:419.30ms
step:431/800 train_loss:4.1149 train_time:176525ms step_avg:419.30ms
step:432/800 train_loss:4.0413 train_time:176944ms step_avg:419.30ms
step:433/800 train_loss:4.1453 train_time:177362ms step_avg:419.30ms
step:434/800 train_loss:4.2154 train_time:177780ms step_avg:419.29ms
step:435/800 train_loss:4.1504 train_time:178198ms step_avg:419.29ms
step:436/800 train_loss:4.1970 train_time:178616ms step_avg:419.29ms
step:437/800 train_loss:4.2121 train_time:179034ms step_avg:419.28ms
step:438/800 train_loss:4.0892 train_time:179454ms step_avg:419.28ms
step:439/800 train_loss:4.1108 train_time:179872ms step_avg:419.28ms
step:440/800 train_loss:4.0872 train_time:180291ms step_avg:419.28ms
step:441/800 train_loss:4.2639 train_time:180707ms step_avg:419.27ms
step:442/800 train_loss:4.1568 train_time:181126ms step_avg:419.27ms
step:443/800 train_loss:4.1382 train_time:181544ms step_avg:419.27ms
step:444/800 train_loss:4.0259 train_time:181962ms step_avg:419.27ms
step:445/800 train_loss:4.2855 train_time:182381ms step_avg:419.27ms
step:446/800 train_loss:4.2189 train_time:182798ms step_avg:419.26ms
step:447/800 train_loss:4.2156 train_time:183217ms step_avg:419.26ms
step:448/800 train_loss:4.1256 train_time:183635ms step_avg:419.26ms
step:449/800 train_loss:4.2232 train_time:184055ms step_avg:419.26ms
step:450/800 train_loss:4.0452 train_time:184473ms step_avg:419.26ms
step:451/800 train_loss:4.0874 train_time:184890ms step_avg:419.25ms
step:452/800 train_loss:3.9666 train_time:185308ms step_avg:419.25ms
step:453/800 train_loss:4.0731 train_time:185726ms step_avg:419.25ms
step:454/800 train_loss:4.0559 train_time:186144ms step_avg:419.24ms
step:455/800 train_loss:4.0102 train_time:186561ms step_avg:419.24ms
step:456/800 train_loss:4.2273 train_time:186981ms step_avg:419.24ms
step:457/800 train_loss:4.0878 train_time:187400ms step_avg:419.24ms
step:458/800 train_loss:4.1689 train_time:187818ms step_avg:419.24ms
step:459/800 train_loss:4.1998 train_time:188235ms step_avg:419.23ms
step:460/800 train_loss:4.0048 train_time:188655ms step_avg:419.23ms
step:461/800 train_loss:4.1785 train_time:189073ms step_avg:419.23ms
step:462/800 train_loss:4.0725 train_time:189490ms step_avg:419.23ms
step:463/800 train_loss:4.0744 train_time:189908ms step_avg:419.22ms
step:464/800 train_loss:4.1484 train_time:190326ms step_avg:419.22ms
step:465/800 train_loss:4.0885 train_time:190742ms step_avg:419.21ms
step:466/800 train_loss:4.0926 train_time:191160ms step_avg:419.21ms
step:467/800 train_loss:4.2066 train_time:191580ms step_avg:419.21ms
step:468/800 train_loss:4.2029 train_time:191998ms step_avg:419.21ms
step:469/800 train_loss:4.1759 train_time:192415ms step_avg:419.20ms
step:470/800 train_loss:4.0748 train_time:192833ms step_avg:419.20ms
step:471/800 train_loss:4.1487 train_time:193253ms step_avg:419.20ms
step:472/800 train_loss:4.2026 train_time:193673ms step_avg:419.21ms
step:473/800 train_loss:4.1279 train_time:194091ms step_avg:419.20ms
step:474/800 train_loss:4.0934 train_time:194510ms step_avg:419.20ms
step:475/800 train_loss:3.9487 train_time:194930ms step_avg:419.20ms
step:476/800 train_loss:4.3813 train_time:195347ms step_avg:419.20ms
step:477/800 train_loss:4.1430 train_time:195765ms step_avg:419.20ms
step:478/800 train_loss:3.9436 train_time:196183ms step_avg:419.19ms
step:479/800 train_loss:4.1651 train_time:196601ms step_avg:419.19ms
step:480/800 train_loss:4.1335 train_time:197019ms step_avg:419.19ms
step:481/800 train_loss:4.2750 train_time:197436ms step_avg:419.18ms
step:482/800 train_loss:4.0872 train_time:197855ms step_avg:419.18ms
step:483/800 train_loss:3.8908 train_time:198273ms step_avg:419.18ms
step:484/800 train_loss:4.1763 train_time:198691ms step_avg:419.18ms
step:485/800 train_loss:4.0230 train_time:199110ms step_avg:419.18ms
step:486/800 train_loss:4.0351 train_time:199526ms step_avg:419.17ms
step:487/800 train_loss:3.9731 train_time:199945ms step_avg:419.17ms
step:488/800 train_loss:4.0284 train_time:200362ms step_avg:419.17ms
step:489/800 train_loss:4.2280 train_time:200781ms step_avg:419.17ms
step:490/800 train_loss:4.0767 train_time:201199ms step_avg:419.16ms
step:491/800 train_loss:3.9677 train_time:201617ms step_avg:419.16ms
step:492/800 train_loss:3.9769 train_time:202035ms step_avg:419.16ms
step:493/800 train_loss:4.0918 train_time:202457ms step_avg:419.17ms
step:494/800 train_loss:3.9386 train_time:202875ms step_avg:419.16ms
step:495/800 train_loss:4.0813 train_time:203293ms step_avg:419.16ms
step:496/800 train_loss:4.0097 train_time:203711ms step_avg:419.16ms
step:497/800 train_loss:3.9070 train_time:204129ms step_avg:419.16ms
step:498/800 train_loss:4.0853 train_time:204546ms step_avg:419.15ms
step:499/800 train_loss:4.1706 train_time:204965ms step_avg:419.15ms
step:500/800 train_loss:4.2052 train_time:205383ms step_avg:419.15ms
step:500/800 val_loss:4.0684 train_time:205396ms step_avg:419.18ms
step:501/800 train_loss:4.1041 train_time:205803ms step_avg:419.15ms
step:502/800 train_loss:4.1564 train_time:206224ms step_avg:419.16ms
step:503/800 train_loss:4.0992 train_time:206642ms step_avg:419.15ms
step:504/800 train_loss:4.1333 train_time:207061ms step_avg:419.15ms
step:505/800 train_loss:4.0920 train_time:207478ms step_avg:419.15ms
step:506/800 train_loss:4.1831 train_time:207893ms step_avg:419.14ms
step:507/800 train_loss:3.9819 train_time:208312ms step_avg:419.14ms
step:508/800 train_loss:4.1132 train_time:208730ms step_avg:419.14ms
step:509/800 train_loss:4.1945 train_time:209148ms step_avg:419.13ms
step:510/800 train_loss:4.1289 train_time:209566ms step_avg:419.13ms
step:511/800 train_loss:3.9436 train_time:209984ms step_avg:419.13ms
step:512/800 train_loss:4.1431 train_time:210399ms step_avg:419.12ms
step:513/800 train_loss:4.0704 train_time:210817ms step_avg:419.12ms
step:514/800 train_loss:4.0388 train_time:211235ms step_avg:419.12ms
step:515/800 train_loss:4.0985 train_time:211654ms step_avg:419.12ms
step:516/800 train_loss:4.1010 train_time:212070ms step_avg:419.11ms
step:517/800 train_loss:4.4331 train_time:212488ms step_avg:419.11ms
step:518/800 train_loss:4.0222 train_time:212906ms step_avg:419.11ms
step:519/800 train_loss:4.1466 train_time:213323ms step_avg:419.10ms
step:520/800 train_loss:4.0716 train_time:213741ms step_avg:419.10ms
step:521/800 train_loss:4.0397 train_time:214159ms step_avg:419.10ms
step:522/800 train_loss:3.9806 train_time:214576ms step_avg:419.09ms
step:523/800 train_loss:4.0021 train_time:214994ms step_avg:419.09ms
step:524/800 train_loss:4.6322 train_time:215411ms step_avg:419.09ms
step:525/800 train_loss:4.1015 train_time:215830ms step_avg:419.09ms
step:526/800 train_loss:4.0486 train_time:216248ms step_avg:419.08ms
step:527/800 train_loss:4.0421 train_time:216664ms step_avg:419.08ms
step:528/800 train_loss:4.0037 train_time:217083ms step_avg:419.08ms
step:529/800 train_loss:3.9745 train_time:217499ms step_avg:419.07ms
step:530/800 train_loss:4.1800 train_time:217917ms step_avg:419.07ms
step:531/800 train_loss:3.9923 train_time:218334ms step_avg:419.07ms
step:532/800 train_loss:4.2697 train_time:218754ms step_avg:419.07ms
step:533/800 train_loss:4.0705 train_time:219170ms step_avg:419.06ms
step:534/800 train_loss:4.0054 train_time:219588ms step_avg:419.06ms
step:535/800 train_loss:4.0289 train_time:220005ms step_avg:419.06ms
step:536/800 train_loss:3.9651 train_time:220424ms step_avg:419.06ms
step:537/800 train_loss:4.0816 train_time:220843ms step_avg:419.06ms
step:538/800 train_loss:4.0739 train_time:221260ms step_avg:419.05ms
step:539/800 train_loss:3.9835 train_time:221678ms step_avg:419.05ms
step:540/800 train_loss:4.4688 train_time:222096ms step_avg:419.05ms
step:541/800 train_loss:4.0123 train_time:222515ms step_avg:419.05ms
step:542/800 train_loss:4.1303 train_time:222933ms step_avg:419.05ms
step:543/800 train_loss:3.9605 train_time:223350ms step_avg:419.04ms
step:544/800 train_loss:3.9445 train_time:223769ms step_avg:419.04ms
step:545/800 train_loss:4.0261 train_time:224188ms step_avg:419.04ms
step:546/800 train_loss:3.9467 train_time:224604ms step_avg:419.04ms
step:547/800 train_loss:3.9980 train_time:225025ms step_avg:419.04ms
step:548/800 train_loss:4.0032 train_time:225443ms step_avg:419.04ms
step:549/800 train_loss:3.9739 train_time:225860ms step_avg:419.04ms
step:550/800 train_loss:4.0661 train_time:226278ms step_avg:419.03ms
step:551/800 train_loss:3.9429 train_time:226696ms step_avg:419.03ms
step:552/800 train_loss:3.9693 train_time:227115ms step_avg:419.03ms
step:553/800 train_loss:4.2923 train_time:227532ms step_avg:419.03ms
step:554/800 train_loss:4.0905 train_time:227952ms step_avg:419.03ms
step:555/800 train_loss:4.0528 train_time:228370ms step_avg:419.03ms
step:556/800 train_loss:4.0140 train_time:228787ms step_avg:419.02ms
step:557/800 train_loss:4.0323 train_time:229207ms step_avg:419.03ms
step:558/800 train_loss:3.7012 train_time:229625ms step_avg:419.02ms
step:559/800 train_loss:3.9487 train_time:230043ms step_avg:419.02ms
step:560/800 train_loss:3.9883 train_time:230462ms step_avg:419.02ms
step:561/800 train_loss:4.0341 train_time:230880ms step_avg:419.02ms
step:562/800 train_loss:3.9483 train_time:231297ms step_avg:419.02ms
step:563/800 train_loss:3.8945 train_time:231715ms step_avg:419.02ms
step:564/800 train_loss:4.0975 train_time:232132ms step_avg:419.01ms
step:565/800 train_loss:3.9107 train_time:232550ms step_avg:419.01ms
step:566/800 train_loss:4.0318 train_time:232968ms step_avg:419.01ms
step:567/800 train_loss:3.9845 train_time:233493ms step_avg:419.20ms
step:568/800 train_loss:3.9303 train_time:233913ms step_avg:419.20ms
step:569/800 train_loss:4.0237 train_time:234330ms step_avg:419.20ms
step:570/800 train_loss:3.9960 train_time:234874ms step_avg:419.42ms
step:571/800 train_loss:4.0186 train_time:235288ms step_avg:419.41ms
step:572/800 train_loss:4.1137 train_time:235707ms step_avg:419.41ms
step:573/800 train_loss:4.0418 train_time:236125ms step_avg:419.40ms
step:574/800 train_loss:4.0515 train_time:236542ms step_avg:419.40ms
step:575/800 train_loss:4.1118 train_time:236962ms step_avg:419.40ms
step:576/800 train_loss:4.0714 train_time:237378ms step_avg:419.40ms
step:577/800 train_loss:4.0749 train_time:237795ms step_avg:419.39ms
step:578/800 train_loss:4.0270 train_time:238213ms step_avg:419.39ms
step:579/800 train_loss:3.9974 train_time:238631ms step_avg:419.39ms
step:580/800 train_loss:3.9883 train_time:239048ms step_avg:419.38ms
step:581/800 train_loss:3.9417 train_time:239466ms step_avg:419.38ms
step:582/800 train_loss:3.9686 train_time:239883ms step_avg:419.38ms
step:583/800 train_loss:4.1958 train_time:240302ms step_avg:419.37ms
step:584/800 train_loss:3.9601 train_time:240723ms step_avg:419.38ms
step:585/800 train_loss:3.9246 train_time:241139ms step_avg:419.37ms
step:586/800 train_loss:4.1064 train_time:241556ms step_avg:419.37ms
step:587/800 train_loss:3.8657 train_time:241976ms step_avg:419.37ms
step:588/800 train_loss:3.9950 train_time:242393ms step_avg:419.37ms
step:589/800 train_loss:3.9997 train_time:242810ms step_avg:419.36ms
step:590/800 train_loss:4.3442 train_time:243226ms step_avg:419.36ms
step:591/800 train_loss:4.1126 train_time:243644ms step_avg:419.35ms
step:592/800 train_loss:3.8579 train_time:244061ms step_avg:419.35ms
step:593/800 train_loss:3.8682 train_time:244477ms step_avg:419.34ms
step:594/800 train_loss:3.8728 train_time:244895ms step_avg:419.34ms
step:595/800 train_loss:3.9011 train_time:245313ms step_avg:419.34ms
step:596/800 train_loss:4.2717 train_time:245729ms step_avg:419.33ms
step:597/800 train_loss:3.9861 train_time:246147ms step_avg:419.33ms
step:598/800 train_loss:3.9229 train_time:246564ms step_avg:419.33ms
step:599/800 train_loss:3.9897 train_time:246981ms step_avg:419.32ms
step:600/800 train_loss:3.8109 train_time:247398ms step_avg:419.32ms
step:601/800 train_loss:3.9346 train_time:247816ms step_avg:419.32ms
step:602/800 train_loss:3.9633 train_time:248233ms step_avg:419.31ms
step:603/800 train_loss:3.9776 train_time:248652ms step_avg:419.31ms
step:604/800 train_loss:4.1111 train_time:249070ms step_avg:419.31ms
step:605/800 train_loss:3.9816 train_time:249487ms step_avg:419.31ms
step:606/800 train_loss:3.9542 train_time:249905ms step_avg:419.30ms
step:607/800 train_loss:3.8814 train_time:250324ms step_avg:419.30ms
step:608/800 train_loss:4.1273 train_time:250742ms step_avg:419.30ms
step:609/800 train_loss:3.9754 train_time:251161ms step_avg:419.30ms
step:610/800 train_loss:3.9483 train_time:251578ms step_avg:419.30ms
step:611/800 train_loss:4.0571 train_time:251995ms step_avg:419.29ms
step:612/800 train_loss:3.9605 train_time:252413ms step_avg:419.29ms
step:613/800 train_loss:3.9332 train_time:252830ms step_avg:419.29ms
step:614/800 train_loss:4.0965 train_time:253248ms step_avg:419.29ms
step:615/800 train_loss:4.0628 train_time:253667ms step_avg:419.28ms
step:616/800 train_loss:4.0311 train_time:254085ms step_avg:419.28ms
step:617/800 train_loss:3.9452 train_time:254502ms step_avg:419.28ms
step:618/800 train_loss:3.9015 train_time:254924ms step_avg:419.28ms
step:619/800 train_loss:4.0060 train_time:255341ms step_avg:419.28ms
step:620/800 train_loss:3.9029 train_time:255759ms step_avg:419.28ms
step:621/800 train_loss:3.9248 train_time:256178ms step_avg:419.28ms
step:622/800 train_loss:4.2182 train_time:256596ms step_avg:419.28ms
step:623/800 train_loss:3.9272 train_time:257013ms step_avg:419.27ms
step:624/800 train_loss:3.9562 train_time:257431ms step_avg:419.27ms
step:625/800 train_loss:4.0339 train_time:257848ms step_avg:419.26ms
step:625/800 val_loss:3.9595 train_time:257862ms step_avg:419.29ms
step:626/800 train_loss:4.0621 train_time:258270ms step_avg:419.27ms
step:627/800 train_loss:4.0805 train_time:258690ms step_avg:419.27ms
step:628/800 train_loss:4.0556 train_time:259107ms step_avg:419.27ms
step:629/800 train_loss:4.1055 train_time:259524ms step_avg:419.26ms
step:630/800 train_loss:3.9167 train_time:259941ms step_avg:419.26ms
step:631/800 train_loss:4.0525 train_time:260360ms step_avg:419.26ms
step:632/800 train_loss:4.0886 train_time:260778ms step_avg:419.26ms
step:633/800 train_loss:3.9850 train_time:261194ms step_avg:419.25ms
step:634/800 train_loss:3.9112 train_time:261611ms step_avg:419.25ms
step:635/800 train_loss:4.0089 train_time:262028ms step_avg:419.24ms
step:636/800 train_loss:4.2649 train_time:262445ms step_avg:419.24ms
step:637/800 train_loss:3.8585 train_time:262862ms step_avg:419.24ms
step:638/800 train_loss:3.6850 train_time:263280ms step_avg:419.24ms
step:639/800 train_loss:3.9092 train_time:263697ms step_avg:419.23ms
step:640/800 train_loss:3.9423 train_time:264115ms step_avg:419.23ms
step:641/800 train_loss:3.9128 train_time:264532ms step_avg:419.23ms
step:642/800 train_loss:3.9102 train_time:264948ms step_avg:419.22ms
step:643/800 train_loss:3.9505 train_time:265365ms step_avg:419.22ms
step:644/800 train_loss:3.9736 train_time:265784ms step_avg:419.22ms
step:645/800 train_loss:3.8905 train_time:266202ms step_avg:419.22ms
step:646/800 train_loss:4.1080 train_time:266619ms step_avg:419.21ms
step:647/800 train_loss:3.9967 train_time:267036ms step_avg:419.21ms
step:648/800 train_loss:4.0029 train_time:267452ms step_avg:419.20ms
step:649/800 train_loss:4.0180 train_time:267869ms step_avg:419.20ms
step:650/800 train_loss:4.0864 train_time:268288ms step_avg:419.20ms
step:651/800 train_loss:3.9495 train_time:268704ms step_avg:419.20ms
step:652/800 train_loss:4.0887 train_time:269123ms step_avg:419.19ms
step:653/800 train_loss:3.9146 train_time:269540ms step_avg:419.19ms
step:654/800 train_loss:3.9900 train_time:269958ms step_avg:419.19ms
step:655/800 train_loss:3.7579 train_time:270376ms step_avg:419.19ms
step:656/800 train_loss:3.9079 train_time:270796ms step_avg:419.19ms
step:657/800 train_loss:3.9100 train_time:271213ms step_avg:419.19ms
step:658/800 train_loss:3.8498 train_time:271632ms step_avg:419.19ms
step:659/800 train_loss:4.0273 train_time:272050ms step_avg:419.18ms
step:660/800 train_loss:3.9212 train_time:272467ms step_avg:419.18ms
step:661/800 train_loss:4.0055 train_time:272884ms step_avg:419.18ms
step:662/800 train_loss:4.0818 train_time:273302ms step_avg:419.17ms
step:663/800 train_loss:3.9913 train_time:273719ms step_avg:419.17ms
step:664/800 train_loss:3.8776 train_time:274137ms step_avg:419.17ms
step:665/800 train_loss:3.9548 train_time:274555ms step_avg:419.17ms
step:666/800 train_loss:3.8252 train_time:274973ms step_avg:419.17ms
step:667/800 train_loss:4.1188 train_time:275392ms step_avg:419.17ms
step:668/800 train_loss:3.9509 train_time:275810ms step_avg:419.16ms
step:669/800 train_loss:3.9475 train_time:276228ms step_avg:419.16ms
step:670/800 train_loss:3.8117 train_time:276645ms step_avg:419.16ms
step:671/800 train_loss:3.9215 train_time:277063ms step_avg:419.16ms
step:672/800 train_loss:3.8767 train_time:277480ms step_avg:419.15ms
step:673/800 train_loss:3.9045 train_time:277896ms step_avg:419.15ms
step:674/800 train_loss:4.1904 train_time:278315ms step_avg:419.15ms
step:675/800 train_loss:3.9702 train_time:278734ms step_avg:419.15ms
step:676/800 train_loss:4.0425 train_time:279150ms step_avg:419.14ms
step:677/800 train_loss:3.8141 train_time:279569ms step_avg:419.14ms
step:678/800 train_loss:3.9171 train_time:279989ms step_avg:419.15ms
step:679/800 train_loss:3.8677 train_time:280404ms step_avg:419.14ms
step:680/800 train_loss:4.0084 train_time:280823ms step_avg:419.14ms
step:681/800 train_loss:3.9146 train_time:281241ms step_avg:419.14ms
step:682/800 train_loss:3.9409 train_time:281660ms step_avg:419.14ms
step:683/800 train_loss:4.0173 train_time:282077ms step_avg:419.13ms
step:684/800 train_loss:4.0618 train_time:282495ms step_avg:419.13ms
step:685/800 train_loss:3.9607 train_time:282914ms step_avg:419.13ms
step:686/800 train_loss:4.0314 train_time:283331ms step_avg:419.13ms
step:687/800 train_loss:3.9586 train_time:283749ms step_avg:419.13ms
step:688/800 train_loss:4.0105 train_time:284167ms step_avg:419.13ms
step:689/800 train_loss:3.6385 train_time:284584ms step_avg:419.12ms
step:690/800 train_loss:3.7488 train_time:285001ms step_avg:419.12ms
step:691/800 train_loss:3.8806 train_time:285419ms step_avg:419.12ms
step:692/800 train_loss:3.7653 train_time:285836ms step_avg:419.11ms
step:693/800 train_loss:3.9818 train_time:286255ms step_avg:419.11ms
step:694/800 train_loss:3.9998 train_time:286670ms step_avg:419.11ms
step:695/800 train_loss:3.8880 train_time:287091ms step_avg:419.11ms
step:696/800 train_loss:3.8706 train_time:287508ms step_avg:419.11ms
step:697/800 train_loss:4.1691 train_time:287928ms step_avg:419.11ms
step:698/800 train_loss:3.9338 train_time:288345ms step_avg:419.11ms
step:699/800 train_loss:3.9658 train_time:288763ms step_avg:419.10ms
step:700/800 train_loss:4.1356 train_time:289182ms step_avg:419.10ms
step:701/800 train_loss:3.9071 train_time:289599ms step_avg:419.10ms
step:702/800 train_loss:3.8554 train_time:290019ms step_avg:419.10ms
step:703/800 train_loss:3.8495 train_time:290436ms step_avg:419.10ms
step:704/800 train_loss:3.7987 train_time:290853ms step_avg:419.10ms
step:705/800 train_loss:3.8950 train_time:291271ms step_avg:419.09ms
step:706/800 train_loss:3.8838 train_time:291691ms step_avg:419.10ms
step:707/800 train_loss:3.9047 train_time:292107ms step_avg:419.09ms
step:708/800 train_loss:3.9715 train_time:292526ms step_avg:419.09ms
step:709/800 train_loss:3.9141 train_time:292944ms step_avg:419.09ms
step:710/800 train_loss:3.8996 train_time:293359ms step_avg:419.08ms
step:711/800 train_loss:3.8723 train_time:293778ms step_avg:419.08ms
step:712/800 train_loss:3.9159 train_time:294195ms step_avg:419.08ms
step:713/800 train_loss:3.9748 train_time:294612ms step_avg:419.08ms
step:714/800 train_loss:3.9800 train_time:295029ms step_avg:419.08ms
step:715/800 train_loss:3.8939 train_time:295448ms step_avg:419.08ms
step:716/800 train_loss:3.9006 train_time:295866ms step_avg:419.07ms
step:717/800 train_loss:3.9209 train_time:296284ms step_avg:419.07ms
step:718/800 train_loss:4.0508 train_time:296702ms step_avg:419.07ms
step:719/800 train_loss:3.9253 train_time:297118ms step_avg:419.07ms
step:720/800 train_loss:3.9894 train_time:297537ms step_avg:419.07ms
step:721/800 train_loss:4.1650 train_time:297954ms step_avg:419.06ms
step:722/800 train_loss:3.7894 train_time:298377ms step_avg:419.07ms
step:723/800 train_loss:4.0460 train_time:298808ms step_avg:419.09ms
step:724/800 train_loss:4.1080 train_time:299225ms step_avg:419.08ms
step:725/800 train_loss:3.8807 train_time:299642ms step_avg:419.08ms
step:726/800 train_loss:3.9730 train_time:300060ms step_avg:419.08ms
step:727/800 train_loss:3.8766 train_time:300477ms step_avg:419.08ms
step:728/800 train_loss:3.8787 train_time:300895ms step_avg:419.07ms
step:729/800 train_loss:4.0518 train_time:301311ms step_avg:419.07ms
step:730/800 train_loss:4.0100 train_time:301729ms step_avg:419.07ms
step:731/800 train_loss:4.0158 train_time:302149ms step_avg:419.07ms
step:732/800 train_loss:3.8914 train_time:302565ms step_avg:419.07ms
step:733/800 train_loss:3.9136 train_time:302983ms step_avg:419.06ms
step:734/800 train_loss:4.1516 train_time:303400ms step_avg:419.06ms
step:735/800 train_loss:3.8760 train_time:303818ms step_avg:419.06ms
step:736/800 train_loss:3.9501 train_time:304236ms step_avg:419.06ms
step:737/800 train_loss:4.0772 train_time:304654ms step_avg:419.06ms
step:738/800 train_loss:3.9780 train_time:305072ms step_avg:419.05ms
step:739/800 train_loss:3.9279 train_time:305492ms step_avg:419.06ms
step:740/800 train_loss:3.8285 train_time:305909ms step_avg:419.05ms
step:741/800 train_loss:4.4783 train_time:306327ms step_avg:419.05ms
step:742/800 train_loss:3.8348 train_time:306745ms step_avg:419.05ms
step:743/800 train_loss:3.9124 train_time:307161ms step_avg:419.05ms
step:744/800 train_loss:3.9105 train_time:307579ms step_avg:419.04ms
step:745/800 train_loss:3.9679 train_time:307995ms step_avg:419.04ms
step:746/800 train_loss:3.9529 train_time:308414ms step_avg:419.04ms
step:747/800 train_loss:3.9255 train_time:308830ms step_avg:419.04ms
step:748/800 train_loss:3.9593 train_time:309246ms step_avg:419.03ms
step:749/800 train_loss:3.8813 train_time:309664ms step_avg:419.03ms
step:750/800 train_loss:3.8924 train_time:310082ms step_avg:419.03ms
step:750/800 val_loss:3.9013 train_time:310096ms step_avg:419.05ms
step:751/800 train_loss:3.9386 train_time:310504ms step_avg:419.03ms
step:752/800 train_loss:3.8882 train_time:310922ms step_avg:419.03ms
step:753/800 train_loss:3.9240 train_time:311340ms step_avg:419.03ms
step:754/800 train_loss:3.9443 train_time:311760ms step_avg:419.03ms
step:755/800 train_loss:3.9129 train_time:312178ms step_avg:419.03ms
step:756/800 train_loss:4.0037 train_time:313126ms step_avg:419.74ms
step:757/800 train_loss:3.8317 train_time:313544ms step_avg:419.74ms
step:758/800 train_loss:4.0550 train_time:313960ms step_avg:419.73ms
step:759/800 train_loss:3.9668 train_time:314378ms step_avg:419.73ms
step:760/800 train_loss:3.9007 train_time:314913ms step_avg:419.88ms
step:761/800 train_loss:4.0035 train_time:315330ms step_avg:419.88ms
step:762/800 train_loss:3.7283 train_time:315748ms step_avg:419.88ms
step:763/800 train_loss:3.8971 train_time:316163ms step_avg:419.87ms
step:764/800 train_loss:3.9962 train_time:316580ms step_avg:419.87ms
step:765/800 train_loss:3.6404 train_time:316997ms step_avg:419.86ms
step:766/800 train_loss:4.0858 train_time:317414ms step_avg:419.86ms
step:767/800 train_loss:3.9322 train_time:317830ms step_avg:419.86ms
step:768/800 train_loss:3.8808 train_time:318247ms step_avg:419.85ms
step:769/800 train_loss:3.9082 train_time:318664ms step_avg:419.85ms
step:770/800 train_loss:3.9291 train_time:319082ms step_avg:419.84ms
step:771/800 train_loss:3.9902 train_time:319500ms step_avg:419.84ms
step:772/800 train_loss:4.2129 train_time:319918ms step_avg:419.84ms
step:773/800 train_loss:3.7835 train_time:320335ms step_avg:419.84ms
step:774/800 train_loss:3.9959 train_time:320754ms step_avg:419.83ms
step:775/800 train_loss:3.9712 train_time:321172ms step_avg:419.83ms
step:776/800 train_loss:3.9312 train_time:321588ms step_avg:419.83ms
step:777/800 train_loss:3.7438 train_time:322007ms step_avg:419.83ms
step:778/800 train_loss:3.7443 train_time:322424ms step_avg:419.82ms
step:779/800 train_loss:3.8055 train_time:322841ms step_avg:419.82ms
step:780/800 train_loss:3.8966 train_time:323260ms step_avg:419.82ms
step:781/800 train_loss:3.9332 train_time:323677ms step_avg:419.81ms
step:782/800 train_loss:3.9957 train_time:324096ms step_avg:419.81ms
step:783/800 train_loss:3.8929 train_time:324515ms step_avg:419.81ms
step:784/800 train_loss:3.9195 train_time:324932ms step_avg:419.81ms
step:785/800 train_loss:3.8960 train_time:325349ms step_avg:419.81ms
step:786/800 train_loss:3.8911 train_time:325767ms step_avg:419.80ms
step:787/800 train_loss:3.7934 train_time:326182ms step_avg:419.80ms
step:788/800 train_loss:4.0487 train_time:326601ms step_avg:419.80ms
step:789/800 train_loss:3.8412 train_time:327019ms step_avg:419.79ms
step:790/800 train_loss:3.9041 train_time:327436ms step_avg:419.79ms
step:791/800 train_loss:3.9643 train_time:327855ms step_avg:419.79ms
step:792/800 train_loss:4.0976 train_time:328274ms step_avg:419.79ms
step:793/800 train_loss:4.1055 train_time:328691ms step_avg:419.78ms
step:794/800 train_loss:3.8363 train_time:329109ms step_avg:419.78ms
step:795/800 train_loss:3.9418 train_time:329526ms step_avg:419.78ms
step:796/800 train_loss:3.9821 train_time:329943ms step_avg:419.77ms
step:797/800 train_loss:4.0819 train_time:330360ms step_avg:419.77ms
step:798/800 train_loss:3.8509 train_time:330777ms step_avg:419.77ms
step:799/800 train_loss:3.9991 train_time:331194ms step_avg:419.76ms
step:800/800 train_loss:3.9055 train_time:331612ms step_avg:419.76ms
step:800/800 val_loss:3.8928 train_time:331626ms step_avg:419.78ms
