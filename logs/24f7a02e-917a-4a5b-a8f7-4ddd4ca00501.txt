====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import json
import dataclasses
import subprocess
import csv
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

def _apply_gate_act(logits: torch.Tensor, kind: str) -> torch.Tensor:
    if kind == "sigmoid":
        return torch.sigmoid(logits)
    if kind == "ns_sigmoid":
        return 0.5 + 0.5 * torch.sigmoid(logits)
    raise ValueError(f"unknown gate_act={kind}")

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.attn_gate = getattr(config, "attn_gate", "none")
        self.gate_pos = getattr(config, "gate_pos", "sdpa")
        self.gate_act = getattr(config, "gate_act", "sigmoid")
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.attn_gate == "headwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_head, bias=False)
            self.gate_param = None
        elif self.attn_gate == "elementwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_embd, bias=False)
            self.gate_param = None
        elif self.attn_gate == "const":
            self.c_gate = None
            self.gate_param = nn.Parameter(torch.zeros(self.n_head, self.head_dim))
        else:
            self.c_gate = None
            self.gate_param = None

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if self.attn_gate != "none" and self.gate_pos == "value":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            v = v * gate
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2) # (B, T, n_head, head_dim)
        if self.attn_gate != "none" and self.gate_pos == "sdpa":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            y = y * gate
        y = y.contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attn_gate : str = "none"
    gate_pos : str = "sdpa"
    gate_act : str = "sigmoid"

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    seed : int = 1337
    attn_gate : str = "none" # none|headwise|elementwise|const
    gate_pos : str = "sdpa" # sdpa|value
    gate_act : str = "sigmoid" # sigmoid|ns_sigmoid
    early_stop_patience : int = 0 # 0 disables early stopping
    early_stop_min_delta : float = 0.0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

def apply_env_overrides():
    # environment-variable overrides allow quick sweeps without editing code
    args.learning_rate = float(os.environ.get("LR", args.learning_rate))
    args.seed = int(os.environ.get("SEED", args.seed))
    args.warmup_iters = int(os.environ.get("WARMUP_ITERS", args.warmup_iters))
    args.num_iterations = int(os.environ.get("NUM_ITERATIONS", args.num_iterations))
    args.early_stop_patience = int(os.environ.get("EARLY_STOP_PATIENCE", args.early_stop_patience))
    args.early_stop_min_delta = float(os.environ.get("EARLY_STOP_MIN_DELTA", args.early_stop_min_delta))
    args.attn_gate = os.environ.get("ATTNGATE", args.attn_gate)
    args.gate_pos = os.environ.get("GATEPOS", args.gate_pos)
    args.gate_act = os.environ.get("GATEACT", args.gate_act)
    args.num_iterations = int(os.environ.get("NUM_ITER", args.num_iterations))
    args.val_loss_every = int(os.environ.get("VAL_EVERY", args.val_loss_every))

def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

apply_env_overrides()
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
np.random.seed(args.seed)

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
git_commit = get_git_commit() if master_process else "unknown"

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    attn_gate=args.attn_gate,
    gate_pos=args.gate_pos,
    gate_act=args.gate_act,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        f.write(f"git_commit: {git_commit}\n")
        f.write(f"seed: {args.seed}\n")
        f.write("hyperparameters:\n")
        f.write(json.dumps(dataclasses.asdict(args), indent=2))
        f.write("\n")
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
best_val_loss = float("inf")
final_val_loss = None
no_improve_count = 0
early_stop_reason = None
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        if not torch.isfinite(val_loss):
            early_stop_reason = "non-finite val_loss"
            final_val_loss = float("nan")
            val_loss_item = float("nan")
        else:
            val_loss_item = val_loss.item()
            final_val_loss = val_loss_item
            if val_loss_item < best_val_loss - args.early_stop_min_delta:
                best_val_loss = val_loss_item
                no_improve_count = 0
            else:
                no_improve_count += 1
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if early_stop_reason is None and args.early_stop_patience > 0 and no_improve_count >= args.early_stop_patience:
                early_stop_reason = f"early_stop patience={args.early_stop_patience} min_delta={args.early_stop_min_delta}"
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process in a subdir to keep logs tidy
        ckpt_dir = os.path.join(logdir, "checkpoints")
        os.makedirs(ckpt_dir, exist_ok=True)
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, os.path.join(ckpt_dir, f"state_step{step:06d}.pt"))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")
    if early_stop_reason is not None:
        break

if master_process:
    if early_stop_reason is not None:
        print(f"stopped early: {early_stop_reason}")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    timed_steps_final = max(args.num_iterations - 9, 1)
    ms_per_step = training_time_ms / timed_steps_final
    os.makedirs("experiments", exist_ok=True)
    results_path = os.path.join("experiments", "results.csv")
    fieldnames = [
        "run_id",
        "date",
        "git_commit",
        "seed",
        "attn_gate",
        "gate_pos",
        "gate_act",
        "learning_rate",
        "batch_size",
        "device_batch_size",
        "sequence_length",
        "num_iterations",
        "warmdown_iters",
        "final_val_loss",
        "best_val_loss",
        "train_time_ms",
        "ms_per_step",
        "gpu_name",
        "n_gpus",
        "runpod_instance",
        "notes",
    ]
    final_loss_value = final_val_loss if final_val_loss is not None else float("nan")
    best_loss_value = best_val_loss if best_val_loss < float("inf") else float("nan")
    row = {
        "run_id": run_id,
        "date": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        "git_commit": git_commit,
        "seed": args.seed,
        "attn_gate": args.attn_gate,
        "gate_pos": args.gate_pos,
        "gate_act": args.gate_act,
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "device_batch_size": args.device_batch_size,
        "sequence_length": args.sequence_length,
        "num_iterations": args.num_iterations,
        "warmdown_iters": args.warmdown_iters,
        "final_val_loss": final_loss_value,
        "best_val_loss": best_loss_value,
        "train_time_ms": training_time_ms,
        "ms_per_step": ms_per_step,
        "gpu_name": torch.cuda.get_device_name(ddp_local_rank),
        "n_gpus": ddp_world_size,
        "runpod_instance": os.environ.get("RUNPOD_INSTANCE_TYPE", "unknown"),
        "notes": "",
    }
    write_header = not os.path.exists(results_path)
    with open(results_path, "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerow(row)
====================================================================================================
git_commit: bf6d1ff4b5e4ce7280e822ab33e90c4fb250e604
seed: 2337
hyperparameters:
{
  "input_bin": "data/fineweb10B/fineweb_train_*.bin",
  "input_val_bin": "data/fineweb10B/fineweb_val_*.bin",
  "batch_size": 512,
  "device_batch_size": 64,
  "sequence_length": 1024,
  "num_iterations": 800,
  "learning_rate": 0.0036,
  "warmup_iters": 150,
  "warmdown_iters": 1450,
  "weight_decay": 0,
  "seed": 2337,
  "attn_gate": "none",
  "gate_pos": "sdpa",
  "gate_act": "sigmoid",
  "early_stop_patience": 2,
  "early_stop_min_delta": 0.0,
  "val_loss_every": 125,
  "val_tokens": 10485760,
  "save_every": 0
}
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Sun Dec  7 16:43:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:00:07.0 Off |                    0 |
| N/A   46C    P0            114W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:00:08.0 Off |                    0 |
| N/A   48C    P0            118W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0            111W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000000:00:0A.0 Off |                    0 |
| N/A   45C    P0            138W /  300W |    2180MiB /  81920MiB |     18%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  |   00000000:00:0B.0 Off |                    0 |
| N/A   47C    P0            118W /  300W |    2180MiB /  81920MiB |     14%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  |   00000000:00:0C.0 Off |                    0 |
| N/A   44C    P0            131W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  |   00000000:00:0D.0 Off |                    0 |
| N/A   47C    P0            136W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  |   00000000:00:0E.0 Off |                    0 |
| N/A   47C    P0            112W /  300W |    2180MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/800 val_loss:16.0277 train_time:244ms step_avg:nanms
step:1/800 train_loss:16.0220 train_time:63224ms step_avg:nanms
step:2/800 train_loss:15.9093 train_time:63609ms step_avg:nanms
step:3/800 train_loss:15.6230 train_time:64004ms step_avg:nanms
step:4/800 train_loss:15.0267 train_time:64395ms step_avg:nanms
step:5/800 train_loss:13.8644 train_time:64788ms step_avg:nanms
step:6/800 train_loss:12.2444 train_time:65179ms step_avg:nanms
step:7/800 train_loss:10.6017 train_time:65571ms step_avg:nanms
step:8/800 train_loss:9.8193 train_time:65963ms step_avg:nanms
step:9/800 train_loss:9.5297 train_time:66354ms step_avg:nanms
step:10/800 train_loss:9.3701 train_time:66745ms step_avg:nanms
step:11/800 train_loss:9.1985 train_time:378ms step_avg:nanms
step:12/800 train_loss:9.0571 train_time:769ms step_avg:nanms
step:13/800 train_loss:8.8192 train_time:1160ms step_avg:386.67ms
step:14/800 train_loss:8.6958 train_time:1551ms step_avg:387.68ms
step:15/800 train_loss:8.5367 train_time:1946ms step_avg:389.15ms
step:16/800 train_loss:8.3632 train_time:2336ms step_avg:389.38ms
step:17/800 train_loss:8.2153 train_time:2728ms step_avg:389.65ms
step:18/800 train_loss:8.0844 train_time:3119ms step_avg:389.87ms
step:19/800 train_loss:7.8441 train_time:3511ms step_avg:390.08ms
step:20/800 train_loss:7.7441 train_time:3901ms step_avg:390.10ms
step:21/800 train_loss:7.3613 train_time:4292ms step_avg:390.15ms
step:22/800 train_loss:7.5986 train_time:4682ms step_avg:390.17ms
step:23/800 train_loss:7.7191 train_time:5071ms step_avg:390.11ms
step:24/800 train_loss:7.3988 train_time:5463ms step_avg:390.24ms
step:25/800 train_loss:7.4629 train_time:5854ms step_avg:390.28ms
step:26/800 train_loss:7.2383 train_time:6245ms step_avg:390.32ms
step:27/800 train_loss:7.1372 train_time:6635ms step_avg:390.29ms
step:28/800 train_loss:7.2808 train_time:7026ms step_avg:390.35ms
step:29/800 train_loss:6.9665 train_time:7418ms step_avg:390.42ms
step:30/800 train_loss:7.1906 train_time:7811ms step_avg:390.55ms
step:31/800 train_loss:7.0443 train_time:8202ms step_avg:390.57ms
step:32/800 train_loss:6.9546 train_time:8593ms step_avg:390.59ms
step:33/800 train_loss:6.7408 train_time:8986ms step_avg:390.68ms
step:34/800 train_loss:7.1238 train_time:9376ms step_avg:390.65ms
step:35/800 train_loss:6.9051 train_time:9768ms step_avg:390.70ms
step:36/800 train_loss:7.0974 train_time:10161ms step_avg:390.79ms
step:37/800 train_loss:6.9779 train_time:10553ms step_avg:390.84ms
step:38/800 train_loss:6.8295 train_time:10947ms step_avg:390.95ms
step:39/800 train_loss:6.7001 train_time:11342ms step_avg:391.11ms
step:40/800 train_loss:6.7903 train_time:11734ms step_avg:391.15ms
step:41/800 train_loss:6.6468 train_time:12128ms step_avg:391.23ms
step:42/800 train_loss:6.6897 train_time:12522ms step_avg:391.30ms
step:43/800 train_loss:6.5004 train_time:12916ms step_avg:391.39ms
step:44/800 train_loss:6.5979 train_time:13309ms step_avg:391.45ms
step:45/800 train_loss:6.5738 train_time:13701ms step_avg:391.46ms
step:46/800 train_loss:6.7790 train_time:14096ms step_avg:391.55ms
step:47/800 train_loss:6.5739 train_time:14488ms step_avg:391.58ms
step:48/800 train_loss:6.4055 train_time:14882ms step_avg:391.62ms
step:49/800 train_loss:6.6483 train_time:15273ms step_avg:391.63ms
step:50/800 train_loss:6.4994 train_time:15666ms step_avg:391.65ms
step:51/800 train_loss:6.6468 train_time:16060ms step_avg:391.71ms
step:52/800 train_loss:6.4974 train_time:16452ms step_avg:391.72ms
step:53/800 train_loss:6.3253 train_time:16847ms step_avg:391.79ms
step:54/800 train_loss:6.4552 train_time:17243ms step_avg:391.89ms
step:55/800 train_loss:6.3816 train_time:17636ms step_avg:391.92ms
step:56/800 train_loss:6.6820 train_time:18029ms step_avg:391.94ms
step:57/800 train_loss:6.3620 train_time:18422ms step_avg:391.97ms
step:58/800 train_loss:6.2313 train_time:18814ms step_avg:391.97ms
step:59/800 train_loss:6.4129 train_time:19207ms step_avg:391.98ms
step:60/800 train_loss:6.3154 train_time:19600ms step_avg:391.99ms
step:61/800 train_loss:6.4295 train_time:19993ms step_avg:392.02ms
step:62/800 train_loss:6.2227 train_time:20388ms step_avg:392.07ms
step:63/800 train_loss:6.3018 train_time:20782ms step_avg:392.11ms
step:64/800 train_loss:6.2537 train_time:21174ms step_avg:392.12ms
step:65/800 train_loss:6.8015 train_time:21567ms step_avg:392.12ms
step:66/800 train_loss:6.0902 train_time:21961ms step_avg:392.17ms
step:67/800 train_loss:6.2464 train_time:22355ms step_avg:392.19ms
step:68/800 train_loss:6.1079 train_time:22748ms step_avg:392.21ms
step:69/800 train_loss:6.4295 train_time:23144ms step_avg:392.27ms
step:70/800 train_loss:6.0318 train_time:23539ms step_avg:392.31ms
step:71/800 train_loss:6.0663 train_time:23932ms step_avg:392.33ms
step:72/800 train_loss:6.2831 train_time:24325ms step_avg:392.34ms
step:73/800 train_loss:6.1968 train_time:24721ms step_avg:392.40ms
step:74/800 train_loss:6.0945 train_time:25115ms step_avg:392.43ms
step:75/800 train_loss:6.1969 train_time:25508ms step_avg:392.43ms
step:76/800 train_loss:6.1471 train_time:25901ms step_avg:392.44ms
step:77/800 train_loss:6.1278 train_time:26296ms step_avg:392.47ms
step:78/800 train_loss:6.1996 train_time:26690ms step_avg:392.50ms
step:79/800 train_loss:6.2534 train_time:27083ms step_avg:392.50ms
step:80/800 train_loss:6.1006 train_time:27477ms step_avg:392.53ms
step:81/800 train_loss:6.1864 train_time:27873ms step_avg:392.57ms
step:82/800 train_loss:5.9164 train_time:28267ms step_avg:392.60ms
step:83/800 train_loss:6.1086 train_time:28662ms step_avg:392.62ms
step:84/800 train_loss:6.0820 train_time:29055ms step_avg:392.63ms
step:85/800 train_loss:6.0110 train_time:29447ms step_avg:392.63ms
step:86/800 train_loss:5.8789 train_time:29844ms step_avg:392.69ms
step:87/800 train_loss:6.0861 train_time:30237ms step_avg:392.69ms
step:88/800 train_loss:5.9958 train_time:30632ms step_avg:392.71ms
step:89/800 train_loss:6.0800 train_time:31025ms step_avg:392.72ms
step:90/800 train_loss:6.0578 train_time:31420ms step_avg:392.75ms
step:91/800 train_loss:5.9551 train_time:31813ms step_avg:392.75ms
step:92/800 train_loss:5.9392 train_time:32209ms step_avg:392.80ms
step:93/800 train_loss:6.0456 train_time:32602ms step_avg:392.80ms
step:94/800 train_loss:5.9117 train_time:32997ms step_avg:392.82ms
step:95/800 train_loss:5.8727 train_time:33391ms step_avg:392.83ms
step:96/800 train_loss:5.8911 train_time:33784ms step_avg:392.84ms
step:97/800 train_loss:5.7952 train_time:34177ms step_avg:392.84ms
step:98/800 train_loss:5.8899 train_time:34572ms step_avg:392.86ms
step:99/800 train_loss:5.7834 train_time:34965ms step_avg:392.86ms
step:100/800 train_loss:5.9306 train_time:35360ms step_avg:392.89ms
step:101/800 train_loss:5.8819 train_time:35754ms step_avg:392.90ms
step:102/800 train_loss:5.7762 train_time:36151ms step_avg:392.94ms
step:103/800 train_loss:5.8772 train_time:36547ms step_avg:392.98ms
step:104/800 train_loss:5.8556 train_time:36942ms step_avg:393.00ms
step:105/800 train_loss:5.6443 train_time:37337ms step_avg:393.02ms
step:106/800 train_loss:5.7835 train_time:37730ms step_avg:393.02ms
step:107/800 train_loss:6.0108 train_time:38125ms step_avg:393.04ms
step:108/800 train_loss:5.7585 train_time:38519ms step_avg:393.05ms
step:109/800 train_loss:5.4911 train_time:38911ms step_avg:393.04ms
step:110/800 train_loss:5.7442 train_time:39307ms step_avg:393.07ms
step:111/800 train_loss:5.6858 train_time:39700ms step_avg:393.07ms
step:112/800 train_loss:5.6624 train_time:40094ms step_avg:393.08ms
step:113/800 train_loss:5.7567 train_time:40488ms step_avg:393.09ms
step:114/800 train_loss:5.6710 train_time:40883ms step_avg:393.11ms
step:115/800 train_loss:5.5253 train_time:41279ms step_avg:393.13ms
step:116/800 train_loss:5.7184 train_time:41673ms step_avg:393.14ms
step:117/800 train_loss:5.5337 train_time:42066ms step_avg:393.14ms
step:118/800 train_loss:5.5362 train_time:42460ms step_avg:393.15ms
step:119/800 train_loss:5.6329 train_time:42854ms step_avg:393.16ms
step:120/800 train_loss:5.6717 train_time:43249ms step_avg:393.18ms
step:121/800 train_loss:5.5706 train_time:43646ms step_avg:393.21ms
step:122/800 train_loss:5.4536 train_time:44041ms step_avg:393.23ms
step:123/800 train_loss:5.5453 train_time:44437ms step_avg:393.24ms
step:124/800 train_loss:5.4136 train_time:44832ms step_avg:393.26ms
step:125/800 train_loss:5.7149 train_time:45226ms step_avg:393.27ms
step:125/800 val_loss:5.5442 train_time:45240ms step_avg:393.39ms
step:126/800 train_loss:5.5556 train_time:45621ms step_avg:393.28ms
step:127/800 train_loss:5.5309 train_time:46017ms step_avg:393.30ms
step:128/800 train_loss:5.5972 train_time:46411ms step_avg:393.32ms
step:129/800 train_loss:5.4404 train_time:46806ms step_avg:393.33ms
step:130/800 train_loss:5.7094 train_time:47200ms step_avg:393.34ms
step:131/800 train_loss:5.4930 train_time:47595ms step_avg:393.34ms
step:132/800 train_loss:5.4987 train_time:47989ms step_avg:393.35ms
step:133/800 train_loss:5.4235 train_time:48384ms step_avg:393.36ms
step:134/800 train_loss:5.4738 train_time:48778ms step_avg:393.37ms
step:135/800 train_loss:5.4345 train_time:49174ms step_avg:393.39ms
step:136/800 train_loss:5.4617 train_time:49568ms step_avg:393.40ms
step:137/800 train_loss:5.2614 train_time:49964ms step_avg:393.42ms
step:138/800 train_loss:5.4222 train_time:50360ms step_avg:393.44ms
step:139/800 train_loss:5.3991 train_time:50754ms step_avg:393.44ms
step:140/800 train_loss:5.4012 train_time:51149ms step_avg:393.45ms
step:141/800 train_loss:5.4212 train_time:51541ms step_avg:393.44ms
step:142/800 train_loss:5.3219 train_time:51935ms step_avg:393.45ms
step:143/800 train_loss:5.4030 train_time:52329ms step_avg:393.45ms
step:144/800 train_loss:5.2008 train_time:52723ms step_avg:393.45ms
step:145/800 train_loss:5.3669 train_time:53118ms step_avg:393.46ms
step:146/800 train_loss:5.2966 train_time:53514ms step_avg:393.48ms
step:147/800 train_loss:5.2065 train_time:53907ms step_avg:393.48ms
step:148/800 train_loss:5.3266 train_time:54301ms step_avg:393.49ms
step:149/800 train_loss:5.3023 train_time:54695ms step_avg:393.49ms
step:150/800 train_loss:5.3647 train_time:55090ms step_avg:393.50ms
step:151/800 train_loss:5.3726 train_time:55485ms step_avg:393.51ms
step:152/800 train_loss:5.2503 train_time:55879ms step_avg:393.52ms
step:153/800 train_loss:5.2325 train_time:56272ms step_avg:393.51ms
step:154/800 train_loss:5.2973 train_time:56668ms step_avg:393.53ms
step:155/800 train_loss:5.2276 train_time:57065ms step_avg:393.55ms
step:156/800 train_loss:5.2091 train_time:57459ms step_avg:393.56ms
step:157/800 train_loss:5.2088 train_time:57855ms step_avg:393.57ms
step:158/800 train_loss:5.3444 train_time:58250ms step_avg:393.58ms
step:159/800 train_loss:5.1166 train_time:58645ms step_avg:393.59ms
step:160/800 train_loss:5.1775 train_time:59040ms step_avg:393.60ms
step:161/800 train_loss:5.0350 train_time:59432ms step_avg:393.59ms
step:162/800 train_loss:5.1785 train_time:59828ms step_avg:393.60ms
step:163/800 train_loss:5.2139 train_time:60221ms step_avg:393.60ms
step:164/800 train_loss:5.2084 train_time:60617ms step_avg:393.62ms
step:165/800 train_loss:5.0188 train_time:61010ms step_avg:393.62ms
step:166/800 train_loss:5.1291 train_time:61405ms step_avg:393.62ms
step:167/800 train_loss:5.2941 train_time:61800ms step_avg:393.63ms
step:168/800 train_loss:5.0649 train_time:62193ms step_avg:393.63ms
step:169/800 train_loss:5.1520 train_time:62588ms step_avg:393.64ms
step:170/800 train_loss:5.0141 train_time:62984ms step_avg:393.65ms
step:171/800 train_loss:4.9554 train_time:63378ms step_avg:393.65ms
step:172/800 train_loss:5.0481 train_time:63771ms step_avg:393.65ms
step:173/800 train_loss:5.0273 train_time:64167ms step_avg:393.66ms
step:174/800 train_loss:5.0903 train_time:64564ms step_avg:393.68ms
step:175/800 train_loss:5.2260 train_time:64958ms step_avg:393.69ms
step:176/800 train_loss:5.1122 train_time:65352ms step_avg:393.69ms
step:177/800 train_loss:4.9446 train_time:65748ms step_avg:393.70ms
step:178/800 train_loss:4.9188 train_time:66142ms step_avg:393.70ms
step:179/800 train_loss:4.9590 train_time:66536ms step_avg:393.71ms
step:180/800 train_loss:5.0037 train_time:66931ms step_avg:393.71ms
step:181/800 train_loss:4.9884 train_time:67326ms step_avg:393.72ms
step:182/800 train_loss:5.0995 train_time:67721ms step_avg:393.72ms
step:183/800 train_loss:4.9822 train_time:68115ms step_avg:393.73ms
step:184/800 train_loss:4.9182 train_time:68511ms step_avg:393.74ms
step:185/800 train_loss:4.9396 train_time:68905ms step_avg:393.74ms
step:186/800 train_loss:5.0691 train_time:69300ms step_avg:393.75ms
step:187/800 train_loss:4.9503 train_time:69694ms step_avg:393.75ms
step:188/800 train_loss:5.1985 train_time:70087ms step_avg:393.75ms
step:189/800 train_loss:4.9794 train_time:70609ms step_avg:394.46ms
step:190/800 train_loss:4.8936 train_time:71137ms step_avg:395.21ms
step:191/800 train_loss:5.0620 train_time:71532ms step_avg:395.21ms
step:192/800 train_loss:4.8891 train_time:71926ms step_avg:395.20ms
step:193/800 train_loss:4.8122 train_time:72320ms step_avg:395.19ms
step:194/800 train_loss:5.0227 train_time:72715ms step_avg:395.19ms
step:195/800 train_loss:4.9606 train_time:73109ms step_avg:395.18ms
step:196/800 train_loss:5.1421 train_time:73503ms step_avg:395.18ms
step:197/800 train_loss:5.0329 train_time:73899ms step_avg:395.18ms
step:198/800 train_loss:4.8594 train_time:74297ms step_avg:395.20ms
step:199/800 train_loss:4.9089 train_time:74691ms step_avg:395.19ms
step:200/800 train_loss:4.8024 train_time:75086ms step_avg:395.19ms
step:201/800 train_loss:4.8827 train_time:75481ms step_avg:395.19ms
step:202/800 train_loss:4.8025 train_time:75876ms step_avg:395.19ms
step:203/800 train_loss:5.0454 train_time:76272ms step_avg:395.19ms
step:204/800 train_loss:4.9303 train_time:76667ms step_avg:395.19ms
step:205/800 train_loss:4.9037 train_time:77064ms step_avg:395.20ms
step:206/800 train_loss:5.0503 train_time:77464ms step_avg:395.22ms
step:207/800 train_loss:4.7235 train_time:77858ms step_avg:395.22ms
step:208/800 train_loss:4.8706 train_time:78253ms step_avg:395.22ms
step:209/800 train_loss:4.8293 train_time:78646ms step_avg:395.21ms
step:210/800 train_loss:4.9901 train_time:79039ms step_avg:395.20ms
step:211/800 train_loss:4.9038 train_time:79435ms step_avg:395.20ms
step:212/800 train_loss:4.8003 train_time:79886ms step_avg:395.47ms
step:213/800 train_loss:4.9404 train_time:80279ms step_avg:395.46ms
step:214/800 train_loss:4.7606 train_time:80674ms step_avg:395.46ms
step:215/800 train_loss:4.8558 train_time:81068ms step_avg:395.46ms
step:216/800 train_loss:4.7118 train_time:81468ms step_avg:395.47ms
step:217/800 train_loss:4.8363 train_time:81868ms step_avg:395.50ms
step:218/800 train_loss:4.8182 train_time:82267ms step_avg:395.51ms
step:219/800 train_loss:4.7823 train_time:82672ms step_avg:395.56ms
step:220/800 train_loss:4.8005 train_time:83068ms step_avg:395.56ms
step:221/800 train_loss:4.8241 train_time:83465ms step_avg:395.57ms
step:222/800 train_loss:4.8707 train_time:83864ms step_avg:395.58ms
step:223/800 train_loss:4.8015 train_time:84259ms step_avg:395.58ms
step:224/800 train_loss:4.8081 train_time:84652ms step_avg:395.57ms
step:225/800 train_loss:4.9325 train_time:85047ms step_avg:395.57ms
step:226/800 train_loss:4.6741 train_time:85441ms step_avg:395.56ms
step:227/800 train_loss:4.7043 train_time:85835ms step_avg:395.55ms
step:228/800 train_loss:4.6911 train_time:86229ms step_avg:395.55ms
step:229/800 train_loss:4.8473 train_time:86624ms step_avg:395.54ms
step:230/800 train_loss:4.6908 train_time:87017ms step_avg:395.53ms
step:231/800 train_loss:4.8362 train_time:87412ms step_avg:395.53ms
step:232/800 train_loss:4.6972 train_time:87807ms step_avg:395.53ms
step:233/800 train_loss:4.6570 train_time:88202ms step_avg:395.53ms
step:234/800 train_loss:4.8563 train_time:88596ms step_avg:395.52ms
step:235/800 train_loss:4.6981 train_time:88990ms step_avg:395.51ms
step:236/800 train_loss:4.6138 train_time:89387ms step_avg:395.52ms
step:237/800 train_loss:4.8900 train_time:89780ms step_avg:395.51ms
step:238/800 train_loss:4.7649 train_time:90176ms step_avg:395.51ms
step:239/800 train_loss:4.6814 train_time:90571ms step_avg:395.50ms
step:240/800 train_loss:4.8130 train_time:90968ms step_avg:395.51ms
step:241/800 train_loss:4.8045 train_time:91364ms step_avg:395.52ms
step:242/800 train_loss:4.7010 train_time:91758ms step_avg:395.51ms
step:243/800 train_loss:4.8719 train_time:92152ms step_avg:395.50ms
step:244/800 train_loss:4.6953 train_time:92546ms step_avg:395.50ms
step:245/800 train_loss:4.7101 train_time:92942ms step_avg:395.50ms
step:246/800 train_loss:4.7802 train_time:93336ms step_avg:395.49ms
step:247/800 train_loss:4.7342 train_time:93729ms step_avg:395.48ms
step:248/800 train_loss:4.6856 train_time:94125ms step_avg:395.48ms
step:249/800 train_loss:4.8567 train_time:94519ms step_avg:395.48ms
step:250/800 train_loss:4.5845 train_time:94914ms step_avg:395.47ms
step:250/800 val_loss:4.6944 train_time:94928ms step_avg:395.53ms
step:251/800 train_loss:4.6229 train_time:95314ms step_avg:395.49ms
step:252/800 train_loss:4.7549 train_time:95709ms step_avg:395.49ms
step:253/800 train_loss:4.7514 train_time:96102ms step_avg:395.48ms
step:254/800 train_loss:4.6219 train_time:96499ms step_avg:395.49ms
step:255/800 train_loss:4.6433 train_time:96892ms step_avg:395.48ms
step:256/800 train_loss:4.7801 train_time:97289ms step_avg:395.48ms
step:257/800 train_loss:4.7231 train_time:97686ms step_avg:395.49ms
step:258/800 train_loss:4.6898 train_time:98083ms step_avg:395.50ms
step:259/800 train_loss:4.6184 train_time:98480ms step_avg:395.50ms
step:260/800 train_loss:4.6335 train_time:98875ms step_avg:395.50ms
step:261/800 train_loss:4.7137 train_time:99269ms step_avg:395.49ms
step:262/800 train_loss:4.7130 train_time:99664ms step_avg:395.49ms
step:263/800 train_loss:4.6150 train_time:100061ms step_avg:395.50ms
step:264/800 train_loss:4.5634 train_time:100455ms step_avg:395.49ms
step:265/800 train_loss:4.6164 train_time:100849ms step_avg:395.49ms
step:266/800 train_loss:4.4717 train_time:101244ms step_avg:395.48ms
step:267/800 train_loss:4.5318 train_time:101637ms step_avg:395.48ms
step:268/800 train_loss:4.5733 train_time:102030ms step_avg:395.47ms
step:269/800 train_loss:4.5362 train_time:102425ms step_avg:395.47ms
step:270/800 train_loss:4.4886 train_time:102822ms step_avg:395.47ms
step:271/800 train_loss:4.7217 train_time:103215ms step_avg:395.46ms
step:272/800 train_loss:4.6438 train_time:103610ms step_avg:395.46ms
step:273/800 train_loss:4.5044 train_time:104004ms step_avg:395.45ms
step:274/800 train_loss:4.5571 train_time:104400ms step_avg:395.45ms
step:275/800 train_loss:4.6730 train_time:104797ms step_avg:395.46ms
step:276/800 train_loss:4.6771 train_time:105192ms step_avg:395.46ms
step:277/800 train_loss:4.8786 train_time:105587ms step_avg:395.46ms
step:278/800 train_loss:4.6304 train_time:105985ms step_avg:395.47ms
step:279/800 train_loss:4.7545 train_time:106379ms step_avg:395.46ms
step:280/800 train_loss:4.5989 train_time:106773ms step_avg:395.46ms
step:281/800 train_loss:4.6674 train_time:107167ms step_avg:395.45ms
step:282/800 train_loss:4.5630 train_time:107561ms step_avg:395.45ms
step:283/800 train_loss:4.6698 train_time:107956ms step_avg:395.44ms
step:284/800 train_loss:4.4962 train_time:108352ms step_avg:395.44ms
step:285/800 train_loss:4.6646 train_time:108747ms step_avg:395.44ms
step:286/800 train_loss:4.6476 train_time:109142ms step_avg:395.44ms
step:287/800 train_loss:4.6846 train_time:109537ms step_avg:395.44ms
step:288/800 train_loss:4.5314 train_time:109931ms step_avg:395.43ms
step:289/800 train_loss:4.5990 train_time:110325ms step_avg:395.43ms
step:290/800 train_loss:4.4607 train_time:110721ms step_avg:395.43ms
step:291/800 train_loss:4.4619 train_time:111115ms step_avg:395.43ms
step:292/800 train_loss:4.5723 train_time:111511ms step_avg:395.43ms
step:293/800 train_loss:4.4588 train_time:111904ms step_avg:395.42ms
step:294/800 train_loss:4.5114 train_time:112299ms step_avg:395.42ms
step:295/800 train_loss:4.5299 train_time:112693ms step_avg:395.42ms
step:296/800 train_loss:4.4010 train_time:113089ms step_avg:395.42ms
step:297/800 train_loss:4.3931 train_time:113488ms step_avg:395.43ms
step:298/800 train_loss:4.4195 train_time:113886ms step_avg:395.44ms
step:299/800 train_loss:4.5274 train_time:114282ms step_avg:395.44ms
step:300/800 train_loss:4.4041 train_time:114676ms step_avg:395.44ms
step:301/800 train_loss:4.5822 train_time:115072ms step_avg:395.44ms
step:302/800 train_loss:4.5537 train_time:115469ms step_avg:395.44ms
step:303/800 train_loss:4.4736 train_time:115862ms step_avg:395.43ms
step:304/800 train_loss:4.5407 train_time:116255ms step_avg:395.42ms
step:305/800 train_loss:4.5250 train_time:116649ms step_avg:395.42ms
step:306/800 train_loss:4.9932 train_time:117043ms step_avg:395.41ms
step:307/800 train_loss:4.4691 train_time:117437ms step_avg:395.41ms
step:308/800 train_loss:4.3840 train_time:117832ms step_avg:395.41ms
step:309/800 train_loss:4.5692 train_time:118227ms step_avg:395.41ms
step:310/800 train_loss:4.3675 train_time:118620ms step_avg:395.40ms
step:311/800 train_loss:4.6115 train_time:119014ms step_avg:395.39ms
step:312/800 train_loss:4.4983 train_time:119407ms step_avg:395.39ms
step:313/800 train_loss:4.4147 train_time:119803ms step_avg:395.39ms
step:314/800 train_loss:4.5451 train_time:120196ms step_avg:395.38ms
step:315/800 train_loss:4.6497 train_time:120591ms step_avg:395.38ms
step:316/800 train_loss:4.4961 train_time:120986ms step_avg:395.38ms
step:317/800 train_loss:4.3644 train_time:121383ms step_avg:395.38ms
step:318/800 train_loss:4.4008 train_time:121778ms step_avg:395.38ms
step:319/800 train_loss:4.4246 train_time:122172ms step_avg:395.38ms
step:320/800 train_loss:4.3803 train_time:122568ms step_avg:395.38ms
step:321/800 train_loss:4.4751 train_time:122961ms step_avg:395.37ms
step:322/800 train_loss:4.4718 train_time:123356ms step_avg:395.37ms
step:323/800 train_loss:4.4287 train_time:123750ms step_avg:395.37ms
step:324/800 train_loss:4.5087 train_time:124145ms step_avg:395.37ms
step:325/800 train_loss:4.4946 train_time:124541ms step_avg:395.37ms
step:326/800 train_loss:4.5612 train_time:124936ms step_avg:395.37ms
step:327/800 train_loss:4.4047 train_time:125329ms step_avg:395.36ms
step:328/800 train_loss:4.8694 train_time:125725ms step_avg:395.36ms
step:329/800 train_loss:4.5733 train_time:126119ms step_avg:395.36ms
step:330/800 train_loss:4.3316 train_time:126512ms step_avg:395.35ms
step:331/800 train_loss:4.2899 train_time:126906ms step_avg:395.35ms
step:332/800 train_loss:4.4729 train_time:127299ms step_avg:395.34ms
step:333/800 train_loss:4.3888 train_time:127694ms step_avg:395.34ms
step:334/800 train_loss:4.3834 train_time:128089ms step_avg:395.34ms
step:335/800 train_loss:4.3379 train_time:128486ms step_avg:395.34ms
step:336/800 train_loss:4.5096 train_time:128882ms step_avg:395.34ms
step:337/800 train_loss:4.4505 train_time:129276ms step_avg:395.34ms
step:338/800 train_loss:4.9635 train_time:129669ms step_avg:395.33ms
step:339/800 train_loss:4.4297 train_time:130063ms step_avg:395.33ms
step:340/800 train_loss:4.3961 train_time:130458ms step_avg:395.33ms
step:341/800 train_loss:4.3876 train_time:130853ms step_avg:395.32ms
step:342/800 train_loss:4.3251 train_time:131248ms step_avg:395.33ms
step:343/800 train_loss:4.2951 train_time:131642ms step_avg:395.32ms
step:344/800 train_loss:4.3545 train_time:132036ms step_avg:395.32ms
step:345/800 train_loss:4.4696 train_time:132431ms step_avg:395.32ms
step:346/800 train_loss:4.3396 train_time:132824ms step_avg:395.31ms
step:347/800 train_loss:4.2615 train_time:133220ms step_avg:395.31ms
step:348/800 train_loss:4.3088 train_time:133616ms step_avg:395.31ms
step:349/800 train_loss:4.3383 train_time:134009ms step_avg:395.31ms
step:350/800 train_loss:4.2788 train_time:134402ms step_avg:395.30ms
step:351/800 train_loss:3.9719 train_time:134797ms step_avg:395.30ms
step:352/800 train_loss:4.2540 train_time:135193ms step_avg:395.30ms
step:353/800 train_loss:4.6053 train_time:135586ms step_avg:395.29ms
step:354/800 train_loss:4.1179 train_time:135983ms step_avg:395.30ms
step:355/800 train_loss:4.3773 train_time:136377ms step_avg:395.30ms
step:356/800 train_loss:4.2605 train_time:136772ms step_avg:395.30ms
step:357/800 train_loss:4.3602 train_time:137166ms step_avg:395.29ms
step:358/800 train_loss:4.3472 train_time:137560ms step_avg:395.29ms
step:359/800 train_loss:4.2952 train_time:137955ms step_avg:395.29ms
step:360/800 train_loss:4.4070 train_time:138349ms step_avg:395.28ms
step:361/800 train_loss:3.9784 train_time:138743ms step_avg:395.28ms
step:362/800 train_loss:4.4844 train_time:139139ms step_avg:395.28ms
step:363/800 train_loss:4.3780 train_time:139532ms step_avg:395.27ms
step:364/800 train_loss:4.2835 train_time:139926ms step_avg:395.27ms
step:365/800 train_loss:4.2093 train_time:140321ms step_avg:395.27ms
step:366/800 train_loss:4.3664 train_time:140716ms step_avg:395.27ms
step:367/800 train_loss:4.3135 train_time:141111ms step_avg:395.27ms
step:368/800 train_loss:4.2895 train_time:141504ms step_avg:395.26ms
step:369/800 train_loss:4.2950 train_time:141899ms step_avg:395.26ms
step:370/800 train_loss:4.1839 train_time:142293ms step_avg:395.26ms
step:371/800 train_loss:4.3371 train_time:142688ms step_avg:395.26ms
step:372/800 train_loss:4.2391 train_time:143086ms step_avg:395.26ms
step:373/800 train_loss:4.1347 train_time:143482ms step_avg:395.27ms
step:374/800 train_loss:4.3393 train_time:143877ms step_avg:395.27ms
step:375/800 train_loss:4.2734 train_time:144271ms step_avg:395.26ms
step:375/800 val_loss:4.2763 train_time:144285ms step_avg:395.30ms
step:376/800 train_loss:4.2486 train_time:144667ms step_avg:395.27ms
step:377/800 train_loss:4.3109 train_time:145063ms step_avg:395.27ms
step:378/800 train_loss:4.2157 train_time:145575ms step_avg:395.58ms
step:379/800 train_loss:4.2698 train_time:145972ms step_avg:395.59ms
step:380/800 train_loss:4.3307 train_time:146500ms step_avg:395.95ms
step:381/800 train_loss:4.3777 train_time:146893ms step_avg:395.94ms
step:382/800 train_loss:4.2966 train_time:147287ms step_avg:395.93ms
step:383/800 train_loss:4.2752 train_time:147682ms step_avg:395.93ms
step:384/800 train_loss:4.2040 train_time:148076ms step_avg:395.93ms
step:385/800 train_loss:4.2991 train_time:148471ms step_avg:395.92ms
step:386/800 train_loss:4.2131 train_time:148865ms step_avg:395.92ms
step:387/800 train_loss:4.3303 train_time:149261ms step_avg:395.92ms
step:388/800 train_loss:4.5232 train_time:149656ms step_avg:395.91ms
step:389/800 train_loss:4.2365 train_time:150050ms step_avg:395.91ms
step:390/800 train_loss:4.2077 train_time:150445ms step_avg:395.91ms
step:391/800 train_loss:4.3180 train_time:150838ms step_avg:395.90ms
step:392/800 train_loss:4.2302 train_time:151233ms step_avg:395.90ms
step:393/800 train_loss:4.3404 train_time:151629ms step_avg:395.90ms
step:394/800 train_loss:4.1658 train_time:152023ms step_avg:395.89ms
step:395/800 train_loss:4.3070 train_time:152419ms step_avg:395.89ms
step:396/800 train_loss:4.0663 train_time:152813ms step_avg:395.89ms
step:397/800 train_loss:4.2519 train_time:153209ms step_avg:395.89ms
step:398/800 train_loss:4.3198 train_time:153606ms step_avg:395.89ms
step:399/800 train_loss:4.2904 train_time:154004ms step_avg:395.90ms
step:400/800 train_loss:4.2080 train_time:154399ms step_avg:395.89ms
step:401/800 train_loss:4.2527 train_time:154792ms step_avg:395.89ms
step:402/800 train_loss:4.3118 train_time:155186ms step_avg:395.88ms
step:403/800 train_loss:4.2678 train_time:155581ms step_avg:395.88ms
step:404/800 train_loss:4.3653 train_time:155977ms step_avg:395.88ms
step:405/800 train_loss:4.1240 train_time:156372ms step_avg:395.88ms
step:406/800 train_loss:4.2068 train_time:156765ms step_avg:395.87ms
step:407/800 train_loss:4.4782 train_time:157159ms step_avg:395.87ms
step:408/800 train_loss:4.2302 train_time:157553ms step_avg:395.86ms
step:409/800 train_loss:4.2323 train_time:157946ms step_avg:395.85ms
step:410/800 train_loss:4.2743 train_time:158342ms step_avg:395.85ms
step:411/800 train_loss:4.1524 train_time:158733ms step_avg:395.84ms
step:412/800 train_loss:4.1814 train_time:159128ms step_avg:395.84ms
step:413/800 train_loss:4.5767 train_time:159524ms step_avg:395.84ms
step:414/800 train_loss:4.0353 train_time:159918ms step_avg:395.84ms
step:415/800 train_loss:4.4235 train_time:160313ms step_avg:395.83ms
step:416/800 train_loss:4.1773 train_time:160706ms step_avg:395.83ms
step:417/800 train_loss:4.1774 train_time:161102ms step_avg:395.83ms
step:418/800 train_loss:4.3633 train_time:161495ms step_avg:395.82ms
step:419/800 train_loss:4.0886 train_time:161891ms step_avg:395.82ms
step:420/800 train_loss:4.1998 train_time:162286ms step_avg:395.82ms
step:421/800 train_loss:4.1526 train_time:162680ms step_avg:395.81ms
step:422/800 train_loss:4.0485 train_time:163074ms step_avg:395.81ms
step:423/800 train_loss:4.1713 train_time:163468ms step_avg:395.81ms
step:424/800 train_loss:4.2740 train_time:163863ms step_avg:395.80ms
step:425/800 train_loss:4.0484 train_time:164258ms step_avg:395.80ms
step:426/800 train_loss:4.2230 train_time:164652ms step_avg:395.80ms
step:427/800 train_loss:4.0996 train_time:165046ms step_avg:395.79ms
step:428/800 train_loss:4.3072 train_time:165439ms step_avg:395.79ms
step:429/800 train_loss:4.2290 train_time:165833ms step_avg:395.78ms
step:430/800 train_loss:4.1510 train_time:166229ms step_avg:395.78ms
step:431/800 train_loss:4.1247 train_time:166624ms step_avg:395.78ms
step:432/800 train_loss:4.0447 train_time:167018ms step_avg:395.78ms
step:433/800 train_loss:4.1637 train_time:167412ms step_avg:395.77ms
step:434/800 train_loss:4.2384 train_time:167808ms step_avg:395.77ms
step:435/800 train_loss:4.1643 train_time:168206ms step_avg:395.78ms
step:436/800 train_loss:4.2148 train_time:168602ms step_avg:395.78ms
step:437/800 train_loss:4.2237 train_time:168995ms step_avg:395.77ms
step:438/800 train_loss:4.0956 train_time:169390ms step_avg:395.77ms
step:439/800 train_loss:4.1244 train_time:169784ms step_avg:395.77ms
step:440/800 train_loss:4.0922 train_time:170178ms step_avg:395.76ms
step:441/800 train_loss:4.2752 train_time:170573ms step_avg:395.76ms
step:442/800 train_loss:4.1644 train_time:170968ms step_avg:395.76ms
step:443/800 train_loss:4.1424 train_time:171360ms step_avg:395.75ms
step:444/800 train_loss:4.0369 train_time:171756ms step_avg:395.75ms
step:445/800 train_loss:4.2945 train_time:172151ms step_avg:395.75ms
step:446/800 train_loss:4.2300 train_time:172546ms step_avg:395.75ms
step:447/800 train_loss:4.2303 train_time:172946ms step_avg:395.76ms
step:448/800 train_loss:4.1395 train_time:173335ms step_avg:395.74ms
step:449/800 train_loss:4.2345 train_time:173728ms step_avg:395.74ms
step:450/800 train_loss:4.0575 train_time:174123ms step_avg:395.73ms
step:451/800 train_loss:4.1071 train_time:174517ms step_avg:395.73ms
step:452/800 train_loss:3.9760 train_time:174911ms step_avg:395.73ms
step:453/800 train_loss:4.0868 train_time:175306ms step_avg:395.72ms
step:454/800 train_loss:4.0695 train_time:175703ms step_avg:395.73ms
step:455/800 train_loss:4.0291 train_time:176096ms step_avg:395.72ms
step:456/800 train_loss:4.2413 train_time:176490ms step_avg:395.72ms
step:457/800 train_loss:4.1060 train_time:176884ms step_avg:395.71ms
step:458/800 train_loss:4.1823 train_time:177280ms step_avg:395.71ms
step:459/800 train_loss:4.2211 train_time:177672ms step_avg:395.71ms
step:460/800 train_loss:4.0152 train_time:178067ms step_avg:395.70ms
step:461/800 train_loss:4.1927 train_time:178459ms step_avg:395.70ms
step:462/800 train_loss:4.0831 train_time:178854ms step_avg:395.70ms
step:463/800 train_loss:4.0888 train_time:179247ms step_avg:395.69ms
step:464/800 train_loss:4.1636 train_time:179641ms step_avg:395.69ms
step:465/800 train_loss:4.1000 train_time:180035ms step_avg:395.68ms
step:466/800 train_loss:4.1076 train_time:180431ms step_avg:395.68ms
step:467/800 train_loss:4.2087 train_time:180823ms step_avg:395.67ms
step:468/800 train_loss:4.2189 train_time:181217ms step_avg:395.67ms
step:469/800 train_loss:4.1911 train_time:181613ms step_avg:395.67ms
step:470/800 train_loss:4.0875 train_time:182009ms step_avg:395.67ms
step:471/800 train_loss:4.1695 train_time:182407ms step_avg:395.68ms
step:472/800 train_loss:4.2203 train_time:182804ms step_avg:395.68ms
step:473/800 train_loss:4.1444 train_time:183199ms step_avg:395.68ms
step:474/800 train_loss:4.1071 train_time:183592ms step_avg:395.67ms
step:475/800 train_loss:3.9631 train_time:183986ms step_avg:395.67ms
step:476/800 train_loss:4.3862 train_time:184380ms step_avg:395.67ms
step:477/800 train_loss:4.1600 train_time:184775ms step_avg:395.66ms
step:478/800 train_loss:3.9557 train_time:185169ms step_avg:395.66ms
step:479/800 train_loss:4.1833 train_time:185562ms step_avg:395.66ms
step:480/800 train_loss:4.1511 train_time:185955ms step_avg:395.65ms
step:481/800 train_loss:4.2897 train_time:186348ms step_avg:395.64ms
step:482/800 train_loss:4.0987 train_time:186742ms step_avg:395.64ms
step:483/800 train_loss:3.9141 train_time:187135ms step_avg:395.63ms
step:484/800 train_loss:4.1930 train_time:187531ms step_avg:395.63ms
step:485/800 train_loss:4.0396 train_time:187924ms step_avg:395.63ms
step:486/800 train_loss:4.0583 train_time:188320ms step_avg:395.63ms
step:487/800 train_loss:3.9796 train_time:188714ms step_avg:395.63ms
step:488/800 train_loss:4.0404 train_time:189108ms step_avg:395.62ms
step:489/800 train_loss:4.2421 train_time:189506ms step_avg:395.63ms
step:490/800 train_loss:4.0922 train_time:189902ms step_avg:395.63ms
step:491/800 train_loss:3.9837 train_time:190294ms step_avg:395.62ms
step:492/800 train_loss:3.9924 train_time:190688ms step_avg:395.62ms
step:493/800 train_loss:4.1087 train_time:191083ms step_avg:395.62ms
step:494/800 train_loss:3.9639 train_time:191476ms step_avg:395.61ms
step:495/800 train_loss:4.0977 train_time:191871ms step_avg:395.61ms
step:496/800 train_loss:4.0277 train_time:192264ms step_avg:395.60ms
step:497/800 train_loss:3.9239 train_time:192658ms step_avg:395.60ms
step:498/800 train_loss:4.1014 train_time:193051ms step_avg:395.60ms
step:499/800 train_loss:4.1852 train_time:193446ms step_avg:395.60ms
step:500/800 train_loss:4.2235 train_time:193840ms step_avg:395.59ms
step:500/800 val_loss:4.0836 train_time:193854ms step_avg:395.62ms
step:501/800 train_loss:4.1217 train_time:194237ms step_avg:395.59ms
step:502/800 train_loss:4.1634 train_time:194630ms step_avg:395.59ms
step:503/800 train_loss:4.1096 train_time:195027ms step_avg:395.59ms
step:504/800 train_loss:4.1529 train_time:195426ms step_avg:395.60ms
step:505/800 train_loss:4.1149 train_time:195819ms step_avg:395.59ms
step:506/800 train_loss:4.2063 train_time:196213ms step_avg:395.59ms
step:507/800 train_loss:3.9987 train_time:196606ms step_avg:395.59ms
step:508/800 train_loss:4.1356 train_time:197000ms step_avg:395.58ms
step:509/800 train_loss:4.2153 train_time:197394ms step_avg:395.58ms
step:510/800 train_loss:4.1501 train_time:197786ms step_avg:395.57ms
step:511/800 train_loss:3.9583 train_time:198180ms step_avg:395.57ms
step:512/800 train_loss:4.1619 train_time:198573ms step_avg:395.56ms
step:513/800 train_loss:4.0897 train_time:198969ms step_avg:395.56ms
step:514/800 train_loss:4.0576 train_time:199361ms step_avg:395.56ms
step:515/800 train_loss:4.1174 train_time:199756ms step_avg:395.56ms
step:516/800 train_loss:4.1203 train_time:200151ms step_avg:395.55ms
step:517/800 train_loss:4.4381 train_time:200543ms step_avg:395.55ms
step:518/800 train_loss:4.0308 train_time:200937ms step_avg:395.55ms
step:519/800 train_loss:4.1623 train_time:201331ms step_avg:395.54ms
step:520/800 train_loss:4.0717 train_time:201728ms step_avg:395.55ms
step:521/800 train_loss:4.0583 train_time:202123ms step_avg:395.54ms
step:522/800 train_loss:4.0001 train_time:202516ms step_avg:395.54ms
step:523/800 train_loss:4.0210 train_time:202910ms step_avg:395.54ms
step:524/800 train_loss:4.6312 train_time:203305ms step_avg:395.54ms
step:525/800 train_loss:4.1236 train_time:203699ms step_avg:395.53ms
step:526/800 train_loss:4.0659 train_time:204094ms step_avg:395.53ms
step:527/800 train_loss:4.0649 train_time:204486ms step_avg:395.52ms
step:528/800 train_loss:4.0185 train_time:204881ms step_avg:395.52ms
step:529/800 train_loss:3.9928 train_time:205274ms step_avg:395.52ms
step:530/800 train_loss:4.1965 train_time:205668ms step_avg:395.52ms
step:531/800 train_loss:4.0108 train_time:206062ms step_avg:395.51ms
step:532/800 train_loss:4.2892 train_time:206455ms step_avg:395.51ms
step:533/800 train_loss:4.0968 train_time:206850ms step_avg:395.51ms
step:534/800 train_loss:4.0268 train_time:207244ms step_avg:395.50ms
step:535/800 train_loss:4.0440 train_time:207639ms step_avg:395.50ms
step:536/800 train_loss:3.9837 train_time:208033ms step_avg:395.50ms
step:537/800 train_loss:4.1010 train_time:208429ms step_avg:395.50ms
step:538/800 train_loss:4.0968 train_time:208825ms step_avg:395.50ms
step:539/800 train_loss:4.0072 train_time:209218ms step_avg:395.50ms
step:540/800 train_loss:4.4867 train_time:209613ms step_avg:395.50ms
step:541/800 train_loss:4.0289 train_time:210007ms step_avg:395.49ms
step:542/800 train_loss:4.1465 train_time:210400ms step_avg:395.49ms
step:543/800 train_loss:3.9829 train_time:210796ms step_avg:395.49ms
step:544/800 train_loss:3.9587 train_time:211189ms step_avg:395.48ms
step:545/800 train_loss:4.0440 train_time:211583ms step_avg:395.48ms
step:546/800 train_loss:3.9650 train_time:211977ms step_avg:395.48ms
step:547/800 train_loss:4.0099 train_time:212371ms step_avg:395.48ms
step:548/800 train_loss:4.0214 train_time:212765ms step_avg:395.47ms
step:549/800 train_loss:3.9955 train_time:213159ms step_avg:395.47ms
step:550/800 train_loss:4.0881 train_time:213554ms step_avg:395.47ms
step:551/800 train_loss:3.9586 train_time:213948ms step_avg:395.47ms
step:552/800 train_loss:3.9888 train_time:214343ms step_avg:395.47ms
step:553/800 train_loss:4.3102 train_time:214736ms step_avg:395.46ms
step:554/800 train_loss:4.1092 train_time:215129ms step_avg:395.46ms
step:555/800 train_loss:4.0781 train_time:215528ms step_avg:395.46ms
step:556/800 train_loss:4.0379 train_time:215926ms step_avg:395.47ms
step:557/800 train_loss:4.0498 train_time:216319ms step_avg:395.46ms
step:558/800 train_loss:3.7171 train_time:216713ms step_avg:395.46ms
step:559/800 train_loss:3.9680 train_time:217107ms step_avg:395.46ms
step:560/800 train_loss:4.0142 train_time:217501ms step_avg:395.46ms
step:561/800 train_loss:4.0577 train_time:217894ms step_avg:395.45ms
step:562/800 train_loss:3.9715 train_time:218290ms step_avg:395.45ms
step:563/800 train_loss:3.9138 train_time:218682ms step_avg:395.45ms
step:564/800 train_loss:4.1160 train_time:219077ms step_avg:395.45ms
step:565/800 train_loss:3.9299 train_time:219471ms step_avg:395.44ms
step:566/800 train_loss:4.0484 train_time:219865ms step_avg:395.44ms
step:567/800 train_loss:4.0072 train_time:220379ms step_avg:395.65ms
step:568/800 train_loss:3.9500 train_time:220773ms step_avg:395.65ms
step:569/800 train_loss:4.0457 train_time:221171ms step_avg:395.65ms
step:570/800 train_loss:4.0189 train_time:221693ms step_avg:395.88ms
step:571/800 train_loss:4.0433 train_time:222085ms step_avg:395.87ms
step:572/800 train_loss:4.1314 train_time:222479ms step_avg:395.87ms
step:573/800 train_loss:4.0584 train_time:222872ms step_avg:395.86ms
step:574/800 train_loss:4.0694 train_time:223265ms step_avg:395.86ms
step:575/800 train_loss:4.1351 train_time:223661ms step_avg:395.86ms
step:576/800 train_loss:4.0953 train_time:224056ms step_avg:395.86ms
step:577/800 train_loss:4.0984 train_time:224449ms step_avg:395.85ms
step:578/800 train_loss:4.0450 train_time:224845ms step_avg:395.85ms
step:579/800 train_loss:4.0118 train_time:225240ms step_avg:395.85ms
step:580/800 train_loss:4.0125 train_time:225635ms step_avg:395.85ms
step:581/800 train_loss:3.9624 train_time:226029ms step_avg:395.85ms
step:582/800 train_loss:3.9880 train_time:226425ms step_avg:395.85ms
step:583/800 train_loss:4.2165 train_time:226818ms step_avg:395.84ms
step:584/800 train_loss:3.9856 train_time:227212ms step_avg:395.84ms
step:585/800 train_loss:3.9422 train_time:227606ms step_avg:395.84ms
step:586/800 train_loss:4.1300 train_time:228001ms step_avg:395.84ms
step:587/800 train_loss:3.8867 train_time:228395ms step_avg:395.83ms
step:588/800 train_loss:4.0200 train_time:228792ms step_avg:395.83ms
step:589/800 train_loss:4.0213 train_time:229185ms step_avg:395.83ms
step:590/800 train_loss:4.3618 train_time:229578ms step_avg:395.82ms
step:591/800 train_loss:4.1377 train_time:229972ms step_avg:395.82ms
step:592/800 train_loss:3.8821 train_time:230367ms step_avg:395.82ms
step:593/800 train_loss:3.8931 train_time:230763ms step_avg:395.82ms
step:594/800 train_loss:3.8888 train_time:231158ms step_avg:395.82ms
step:595/800 train_loss:3.9243 train_time:231552ms step_avg:395.82ms
step:596/800 train_loss:4.2936 train_time:231947ms step_avg:395.81ms
step:597/800 train_loss:4.0049 train_time:232341ms step_avg:395.81ms
step:598/800 train_loss:3.9516 train_time:232735ms step_avg:395.81ms
step:599/800 train_loss:4.0105 train_time:233129ms step_avg:395.81ms
step:600/800 train_loss:3.8330 train_time:233528ms step_avg:395.81ms
step:601/800 train_loss:3.9533 train_time:233926ms step_avg:395.81ms
step:602/800 train_loss:3.9864 train_time:234322ms step_avg:395.81ms
step:603/800 train_loss:3.9998 train_time:234716ms step_avg:395.81ms
step:604/800 train_loss:4.1310 train_time:235111ms step_avg:395.81ms
step:605/800 train_loss:3.9964 train_time:235505ms step_avg:395.81ms
step:606/800 train_loss:3.9725 train_time:235899ms step_avg:395.80ms
step:607/800 train_loss:3.9028 train_time:236293ms step_avg:395.80ms
step:608/800 train_loss:4.1515 train_time:236688ms step_avg:395.80ms
step:609/800 train_loss:3.9998 train_time:237081ms step_avg:395.79ms
step:610/800 train_loss:3.9643 train_time:237475ms step_avg:395.79ms
step:611/800 train_loss:4.0788 train_time:237868ms step_avg:395.79ms
step:612/800 train_loss:3.9843 train_time:238264ms step_avg:395.79ms
step:613/800 train_loss:3.9500 train_time:238658ms step_avg:395.78ms
step:614/800 train_loss:4.1173 train_time:239052ms step_avg:395.78ms
step:615/800 train_loss:4.0862 train_time:239446ms step_avg:395.78ms
step:616/800 train_loss:4.0433 train_time:239840ms step_avg:395.78ms
step:617/800 train_loss:3.9655 train_time:240233ms step_avg:395.77ms
step:618/800 train_loss:3.9165 train_time:240629ms step_avg:395.77ms
step:619/800 train_loss:4.0280 train_time:241024ms step_avg:395.77ms
step:620/800 train_loss:3.9405 train_time:241420ms step_avg:395.77ms
step:621/800 train_loss:3.9437 train_time:241815ms step_avg:395.77ms
step:622/800 train_loss:4.2371 train_time:242209ms step_avg:395.77ms
step:623/800 train_loss:3.9416 train_time:242602ms step_avg:395.76ms
step:624/800 train_loss:3.9755 train_time:242996ms step_avg:395.76ms
step:625/800 train_loss:4.0551 train_time:243390ms step_avg:395.76ms
step:625/800 val_loss:3.9809 train_time:243404ms step_avg:395.78ms
step:626/800 train_loss:4.0886 train_time:243788ms step_avg:395.76ms
step:627/800 train_loss:4.1017 train_time:244181ms step_avg:395.76ms
step:628/800 train_loss:4.0728 train_time:244574ms step_avg:395.75ms
step:629/800 train_loss:4.1265 train_time:244967ms step_avg:395.75ms
step:630/800 train_loss:3.9394 train_time:245361ms step_avg:395.74ms
step:631/800 train_loss:4.0696 train_time:245755ms step_avg:395.74ms
step:632/800 train_loss:4.1182 train_time:246148ms step_avg:395.74ms
step:633/800 train_loss:4.0127 train_time:246542ms step_avg:395.73ms
step:634/800 train_loss:3.9328 train_time:246938ms step_avg:395.73ms
step:635/800 train_loss:4.0319 train_time:247331ms step_avg:395.73ms
step:636/800 train_loss:4.2857 train_time:247726ms step_avg:395.73ms
step:637/800 train_loss:3.8830 train_time:248118ms step_avg:395.72ms
step:638/800 train_loss:3.6909 train_time:248514ms step_avg:395.72ms
step:639/800 train_loss:3.9335 train_time:248907ms step_avg:395.72ms
step:640/800 train_loss:3.9641 train_time:249303ms step_avg:395.72ms
step:641/800 train_loss:3.9369 train_time:249696ms step_avg:395.71ms
step:642/800 train_loss:3.9318 train_time:250092ms step_avg:395.72ms
step:643/800 train_loss:3.9784 train_time:250486ms step_avg:395.71ms
step:644/800 train_loss:3.9962 train_time:250880ms step_avg:395.71ms
step:645/800 train_loss:3.9123 train_time:251273ms step_avg:395.70ms
step:646/800 train_loss:4.1314 train_time:251667ms step_avg:395.70ms
step:647/800 train_loss:4.0257 train_time:252062ms step_avg:395.70ms
step:648/800 train_loss:4.0259 train_time:252456ms step_avg:395.70ms
step:649/800 train_loss:4.0353 train_time:252849ms step_avg:395.70ms
step:650/800 train_loss:4.1071 train_time:253244ms step_avg:395.69ms
step:651/800 train_loss:3.9687 train_time:253640ms step_avg:395.69ms
step:652/800 train_loss:4.1067 train_time:254033ms step_avg:395.69ms
step:653/800 train_loss:3.9371 train_time:254427ms step_avg:395.69ms
step:654/800 train_loss:4.0180 train_time:254820ms step_avg:395.68ms
step:655/800 train_loss:3.7744 train_time:255213ms step_avg:395.68ms
step:656/800 train_loss:3.9257 train_time:255607ms step_avg:395.68ms
step:657/800 train_loss:3.9345 train_time:256000ms step_avg:395.67ms
step:658/800 train_loss:3.8677 train_time:256395ms step_avg:395.67ms
step:659/800 train_loss:4.0509 train_time:256797ms step_avg:395.68ms
step:660/800 train_loss:3.9477 train_time:257191ms step_avg:395.68ms
step:661/800 train_loss:4.0265 train_time:257585ms step_avg:395.68ms
step:662/800 train_loss:4.1007 train_time:257978ms step_avg:395.67ms
step:663/800 train_loss:4.0082 train_time:258372ms step_avg:395.67ms
step:664/800 train_loss:3.8952 train_time:258766ms step_avg:395.67ms
step:665/800 train_loss:3.9769 train_time:259161ms step_avg:395.67ms
step:666/800 train_loss:3.8451 train_time:259554ms step_avg:395.66ms
step:667/800 train_loss:4.1450 train_time:259949ms step_avg:395.66ms
step:668/800 train_loss:3.9867 train_time:260344ms step_avg:395.66ms
step:669/800 train_loss:3.9713 train_time:260742ms step_avg:395.66ms
step:670/800 train_loss:3.8290 train_time:261139ms step_avg:395.67ms
step:671/800 train_loss:3.9410 train_time:261533ms step_avg:395.66ms
step:672/800 train_loss:3.9049 train_time:261928ms step_avg:395.66ms
step:673/800 train_loss:3.9294 train_time:262321ms step_avg:395.66ms
step:674/800 train_loss:4.2091 train_time:262715ms step_avg:395.66ms
step:675/800 train_loss:4.0007 train_time:263109ms step_avg:395.65ms
step:676/800 train_loss:4.0681 train_time:263503ms step_avg:395.65ms
step:677/800 train_loss:3.8378 train_time:263897ms step_avg:395.65ms
step:678/800 train_loss:3.9389 train_time:264291ms step_avg:395.64ms
step:679/800 train_loss:3.8882 train_time:264683ms step_avg:395.64ms
step:680/800 train_loss:4.0294 train_time:265077ms step_avg:395.64ms
step:681/800 train_loss:3.9378 train_time:265470ms step_avg:395.63ms
step:682/800 train_loss:3.9704 train_time:265865ms step_avg:395.63ms
step:683/800 train_loss:4.0352 train_time:266259ms step_avg:395.63ms
step:684/800 train_loss:4.0795 train_time:266652ms step_avg:395.63ms
step:685/800 train_loss:3.9806 train_time:267045ms step_avg:395.62ms
step:686/800 train_loss:4.0600 train_time:267441ms step_avg:395.62ms
step:687/800 train_loss:3.9807 train_time:267840ms step_avg:395.63ms
step:688/800 train_loss:4.0276 train_time:268238ms step_avg:395.63ms
step:689/800 train_loss:3.6446 train_time:268632ms step_avg:395.63ms
step:690/800 train_loss:3.7634 train_time:269025ms step_avg:395.63ms
step:691/800 train_loss:3.9055 train_time:269421ms step_avg:395.63ms
step:692/800 train_loss:3.7893 train_time:269815ms step_avg:395.62ms
step:693/800 train_loss:4.0059 train_time:270209ms step_avg:395.62ms
step:694/800 train_loss:4.0229 train_time:270602ms step_avg:395.62ms
step:695/800 train_loss:3.9046 train_time:270997ms step_avg:395.62ms
step:696/800 train_loss:3.8936 train_time:271390ms step_avg:395.61ms
step:697/800 train_loss:4.1919 train_time:271786ms step_avg:395.61ms
step:698/800 train_loss:3.9564 train_time:272191ms step_avg:395.63ms
step:699/800 train_loss:3.9865 train_time:272584ms step_avg:395.62ms
step:700/800 train_loss:4.1524 train_time:272979ms step_avg:395.62ms
step:701/800 train_loss:3.9308 train_time:273373ms step_avg:395.62ms
step:702/800 train_loss:3.8786 train_time:273766ms step_avg:395.62ms
step:703/800 train_loss:3.8756 train_time:274159ms step_avg:395.61ms
step:704/800 train_loss:3.8196 train_time:274553ms step_avg:395.61ms
step:705/800 train_loss:3.9103 train_time:274947ms step_avg:395.61ms
step:706/800 train_loss:3.9047 train_time:275341ms step_avg:395.60ms
step:707/800 train_loss:3.9301 train_time:275737ms step_avg:395.61ms
step:708/800 train_loss:3.9959 train_time:276131ms step_avg:395.60ms
step:709/800 train_loss:3.9391 train_time:276526ms step_avg:395.60ms
step:710/800 train_loss:3.9194 train_time:276920ms step_avg:395.60ms
step:711/800 train_loss:3.8965 train_time:277313ms step_avg:395.60ms
step:712/800 train_loss:3.9419 train_time:277708ms step_avg:395.59ms
step:713/800 train_loss:3.9992 train_time:278101ms step_avg:395.59ms
step:714/800 train_loss:4.0078 train_time:278495ms step_avg:395.59ms
step:715/800 train_loss:3.9157 train_time:278889ms step_avg:395.59ms
step:716/800 train_loss:3.9321 train_time:279283ms step_avg:395.58ms
step:717/800 train_loss:3.9437 train_time:279677ms step_avg:395.58ms
step:718/800 train_loss:4.0738 train_time:280070ms step_avg:395.58ms
step:719/800 train_loss:3.9484 train_time:280464ms step_avg:395.58ms
step:720/800 train_loss:4.0116 train_time:280857ms step_avg:395.57ms
step:721/800 train_loss:4.1796 train_time:281251ms step_avg:395.57ms
step:722/800 train_loss:3.8111 train_time:281644ms step_avg:395.57ms
step:723/800 train_loss:4.0680 train_time:282040ms step_avg:395.57ms
step:724/800 train_loss:4.1338 train_time:282438ms step_avg:395.57ms
step:725/800 train_loss:3.9071 train_time:282830ms step_avg:395.57ms
step:726/800 train_loss:3.9973 train_time:283224ms step_avg:395.56ms
step:727/800 train_loss:3.9048 train_time:283621ms step_avg:395.57ms
step:728/800 train_loss:3.8966 train_time:284015ms step_avg:395.56ms
step:729/800 train_loss:4.0733 train_time:284409ms step_avg:395.56ms
step:730/800 train_loss:4.0319 train_time:284803ms step_avg:395.56ms
step:731/800 train_loss:4.0402 train_time:285195ms step_avg:395.56ms
step:732/800 train_loss:3.9188 train_time:285590ms step_avg:395.55ms
step:733/800 train_loss:3.9416 train_time:285984ms step_avg:395.55ms
step:734/800 train_loss:4.1768 train_time:286379ms step_avg:395.55ms
step:735/800 train_loss:3.8984 train_time:286772ms step_avg:395.55ms
step:736/800 train_loss:3.9692 train_time:287166ms step_avg:395.54ms
step:737/800 train_loss:4.0971 train_time:287559ms step_avg:395.54ms
step:738/800 train_loss:4.0004 train_time:287952ms step_avg:395.54ms
step:739/800 train_loss:3.9508 train_time:288345ms step_avg:395.54ms
step:740/800 train_loss:3.8549 train_time:288742ms step_avg:395.54ms
step:741/800 train_loss:4.4966 train_time:289138ms step_avg:395.54ms
step:742/800 train_loss:3.8556 train_time:289531ms step_avg:395.53ms
step:743/800 train_loss:3.9402 train_time:289925ms step_avg:395.53ms
step:744/800 train_loss:3.9326 train_time:290319ms step_avg:395.53ms
step:745/800 train_loss:3.9928 train_time:290713ms step_avg:395.53ms
step:746/800 train_loss:3.9670 train_time:291108ms step_avg:395.53ms
step:747/800 train_loss:3.9505 train_time:291502ms step_avg:395.53ms
step:748/800 train_loss:3.9820 train_time:291893ms step_avg:395.52ms
step:749/800 train_loss:3.9052 train_time:292290ms step_avg:395.52ms
step:750/800 train_loss:3.9248 train_time:292684ms step_avg:395.52ms
step:750/800 val_loss:3.9255 train_time:292698ms step_avg:395.54ms
step:751/800 train_loss:3.9650 train_time:293082ms step_avg:395.52ms
step:752/800 train_loss:3.9088 train_time:293476ms step_avg:395.52ms
step:753/800 train_loss:3.9489 train_time:293870ms step_avg:395.52ms
step:754/800 train_loss:3.9734 train_time:294265ms step_avg:395.52ms
step:755/800 train_loss:3.9367 train_time:294663ms step_avg:395.52ms
step:756/800 train_loss:4.0203 train_time:295761ms step_avg:396.46ms
step:757/800 train_loss:3.8566 train_time:296158ms step_avg:396.46ms
step:758/800 train_loss:4.0734 train_time:296552ms step_avg:396.46ms
step:759/800 train_loss:3.9914 train_time:296946ms step_avg:396.46ms
step:760/800 train_loss:3.9231 train_time:297468ms step_avg:396.62ms
step:761/800 train_loss:4.0246 train_time:297865ms step_avg:396.62ms
step:762/800 train_loss:3.7495 train_time:298262ms step_avg:396.63ms
step:763/800 train_loss:3.9163 train_time:298658ms step_avg:396.62ms
step:764/800 train_loss:4.0181 train_time:299052ms step_avg:396.62ms
step:765/800 train_loss:3.6663 train_time:299445ms step_avg:396.62ms
step:766/800 train_loss:4.1073 train_time:299839ms step_avg:396.61ms
step:767/800 train_loss:3.9531 train_time:300231ms step_avg:396.61ms
step:768/800 train_loss:3.9069 train_time:300624ms step_avg:396.60ms
step:769/800 train_loss:3.9315 train_time:301018ms step_avg:396.60ms
step:770/800 train_loss:3.9529 train_time:301413ms step_avg:396.60ms
step:771/800 train_loss:4.0146 train_time:301805ms step_avg:396.59ms
step:772/800 train_loss:4.2334 train_time:302199ms step_avg:396.59ms
step:773/800 train_loss:3.8096 train_time:302592ms step_avg:396.58ms
step:774/800 train_loss:4.0171 train_time:302986ms step_avg:396.58ms
step:775/800 train_loss:3.9919 train_time:303379ms step_avg:396.57ms
step:776/800 train_loss:3.9561 train_time:303772ms step_avg:396.57ms
step:777/800 train_loss:3.7670 train_time:304166ms step_avg:396.57ms
step:778/800 train_loss:3.7645 train_time:304564ms step_avg:396.57ms
step:779/800 train_loss:3.8370 train_time:304958ms step_avg:396.56ms
step:780/800 train_loss:3.9212 train_time:305352ms step_avg:396.56ms
step:781/800 train_loss:3.9576 train_time:305747ms step_avg:396.56ms
step:782/800 train_loss:4.0166 train_time:306141ms step_avg:396.56ms
step:783/800 train_loss:3.9094 train_time:306535ms step_avg:396.55ms
step:784/800 train_loss:3.9413 train_time:306927ms step_avg:396.55ms
step:785/800 train_loss:3.9252 train_time:307321ms step_avg:396.54ms
step:786/800 train_loss:3.9206 train_time:307717ms step_avg:396.54ms
step:787/800 train_loss:3.8216 train_time:308110ms step_avg:396.54ms
step:788/800 train_loss:4.0797 train_time:308503ms step_avg:396.53ms
step:789/800 train_loss:3.8671 train_time:308898ms step_avg:396.53ms
step:790/800 train_loss:3.9334 train_time:309292ms step_avg:396.53ms
step:791/800 train_loss:3.9933 train_time:309686ms step_avg:396.52ms
step:792/800 train_loss:4.1222 train_time:310079ms step_avg:396.52ms
step:793/800 train_loss:4.1308 train_time:310475ms step_avg:396.52ms
step:794/800 train_loss:3.8637 train_time:310867ms step_avg:396.51ms
step:795/800 train_loss:3.9643 train_time:311263ms step_avg:396.51ms
step:796/800 train_loss:3.9993 train_time:311656ms step_avg:396.51ms
step:797/800 train_loss:4.1114 train_time:312050ms step_avg:396.51ms
step:798/800 train_loss:3.8770 train_time:312444ms step_avg:396.50ms
step:799/800 train_loss:4.0251 train_time:312838ms step_avg:396.50ms
step:800/800 train_loss:3.9273 train_time:313232ms step_avg:396.50ms
step:800/800 val_loss:3.9174 train_time:313247ms step_avg:396.52ms
