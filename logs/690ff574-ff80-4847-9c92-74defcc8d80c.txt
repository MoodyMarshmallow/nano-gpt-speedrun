====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import json
import dataclasses
import subprocess
import csv
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

def _apply_gate_act(logits: torch.Tensor, kind: str) -> torch.Tensor:
    if kind == "sigmoid":
        return torch.sigmoid(logits)
    if kind == "ns_sigmoid":
        return 0.5 + 0.5 * torch.sigmoid(logits)
    raise ValueError(f"unknown gate_act={kind}")

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.attn_gate = getattr(config, "attn_gate", "none")
        self.gate_pos = getattr(config, "gate_pos", "sdpa")
        self.gate_act = getattr(config, "gate_act", "sigmoid")
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.attn_gate == "headwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_head, bias=False)
            self.gate_param = None
        elif self.attn_gate == "elementwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_embd, bias=False)
            self.gate_param = None
        elif self.attn_gate == "const":
            self.c_gate = None
            self.gate_param = nn.Parameter(torch.zeros(self.n_head, self.head_dim))
        else:
            self.c_gate = None
            self.gate_param = None

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if self.attn_gate != "none" and self.gate_pos == "value":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            v = v * gate
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2) # (B, T, n_head, head_dim)
        if self.attn_gate != "none" and self.gate_pos == "sdpa":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            y = y * gate
        y = y.contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attn_gate : str = "none"
    gate_pos : str = "sdpa"
    gate_act : str = "sigmoid"

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    seed : int = 1337
    attn_gate : str = "none" # none|headwise|elementwise|const
    gate_pos : str = "sdpa" # sdpa|value
    gate_act : str = "sigmoid" # sigmoid|ns_sigmoid
    early_stop_patience : int = 0 # 0 disables early stopping
    early_stop_min_delta : float = 0.0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

def apply_env_overrides():
    # environment-variable overrides allow quick sweeps without editing code
    args.learning_rate = float(os.environ.get("LR", args.learning_rate))
    args.seed = int(os.environ.get("SEED", args.seed))
    args.warmup_iters = int(os.environ.get("WARMUP_ITERS", args.warmup_iters))
    args.num_iterations = int(os.environ.get("NUM_ITERATIONS", args.num_iterations))
    args.early_stop_patience = int(os.environ.get("EARLY_STOP_PATIENCE", args.early_stop_patience))
    args.early_stop_min_delta = float(os.environ.get("EARLY_STOP_MIN_DELTA", args.early_stop_min_delta))
    args.attn_gate = os.environ.get("ATTNGATE", args.attn_gate)
    args.gate_pos = os.environ.get("GATEPOS", args.gate_pos)
    args.gate_act = os.environ.get("GATEACT", args.gate_act)
    args.num_iterations = int(os.environ.get("NUM_ITER", args.num_iterations))
    args.val_loss_every = int(os.environ.get("VAL_EVERY", args.val_loss_every))

def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

apply_env_overrides()
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
np.random.seed(args.seed)

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
git_commit = get_git_commit() if master_process else "unknown"

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    attn_gate=args.attn_gate,
    gate_pos=args.gate_pos,
    gate_act=args.gate_act,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        f.write(f"git_commit: {git_commit}\n")
        f.write(f"seed: {args.seed}\n")
        f.write("hyperparameters:\n")
        f.write(json.dumps(dataclasses.asdict(args), indent=2))
        f.write("\n")
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
best_val_loss = float("inf")
final_val_loss = None
no_improve_count = 0
early_stop_reason = None
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        if not torch.isfinite(val_loss):
            early_stop_reason = "non-finite val_loss"
            final_val_loss = float("nan")
            val_loss_item = float("nan")
        else:
            val_loss_item = val_loss.item()
            final_val_loss = val_loss_item
            if val_loss_item < best_val_loss - args.early_stop_min_delta:
                best_val_loss = val_loss_item
                no_improve_count = 0
            else:
                no_improve_count += 1
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if early_stop_reason is None and args.early_stop_patience > 0 and no_improve_count >= args.early_stop_patience:
                early_stop_reason = f"early_stop patience={args.early_stop_patience} min_delta={args.early_stop_min_delta}"
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process in a subdir to keep logs tidy
        ckpt_dir = os.path.join(logdir, "checkpoints")
        os.makedirs(ckpt_dir, exist_ok=True)
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, os.path.join(ckpt_dir, f"state_step{step:06d}.pt"))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")
    if early_stop_reason is not None:
        break

if master_process:
    if early_stop_reason is not None:
        print(f"stopped early: {early_stop_reason}")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    timed_steps_final = max(args.num_iterations - 9, 1)
    ms_per_step = training_time_ms / timed_steps_final
    os.makedirs("experiments", exist_ok=True)
    results_path = os.path.join("experiments", "results.csv")
    fieldnames = [
        "run_id",
        "date",
        "git_commit",
        "seed",
        "attn_gate",
        "gate_pos",
        "gate_act",
        "learning_rate",
        "batch_size",
        "device_batch_size",
        "sequence_length",
        "num_iterations",
        "warmdown_iters",
        "final_val_loss",
        "best_val_loss",
        "train_time_ms",
        "ms_per_step",
        "gpu_name",
        "n_gpus",
        "runpod_instance",
        "notes",
    ]
    final_loss_value = final_val_loss if final_val_loss is not None else float("nan")
    best_loss_value = best_val_loss if best_val_loss < float("inf") else float("nan")
    row = {
        "run_id": run_id,
        "date": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        "git_commit": git_commit,
        "seed": args.seed,
        "attn_gate": args.attn_gate,
        "gate_pos": args.gate_pos,
        "gate_act": args.gate_act,
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "device_batch_size": args.device_batch_size,
        "sequence_length": args.sequence_length,
        "num_iterations": args.num_iterations,
        "warmdown_iters": args.warmdown_iters,
        "final_val_loss": final_loss_value,
        "best_val_loss": best_loss_value,
        "train_time_ms": training_time_ms,
        "ms_per_step": ms_per_step,
        "gpu_name": torch.cuda.get_device_name(ddp_local_rank),
        "n_gpus": ddp_world_size,
        "runpod_instance": os.environ.get("RUNPOD_INSTANCE_TYPE", "unknown"),
        "notes": "",
    }
    write_header = not os.path.exists(results_path)
    with open(results_path, "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerow(row)
====================================================================================================
git_commit: bf6d1ff4b5e4ce7280e822ab33e90c4fb250e604
seed: 2337
hyperparameters:
{
  "input_bin": "data/fineweb10B/fineweb_train_*.bin",
  "input_val_bin": "data/fineweb10B/fineweb_val_*.bin",
  "batch_size": 512,
  "device_batch_size": 64,
  "sequence_length": 1024,
  "num_iterations": 800,
  "learning_rate": 0.00468,
  "warmup_iters": 150,
  "warmdown_iters": 1450,
  "weight_decay": 0,
  "seed": 2337,
  "attn_gate": "elementwise",
  "gate_pos": "sdpa",
  "gate_act": "sigmoid",
  "early_stop_patience": 2,
  "early_stop_min_delta": 0.0,
  "val_loss_every": 125,
  "val_tokens": 10485760,
  "save_every": 0
}
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Sun Dec  7 18:34:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:00:07.0 Off |                    0 |
| N/A   46C    P0            115W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:00:08.0 Off |                    0 |
| N/A   48C    P0            118W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0            128W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000000:00:0A.0 Off |                    0 |
| N/A   45C    P0            114W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  |   00000000:00:0B.0 Off |                    0 |
| N/A   48C    P0            135W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  |   00000000:00:0C.0 Off |                    0 |
| N/A   45C    P0            110W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  |   00000000:00:0D.0 Off |                    0 |
| N/A   47C    P0            119W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  |   00000000:00:0E.0 Off |                    0 |
| N/A   46C    P0            138W /  300W |    2276MiB /  81920MiB |     19%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/800 val_loss:16.0092 train_time:252ms step_avg:nanms
step:1/800 train_loss:16.0082 train_time:41376ms step_avg:nanms
step:2/800 train_loss:15.8515 train_time:41839ms step_avg:nanms
step:3/800 train_loss:15.4511 train_time:42254ms step_avg:nanms
step:4/800 train_loss:14.6029 train_time:42668ms step_avg:nanms
step:5/800 train_loss:12.9940 train_time:43084ms step_avg:nanms
step:6/800 train_loss:11.1910 train_time:43500ms step_avg:nanms
step:7/800 train_loss:9.9062 train_time:43915ms step_avg:nanms
step:8/800 train_loss:9.6645 train_time:44331ms step_avg:nanms
step:9/800 train_loss:9.4769 train_time:44747ms step_avg:nanms
step:10/800 train_loss:9.2748 train_time:45162ms step_avg:nanms
step:11/800 train_loss:9.0565 train_time:401ms step_avg:nanms
step:12/800 train_loss:8.8696 train_time:815ms step_avg:nanms
step:13/800 train_loss:8.5871 train_time:1231ms step_avg:410.21ms
step:14/800 train_loss:8.4335 train_time:1644ms step_avg:411.02ms
step:15/800 train_loss:8.2466 train_time:2058ms step_avg:411.69ms
step:16/800 train_loss:8.0459 train_time:2472ms step_avg:411.94ms
step:17/800 train_loss:7.8908 train_time:2887ms step_avg:412.43ms
step:18/800 train_loss:7.7671 train_time:3301ms step_avg:412.68ms
step:19/800 train_loss:7.5321 train_time:3718ms step_avg:413.06ms
step:20/800 train_loss:7.4408 train_time:4131ms step_avg:413.15ms
step:21/800 train_loss:7.0821 train_time:4547ms step_avg:413.37ms
step:22/800 train_loss:7.4050 train_time:4961ms step_avg:413.43ms
step:23/800 train_loss:7.5877 train_time:5376ms step_avg:413.51ms
step:24/800 train_loss:7.2468 train_time:5790ms step_avg:413.59ms
step:25/800 train_loss:7.3323 train_time:6204ms step_avg:413.59ms
step:26/800 train_loss:7.0812 train_time:6617ms step_avg:413.57ms
step:27/800 train_loss:6.9648 train_time:7033ms step_avg:413.73ms
step:28/800 train_loss:7.0974 train_time:7449ms step_avg:413.82ms
step:29/800 train_loss:6.7413 train_time:7867ms step_avg:414.03ms
step:30/800 train_loss:6.9708 train_time:8283ms step_avg:414.16ms
step:31/800 train_loss:6.8200 train_time:8699ms step_avg:414.23ms
step:32/800 train_loss:6.7483 train_time:9115ms step_avg:414.30ms
step:33/800 train_loss:6.5285 train_time:9530ms step_avg:414.36ms
step:34/800 train_loss:6.9626 train_time:9948ms step_avg:414.49ms
step:35/800 train_loss:6.7342 train_time:10364ms step_avg:414.56ms
step:36/800 train_loss:6.9253 train_time:10780ms step_avg:414.60ms
step:37/800 train_loss:6.8077 train_time:11197ms step_avg:414.72ms
step:38/800 train_loss:6.6811 train_time:11613ms step_avg:414.73ms
step:39/800 train_loss:6.5407 train_time:12030ms step_avg:414.83ms
step:40/800 train_loss:6.6360 train_time:12447ms step_avg:414.90ms
step:41/800 train_loss:6.5106 train_time:12863ms step_avg:414.94ms
step:42/800 train_loss:6.5441 train_time:13281ms step_avg:415.03ms
step:43/800 train_loss:6.3776 train_time:13697ms step_avg:415.06ms
step:44/800 train_loss:6.4733 train_time:14113ms step_avg:415.08ms
step:45/800 train_loss:6.4589 train_time:14529ms step_avg:415.11ms
step:46/800 train_loss:6.6668 train_time:14946ms step_avg:415.16ms
step:47/800 train_loss:6.4594 train_time:15364ms step_avg:415.23ms
step:48/800 train_loss:6.2920 train_time:15780ms step_avg:415.26ms
step:49/800 train_loss:6.5385 train_time:16196ms step_avg:415.29ms
step:50/800 train_loss:6.3997 train_time:16613ms step_avg:415.33ms
step:51/800 train_loss:6.5531 train_time:17031ms step_avg:415.38ms
step:52/800 train_loss:6.3976 train_time:17446ms step_avg:415.39ms
step:53/800 train_loss:6.2346 train_time:17864ms step_avg:415.44ms
step:54/800 train_loss:6.3620 train_time:18281ms step_avg:415.48ms
step:55/800 train_loss:6.2940 train_time:18699ms step_avg:415.54ms
step:56/800 train_loss:6.6038 train_time:19117ms step_avg:415.59ms
step:57/800 train_loss:6.2681 train_time:19535ms step_avg:415.64ms
step:58/800 train_loss:6.1432 train_time:19953ms step_avg:415.69ms
step:59/800 train_loss:6.3249 train_time:20372ms step_avg:415.76ms
step:60/800 train_loss:6.2335 train_time:20789ms step_avg:415.78ms
step:61/800 train_loss:6.3446 train_time:21205ms step_avg:415.78ms
step:62/800 train_loss:6.1461 train_time:21624ms step_avg:415.84ms
step:63/800 train_loss:6.2270 train_time:22041ms step_avg:415.87ms
step:64/800 train_loss:6.1825 train_time:22458ms step_avg:415.88ms
step:65/800 train_loss:6.5478 train_time:22876ms step_avg:415.93ms
step:66/800 train_loss:6.0188 train_time:23293ms step_avg:415.95ms
step:67/800 train_loss:6.1841 train_time:23711ms step_avg:415.98ms
step:68/800 train_loss:6.0414 train_time:24127ms step_avg:415.98ms
step:69/800 train_loss:6.3645 train_time:24545ms step_avg:416.01ms
step:70/800 train_loss:5.9639 train_time:24962ms step_avg:416.04ms
step:71/800 train_loss:6.0179 train_time:25381ms step_avg:416.08ms
step:72/800 train_loss:6.2170 train_time:25798ms step_avg:416.09ms
step:73/800 train_loss:6.1306 train_time:26214ms step_avg:416.10ms
step:74/800 train_loss:6.0284 train_time:26632ms step_avg:416.13ms
step:75/800 train_loss:6.1309 train_time:27049ms step_avg:416.14ms
step:76/800 train_loss:6.0668 train_time:27471ms step_avg:416.23ms
step:77/800 train_loss:6.0725 train_time:27888ms step_avg:416.24ms
step:78/800 train_loss:6.1410 train_time:28306ms step_avg:416.26ms
step:79/800 train_loss:6.2164 train_time:28724ms step_avg:416.28ms
step:80/800 train_loss:6.0416 train_time:29143ms step_avg:416.33ms
step:81/800 train_loss:6.1360 train_time:29559ms step_avg:416.33ms
step:82/800 train_loss:5.8587 train_time:29975ms step_avg:416.32ms
step:83/800 train_loss:6.0509 train_time:30394ms step_avg:416.36ms
step:84/800 train_loss:6.0278 train_time:30812ms step_avg:416.37ms
step:85/800 train_loss:5.9601 train_time:31227ms step_avg:416.36ms
step:86/800 train_loss:5.8212 train_time:31645ms step_avg:416.38ms
step:87/800 train_loss:6.0392 train_time:32064ms step_avg:416.41ms
step:88/800 train_loss:5.9329 train_time:32483ms step_avg:416.45ms
step:89/800 train_loss:6.0062 train_time:32901ms step_avg:416.47ms
step:90/800 train_loss:5.9965 train_time:33319ms step_avg:416.49ms
step:91/800 train_loss:5.8935 train_time:33736ms step_avg:416.50ms
step:92/800 train_loss:5.8848 train_time:34154ms step_avg:416.51ms
step:93/800 train_loss:5.9892 train_time:34574ms step_avg:416.55ms
step:94/800 train_loss:5.8501 train_time:34994ms step_avg:416.59ms
step:95/800 train_loss:5.8180 train_time:35411ms step_avg:416.60ms
step:96/800 train_loss:5.8227 train_time:35828ms step_avg:416.60ms
step:97/800 train_loss:5.7360 train_time:36246ms step_avg:416.62ms
step:98/800 train_loss:5.8279 train_time:36664ms step_avg:416.63ms
step:99/800 train_loss:5.7336 train_time:37081ms step_avg:416.64ms
step:100/800 train_loss:5.8693 train_time:37499ms step_avg:416.65ms
step:101/800 train_loss:5.8140 train_time:37917ms step_avg:416.66ms
step:102/800 train_loss:5.6953 train_time:38336ms step_avg:416.69ms
step:103/800 train_loss:5.8152 train_time:38754ms step_avg:416.71ms
step:104/800 train_loss:5.7846 train_time:39173ms step_avg:416.73ms
step:105/800 train_loss:5.6040 train_time:39591ms step_avg:416.75ms
step:106/800 train_loss:5.7299 train_time:40009ms step_avg:416.76ms
step:107/800 train_loss:5.9656 train_time:40425ms step_avg:416.75ms
step:108/800 train_loss:5.7092 train_time:40842ms step_avg:416.76ms
step:109/800 train_loss:5.4338 train_time:41261ms step_avg:416.77ms
step:110/800 train_loss:5.6765 train_time:41679ms step_avg:416.79ms
step:111/800 train_loss:5.6232 train_time:42098ms step_avg:416.81ms
step:112/800 train_loss:5.5986 train_time:42517ms step_avg:416.83ms
step:113/800 train_loss:5.6953 train_time:42935ms step_avg:416.84ms
step:114/800 train_loss:5.6270 train_time:43353ms step_avg:416.86ms
step:115/800 train_loss:5.4579 train_time:43773ms step_avg:416.89ms
step:116/800 train_loss:5.6403 train_time:44192ms step_avg:416.90ms
step:117/800 train_loss:5.4748 train_time:44608ms step_avg:416.90ms
step:118/800 train_loss:5.4732 train_time:45026ms step_avg:416.90ms
step:119/800 train_loss:5.5700 train_time:45443ms step_avg:416.90ms
step:120/800 train_loss:5.6026 train_time:45861ms step_avg:416.91ms
step:121/800 train_loss:5.5067 train_time:46280ms step_avg:416.94ms
step:122/800 train_loss:5.3984 train_time:46696ms step_avg:416.93ms
step:123/800 train_loss:5.4852 train_time:47114ms step_avg:416.94ms
step:124/800 train_loss:5.3580 train_time:47533ms step_avg:416.96ms
step:125/800 train_loss:5.6710 train_time:47950ms step_avg:416.95ms
step:125/800 val_loss:5.4952 train_time:47968ms step_avg:417.12ms
step:126/800 train_loss:5.5137 train_time:48376ms step_avg:417.04ms
step:127/800 train_loss:5.4800 train_time:48794ms step_avg:417.04ms
step:128/800 train_loss:5.5368 train_time:49211ms step_avg:417.04ms
step:129/800 train_loss:5.3890 train_time:49628ms step_avg:417.04ms
step:130/800 train_loss:5.6708 train_time:50047ms step_avg:417.06ms
step:131/800 train_loss:5.4599 train_time:50465ms step_avg:417.07ms
step:132/800 train_loss:5.4542 train_time:50883ms step_avg:417.08ms
step:133/800 train_loss:5.3837 train_time:51302ms step_avg:417.09ms
step:134/800 train_loss:5.4087 train_time:51722ms step_avg:417.11ms
step:135/800 train_loss:5.3548 train_time:52144ms step_avg:417.15ms
step:136/800 train_loss:5.4155 train_time:52561ms step_avg:417.15ms
step:137/800 train_loss:5.2165 train_time:52980ms step_avg:417.16ms
step:138/800 train_loss:5.3685 train_time:53396ms step_avg:417.16ms
step:139/800 train_loss:5.3553 train_time:53814ms step_avg:417.17ms
step:140/800 train_loss:5.3524 train_time:54234ms step_avg:417.18ms
step:141/800 train_loss:5.4002 train_time:54653ms step_avg:417.20ms
step:142/800 train_loss:5.2780 train_time:55070ms step_avg:417.20ms
step:143/800 train_loss:5.3593 train_time:55487ms step_avg:417.20ms
step:144/800 train_loss:5.1484 train_time:55907ms step_avg:417.22ms
step:145/800 train_loss:5.3289 train_time:56326ms step_avg:417.23ms
step:146/800 train_loss:5.2621 train_time:56745ms step_avg:417.24ms
step:147/800 train_loss:5.1636 train_time:57195ms step_avg:417.48ms
step:148/800 train_loss:5.2959 train_time:57611ms step_avg:417.47ms
step:149/800 train_loss:5.2701 train_time:58030ms step_avg:417.48ms
step:150/800 train_loss:5.3270 train_time:58447ms step_avg:417.48ms
step:151/800 train_loss:5.3340 train_time:58864ms step_avg:417.47ms
step:152/800 train_loss:5.2168 train_time:59282ms step_avg:417.48ms
step:153/800 train_loss:5.1872 train_time:59699ms step_avg:417.48ms
step:154/800 train_loss:5.2535 train_time:60116ms step_avg:417.47ms
step:155/800 train_loss:5.1829 train_time:60534ms step_avg:417.48ms
step:156/800 train_loss:5.1552 train_time:60951ms step_avg:417.47ms
step:157/800 train_loss:5.1612 train_time:61370ms step_avg:417.48ms
step:158/800 train_loss:5.2948 train_time:61786ms step_avg:417.47ms
step:159/800 train_loss:5.0693 train_time:62202ms step_avg:417.47ms
step:160/800 train_loss:5.1260 train_time:62620ms step_avg:417.47ms
step:161/800 train_loss:4.9758 train_time:63037ms step_avg:417.46ms
step:162/800 train_loss:5.1317 train_time:63455ms step_avg:417.47ms
step:163/800 train_loss:5.1650 train_time:63872ms step_avg:417.47ms
step:164/800 train_loss:5.1551 train_time:64291ms step_avg:417.47ms
step:165/800 train_loss:4.9600 train_time:64707ms step_avg:417.47ms
step:166/800 train_loss:5.0796 train_time:65126ms step_avg:417.47ms
step:167/800 train_loss:5.2394 train_time:65547ms step_avg:417.49ms
step:168/800 train_loss:5.0172 train_time:65964ms step_avg:417.50ms
step:169/800 train_loss:5.1046 train_time:66382ms step_avg:417.50ms
step:170/800 train_loss:4.9562 train_time:66799ms step_avg:417.49ms
step:171/800 train_loss:4.8852 train_time:67218ms step_avg:417.50ms
step:172/800 train_loss:5.0036 train_time:67636ms step_avg:417.51ms
step:173/800 train_loss:4.9764 train_time:68053ms step_avg:417.50ms
step:174/800 train_loss:5.0339 train_time:68473ms step_avg:417.52ms
step:175/800 train_loss:5.1780 train_time:68888ms step_avg:417.51ms
step:176/800 train_loss:5.0524 train_time:69306ms step_avg:417.51ms
step:177/800 train_loss:4.8946 train_time:69724ms step_avg:417.51ms
step:178/800 train_loss:4.8657 train_time:70145ms step_avg:417.53ms
step:179/800 train_loss:4.9105 train_time:70563ms step_avg:417.53ms
step:180/800 train_loss:4.9495 train_time:70982ms step_avg:417.54ms
step:181/800 train_loss:4.9226 train_time:71399ms step_avg:417.54ms
step:182/800 train_loss:5.0510 train_time:71818ms step_avg:417.54ms
step:183/800 train_loss:4.9270 train_time:72236ms step_avg:417.55ms
step:184/800 train_loss:4.8603 train_time:72652ms step_avg:417.54ms
step:185/800 train_loss:4.8839 train_time:73069ms step_avg:417.54ms
step:186/800 train_loss:5.0094 train_time:73487ms step_avg:417.54ms
step:187/800 train_loss:4.8998 train_time:73906ms step_avg:417.55ms
step:188/800 train_loss:5.1458 train_time:74322ms step_avg:417.54ms
step:189/800 train_loss:4.9331 train_time:74849ms step_avg:418.15ms
step:190/800 train_loss:4.8431 train_time:75394ms step_avg:418.86ms
step:191/800 train_loss:5.0026 train_time:75812ms step_avg:418.85ms
step:192/800 train_loss:4.8389 train_time:76230ms step_avg:418.85ms
step:193/800 train_loss:4.7615 train_time:76649ms step_avg:418.84ms
step:194/800 train_loss:4.9729 train_time:77066ms step_avg:418.84ms
step:195/800 train_loss:4.9020 train_time:77485ms step_avg:418.84ms
step:196/800 train_loss:5.0977 train_time:77904ms step_avg:418.84ms
step:197/800 train_loss:4.9803 train_time:78323ms step_avg:418.84ms
step:198/800 train_loss:4.8090 train_time:78744ms step_avg:418.85ms
step:199/800 train_loss:4.8559 train_time:79163ms step_avg:418.85ms
step:200/800 train_loss:4.7431 train_time:79580ms step_avg:418.84ms
step:201/800 train_loss:4.8324 train_time:79996ms step_avg:418.83ms
step:202/800 train_loss:4.7546 train_time:80413ms step_avg:418.82ms
step:203/800 train_loss:4.9891 train_time:80831ms step_avg:418.82ms
step:204/800 train_loss:4.8723 train_time:81249ms step_avg:418.81ms
step:205/800 train_loss:4.8484 train_time:81668ms step_avg:418.81ms
step:206/800 train_loss:4.9996 train_time:82086ms step_avg:418.80ms
step:207/800 train_loss:4.6738 train_time:82504ms step_avg:418.80ms
step:208/800 train_loss:4.8216 train_time:82921ms step_avg:418.79ms
step:209/800 train_loss:4.7819 train_time:83342ms step_avg:418.81ms
step:210/800 train_loss:4.9474 train_time:83760ms step_avg:418.80ms
step:211/800 train_loss:4.8655 train_time:84178ms step_avg:418.80ms
step:212/800 train_loss:4.7451 train_time:84596ms step_avg:418.79ms
step:213/800 train_loss:4.9012 train_time:85013ms step_avg:418.78ms
step:214/800 train_loss:4.7169 train_time:85433ms step_avg:418.79ms
step:215/800 train_loss:4.8052 train_time:85851ms step_avg:418.78ms
step:216/800 train_loss:4.6649 train_time:86268ms step_avg:418.78ms
step:217/800 train_loss:4.8031 train_time:86686ms step_avg:418.77ms
step:218/800 train_loss:4.7673 train_time:87105ms step_avg:418.77ms
step:219/800 train_loss:4.7385 train_time:87522ms step_avg:418.77ms
step:220/800 train_loss:4.7369 train_time:87943ms step_avg:418.78ms
step:221/800 train_loss:4.7794 train_time:88361ms step_avg:418.77ms
step:222/800 train_loss:4.8182 train_time:88778ms step_avg:418.76ms
step:223/800 train_loss:4.7515 train_time:89197ms step_avg:418.76ms
step:224/800 train_loss:4.7640 train_time:89614ms step_avg:418.76ms
step:225/800 train_loss:4.8921 train_time:90031ms step_avg:418.75ms
step:226/800 train_loss:4.6317 train_time:90450ms step_avg:418.75ms
step:227/800 train_loss:4.6541 train_time:90872ms step_avg:418.76ms
step:228/800 train_loss:4.6423 train_time:91289ms step_avg:418.76ms
step:229/800 train_loss:4.8067 train_time:91706ms step_avg:418.75ms
step:230/800 train_loss:4.6406 train_time:92123ms step_avg:418.74ms
step:231/800 train_loss:4.7860 train_time:92546ms step_avg:418.76ms
step:232/800 train_loss:4.6503 train_time:92963ms step_avg:418.75ms
step:233/800 train_loss:4.6144 train_time:93380ms step_avg:418.74ms
step:234/800 train_loss:4.8173 train_time:93798ms step_avg:418.74ms
step:235/800 train_loss:4.6514 train_time:94217ms step_avg:418.74ms
step:236/800 train_loss:4.5890 train_time:94635ms step_avg:418.74ms
step:237/800 train_loss:4.8305 train_time:95052ms step_avg:418.73ms
step:238/800 train_loss:4.7270 train_time:95470ms step_avg:418.73ms
step:239/800 train_loss:4.6368 train_time:95887ms step_avg:418.72ms
step:240/800 train_loss:4.7776 train_time:96305ms step_avg:418.72ms
step:241/800 train_loss:4.7592 train_time:96723ms step_avg:418.71ms
step:242/800 train_loss:4.6627 train_time:97144ms step_avg:418.73ms
step:243/800 train_loss:4.8418 train_time:97562ms step_avg:418.72ms
step:244/800 train_loss:4.6504 train_time:97980ms step_avg:418.72ms
step:245/800 train_loss:4.6617 train_time:98397ms step_avg:418.71ms
step:246/800 train_loss:4.7363 train_time:98815ms step_avg:418.71ms
step:247/800 train_loss:4.6884 train_time:99234ms step_avg:418.71ms
step:248/800 train_loss:4.6411 train_time:99650ms step_avg:418.70ms
step:249/800 train_loss:4.8064 train_time:100068ms step_avg:418.70ms
step:250/800 train_loss:4.5417 train_time:100486ms step_avg:418.69ms
step:250/800 val_loss:4.6545 train_time:100500ms step_avg:418.75ms
step:251/800 train_loss:4.5872 train_time:100905ms step_avg:418.69ms
step:252/800 train_loss:4.7166 train_time:101332ms step_avg:418.73ms
step:253/800 train_loss:4.7178 train_time:101749ms step_avg:418.72ms
step:254/800 train_loss:4.5860 train_time:102167ms step_avg:418.72ms
step:255/800 train_loss:4.6016 train_time:102585ms step_avg:418.71ms
step:256/800 train_loss:4.7539 train_time:103005ms step_avg:418.72ms
step:257/800 train_loss:4.6887 train_time:103423ms step_avg:418.72ms
step:258/800 train_loss:4.6476 train_time:103842ms step_avg:418.72ms
step:259/800 train_loss:4.5781 train_time:104258ms step_avg:418.71ms
step:260/800 train_loss:4.5965 train_time:104674ms step_avg:418.70ms
step:261/800 train_loss:4.6737 train_time:105092ms step_avg:418.69ms
step:262/800 train_loss:4.6811 train_time:105510ms step_avg:418.69ms
step:263/800 train_loss:4.5813 train_time:105927ms step_avg:418.68ms
step:264/800 train_loss:4.5177 train_time:106346ms step_avg:418.69ms
step:265/800 train_loss:4.5768 train_time:106762ms step_avg:418.68ms
step:266/800 train_loss:4.4422 train_time:107181ms step_avg:418.68ms
step:267/800 train_loss:4.4921 train_time:107602ms step_avg:418.69ms
step:268/800 train_loss:4.5383 train_time:108039ms step_avg:418.75ms
step:269/800 train_loss:4.4905 train_time:108458ms step_avg:418.76ms
step:270/800 train_loss:4.4486 train_time:108875ms step_avg:418.75ms
step:271/800 train_loss:4.6872 train_time:109292ms step_avg:418.74ms
step:272/800 train_loss:4.6094 train_time:109709ms step_avg:418.74ms
step:273/800 train_loss:4.4721 train_time:110128ms step_avg:418.74ms
step:274/800 train_loss:4.5196 train_time:110545ms step_avg:418.73ms
step:275/800 train_loss:4.6403 train_time:110962ms step_avg:418.72ms
step:276/800 train_loss:4.6552 train_time:111380ms step_avg:418.72ms
step:277/800 train_loss:4.8561 train_time:111799ms step_avg:418.72ms
step:278/800 train_loss:4.5973 train_time:112216ms step_avg:418.72ms
step:279/800 train_loss:4.7238 train_time:112635ms step_avg:418.72ms
step:280/800 train_loss:4.5710 train_time:113053ms step_avg:418.71ms
step:281/800 train_loss:4.6537 train_time:113469ms step_avg:418.71ms
step:282/800 train_loss:4.5349 train_time:113886ms step_avg:418.70ms
step:283/800 train_loss:4.6303 train_time:114306ms step_avg:418.70ms
step:284/800 train_loss:4.4637 train_time:114725ms step_avg:418.70ms
step:285/800 train_loss:4.6321 train_time:115148ms step_avg:418.72ms
step:286/800 train_loss:4.6236 train_time:115568ms step_avg:418.73ms
step:287/800 train_loss:4.6650 train_time:115987ms step_avg:418.73ms
step:288/800 train_loss:4.5113 train_time:116410ms step_avg:418.74ms
step:289/800 train_loss:4.5839 train_time:116827ms step_avg:418.73ms
step:290/800 train_loss:4.4438 train_time:117245ms step_avg:418.73ms
step:291/800 train_loss:4.4344 train_time:117662ms step_avg:418.73ms
step:292/800 train_loss:4.5359 train_time:118080ms step_avg:418.72ms
step:293/800 train_loss:4.4378 train_time:118499ms step_avg:418.72ms
step:294/800 train_loss:4.4952 train_time:118917ms step_avg:418.72ms
step:295/800 train_loss:4.5063 train_time:119333ms step_avg:418.71ms
step:296/800 train_loss:4.3813 train_time:119751ms step_avg:418.71ms
step:297/800 train_loss:4.3674 train_time:120168ms step_avg:418.70ms
step:298/800 train_loss:4.4011 train_time:120587ms step_avg:418.70ms
step:299/800 train_loss:4.5087 train_time:121006ms step_avg:418.71ms
step:300/800 train_loss:4.3815 train_time:121423ms step_avg:418.70ms
step:301/800 train_loss:4.5666 train_time:121841ms step_avg:418.70ms
step:302/800 train_loss:4.5409 train_time:122259ms step_avg:418.69ms
step:303/800 train_loss:4.4525 train_time:122677ms step_avg:418.69ms
step:304/800 train_loss:4.5320 train_time:123093ms step_avg:418.68ms
step:305/800 train_loss:4.5086 train_time:123511ms step_avg:418.68ms
step:306/800 train_loss:4.9898 train_time:123928ms step_avg:418.68ms
step:307/800 train_loss:4.4583 train_time:124346ms step_avg:418.67ms
step:308/800 train_loss:4.3699 train_time:124777ms step_avg:418.71ms
step:309/800 train_loss:4.5475 train_time:125194ms step_avg:418.71ms
step:310/800 train_loss:4.3482 train_time:125612ms step_avg:418.71ms
step:311/800 train_loss:4.5905 train_time:126030ms step_avg:418.71ms
step:312/800 train_loss:4.4878 train_time:126446ms step_avg:418.70ms
step:313/800 train_loss:4.3985 train_time:126863ms step_avg:418.69ms
step:314/800 train_loss:4.5250 train_time:127281ms step_avg:418.69ms
step:315/800 train_loss:4.6626 train_time:127698ms step_avg:418.68ms
step:316/800 train_loss:4.4944 train_time:128116ms step_avg:418.68ms
step:317/800 train_loss:4.3729 train_time:128533ms step_avg:418.67ms
step:318/800 train_loss:4.3876 train_time:128950ms step_avg:418.67ms
step:319/800 train_loss:4.4100 train_time:129366ms step_avg:418.66ms
step:320/800 train_loss:4.3653 train_time:129783ms step_avg:418.65ms
step:321/800 train_loss:4.4605 train_time:130204ms step_avg:418.66ms
step:322/800 train_loss:4.4583 train_time:130622ms step_avg:418.66ms
step:323/800 train_loss:4.4233 train_time:131041ms step_avg:418.66ms
step:324/800 train_loss:4.5018 train_time:131460ms step_avg:418.66ms
step:325/800 train_loss:4.4750 train_time:131877ms step_avg:418.66ms
step:326/800 train_loss:4.5505 train_time:132297ms step_avg:418.66ms
step:327/800 train_loss:4.4034 train_time:132714ms step_avg:418.66ms
step:328/800 train_loss:4.8749 train_time:133131ms step_avg:418.65ms
step:329/800 train_loss:4.5651 train_time:133549ms step_avg:418.65ms
step:330/800 train_loss:4.3308 train_time:133966ms step_avg:418.64ms
step:331/800 train_loss:4.2960 train_time:134385ms step_avg:418.64ms
step:332/800 train_loss:4.4693 train_time:134806ms step_avg:418.65ms
step:333/800 train_loss:4.3837 train_time:135225ms step_avg:418.65ms
step:334/800 train_loss:4.3741 train_time:135642ms step_avg:418.65ms
step:335/800 train_loss:4.3246 train_time:136060ms step_avg:418.65ms
step:336/800 train_loss:4.5105 train_time:136476ms step_avg:418.64ms
step:337/800 train_loss:4.4442 train_time:136893ms step_avg:418.63ms
step:338/800 train_loss:4.9898 train_time:137310ms step_avg:418.63ms
step:339/800 train_loss:4.4238 train_time:137727ms step_avg:418.62ms
step:340/800 train_loss:4.3876 train_time:138145ms step_avg:418.62ms
step:341/800 train_loss:4.3825 train_time:138562ms step_avg:418.62ms
step:342/800 train_loss:4.3152 train_time:138980ms step_avg:418.61ms
step:343/800 train_loss:4.2878 train_time:139396ms step_avg:418.61ms
step:344/800 train_loss:4.3469 train_time:139814ms step_avg:418.61ms
step:345/800 train_loss:4.4553 train_time:140232ms step_avg:418.60ms
step:346/800 train_loss:4.3185 train_time:140650ms step_avg:418.60ms
step:347/800 train_loss:4.2558 train_time:141068ms step_avg:418.60ms
step:348/800 train_loss:4.3057 train_time:141486ms step_avg:418.60ms
step:349/800 train_loss:4.3189 train_time:141906ms step_avg:418.60ms
step:350/800 train_loss:4.2585 train_time:142324ms step_avg:418.60ms
step:351/800 train_loss:3.9559 train_time:142740ms step_avg:418.59ms
step:352/800 train_loss:4.2424 train_time:143157ms step_avg:418.59ms
step:353/800 train_loss:4.6057 train_time:143575ms step_avg:418.59ms
step:354/800 train_loss:4.1151 train_time:143994ms step_avg:418.59ms
step:355/800 train_loss:4.3633 train_time:144410ms step_avg:418.58ms
step:356/800 train_loss:4.2577 train_time:144829ms step_avg:418.58ms
step:357/800 train_loss:4.3497 train_time:145246ms step_avg:418.58ms
step:358/800 train_loss:4.3461 train_time:145663ms step_avg:418.57ms
step:359/800 train_loss:4.2823 train_time:146080ms step_avg:418.57ms
step:360/800 train_loss:4.5301 train_time:146498ms step_avg:418.57ms
step:361/800 train_loss:3.9678 train_time:146915ms step_avg:418.56ms
step:362/800 train_loss:4.4724 train_time:147332ms step_avg:418.56ms
step:363/800 train_loss:4.3691 train_time:147749ms step_avg:418.55ms
step:364/800 train_loss:4.2633 train_time:148167ms step_avg:418.55ms
step:365/800 train_loss:4.1920 train_time:148584ms step_avg:418.55ms
step:366/800 train_loss:4.3581 train_time:149006ms step_avg:418.56ms
step:367/800 train_loss:4.2953 train_time:149424ms step_avg:418.55ms
step:368/800 train_loss:4.2761 train_time:149843ms step_avg:418.56ms
step:369/800 train_loss:4.2730 train_time:150261ms step_avg:418.55ms
step:370/800 train_loss:4.1669 train_time:150679ms step_avg:418.55ms
step:371/800 train_loss:4.3094 train_time:151095ms step_avg:418.55ms
step:372/800 train_loss:4.2280 train_time:151513ms step_avg:418.54ms
step:373/800 train_loss:4.1177 train_time:151929ms step_avg:418.54ms
step:374/800 train_loss:4.3184 train_time:152347ms step_avg:418.54ms
step:375/800 train_loss:4.2531 train_time:152765ms step_avg:418.53ms
step:375/800 val_loss:4.2582 train_time:152779ms step_avg:418.57ms
step:376/800 train_loss:4.2281 train_time:153188ms step_avg:418.55ms
step:377/800 train_loss:4.2935 train_time:153605ms step_avg:418.54ms
step:378/800 train_loss:4.1947 train_time:154125ms step_avg:418.82ms
step:379/800 train_loss:4.2559 train_time:154544ms step_avg:418.82ms
step:380/800 train_loss:4.3052 train_time:155094ms step_avg:419.17ms
step:381/800 train_loss:4.3543 train_time:155509ms step_avg:419.16ms
step:382/800 train_loss:4.2713 train_time:155926ms step_avg:419.16ms
step:383/800 train_loss:4.2515 train_time:156344ms step_avg:419.15ms
step:384/800 train_loss:4.1806 train_time:156764ms step_avg:419.15ms
step:385/800 train_loss:4.2726 train_time:157181ms step_avg:419.15ms
step:386/800 train_loss:4.1868 train_time:157601ms step_avg:419.15ms
step:387/800 train_loss:4.3115 train_time:158018ms step_avg:419.15ms
step:388/800 train_loss:4.5043 train_time:158436ms step_avg:419.14ms
step:389/800 train_loss:4.2024 train_time:158852ms step_avg:419.14ms
step:390/800 train_loss:4.1840 train_time:159269ms step_avg:419.13ms
step:391/800 train_loss:4.2940 train_time:159687ms step_avg:419.13ms
step:392/800 train_loss:4.2086 train_time:160104ms step_avg:419.12ms
step:393/800 train_loss:4.3175 train_time:160524ms step_avg:419.12ms
step:394/800 train_loss:4.1411 train_time:160942ms step_avg:419.12ms
step:395/800 train_loss:4.2850 train_time:161361ms step_avg:419.12ms
step:396/800 train_loss:4.0520 train_time:161778ms step_avg:419.11ms
step:397/800 train_loss:4.2304 train_time:162195ms step_avg:419.11ms
step:398/800 train_loss:4.3043 train_time:162611ms step_avg:419.10ms
step:399/800 train_loss:4.2766 train_time:163028ms step_avg:419.09ms
step:400/800 train_loss:4.1790 train_time:163445ms step_avg:419.09ms
step:401/800 train_loss:4.2386 train_time:163865ms step_avg:419.09ms
step:402/800 train_loss:4.2854 train_time:164284ms step_avg:419.09ms
step:403/800 train_loss:4.2429 train_time:164701ms step_avg:419.09ms
step:404/800 train_loss:4.3398 train_time:165118ms step_avg:419.08ms
step:405/800 train_loss:4.1093 train_time:165535ms step_avg:419.08ms
step:406/800 train_loss:4.1752 train_time:165953ms step_avg:419.07ms
step:407/800 train_loss:4.4584 train_time:166369ms step_avg:419.07ms
step:408/800 train_loss:4.2059 train_time:166787ms step_avg:419.06ms
step:409/800 train_loss:4.2045 train_time:167205ms step_avg:419.06ms
step:410/800 train_loss:4.2491 train_time:167623ms step_avg:419.06ms
step:411/800 train_loss:4.1294 train_time:168041ms step_avg:419.06ms
step:412/800 train_loss:4.1512 train_time:168459ms step_avg:419.05ms
step:413/800 train_loss:4.5660 train_time:168875ms step_avg:419.04ms
step:414/800 train_loss:4.0184 train_time:169292ms step_avg:419.04ms
step:415/800 train_loss:4.3917 train_time:169711ms step_avg:419.04ms
step:416/800 train_loss:4.1516 train_time:170128ms step_avg:419.03ms
step:417/800 train_loss:4.1518 train_time:170547ms step_avg:419.04ms
step:418/800 train_loss:4.3394 train_time:170967ms step_avg:419.04ms
step:419/800 train_loss:4.0685 train_time:171382ms step_avg:419.03ms
step:420/800 train_loss:4.1680 train_time:171799ms step_avg:419.02ms
step:421/800 train_loss:4.1221 train_time:172216ms step_avg:419.02ms
step:422/800 train_loss:4.0213 train_time:172634ms step_avg:419.01ms
step:423/800 train_loss:4.1492 train_time:173052ms step_avg:419.01ms
step:424/800 train_loss:4.2428 train_time:173470ms step_avg:419.01ms
step:425/800 train_loss:4.0202 train_time:173887ms step_avg:419.01ms
step:426/800 train_loss:4.1882 train_time:174305ms step_avg:419.00ms
step:427/800 train_loss:4.0733 train_time:174724ms step_avg:419.00ms
step:428/800 train_loss:4.2792 train_time:175141ms step_avg:419.00ms
step:429/800 train_loss:4.1983 train_time:175559ms step_avg:418.99ms
step:430/800 train_loss:4.1234 train_time:175976ms step_avg:418.99ms
step:431/800 train_loss:4.0988 train_time:176393ms step_avg:418.99ms
step:432/800 train_loss:4.0191 train_time:176811ms step_avg:418.98ms
step:433/800 train_loss:4.1270 train_time:177228ms step_avg:418.98ms
step:434/800 train_loss:4.1960 train_time:177645ms step_avg:418.97ms
step:435/800 train_loss:4.1344 train_time:178067ms step_avg:418.98ms
step:436/800 train_loss:4.1806 train_time:178483ms step_avg:418.97ms
step:437/800 train_loss:4.1917 train_time:178900ms step_avg:418.97ms
step:438/800 train_loss:4.0667 train_time:179318ms step_avg:418.97ms
step:439/800 train_loss:4.0921 train_time:179735ms step_avg:418.96ms
step:440/800 train_loss:4.0679 train_time:180152ms step_avg:418.96ms
step:441/800 train_loss:4.2466 train_time:180570ms step_avg:418.96ms
step:442/800 train_loss:4.1357 train_time:180989ms step_avg:418.96ms
step:443/800 train_loss:4.1196 train_time:181406ms step_avg:418.95ms
step:444/800 train_loss:4.0093 train_time:181823ms step_avg:418.95ms
step:445/800 train_loss:4.2696 train_time:182242ms step_avg:418.95ms
step:446/800 train_loss:4.2033 train_time:182657ms step_avg:418.94ms
step:447/800 train_loss:4.2024 train_time:183075ms step_avg:418.94ms
step:448/800 train_loss:4.1074 train_time:183492ms step_avg:418.93ms
step:449/800 train_loss:4.2072 train_time:183907ms step_avg:418.92ms
step:450/800 train_loss:4.0295 train_time:184324ms step_avg:418.92ms
step:451/800 train_loss:4.0678 train_time:184745ms step_avg:418.92ms
step:452/800 train_loss:3.9496 train_time:185164ms step_avg:418.92ms
step:453/800 train_loss:4.0586 train_time:185581ms step_avg:418.92ms
step:454/800 train_loss:4.0391 train_time:186000ms step_avg:418.92ms
step:455/800 train_loss:3.9927 train_time:186418ms step_avg:418.92ms
step:456/800 train_loss:4.2080 train_time:186835ms step_avg:418.91ms
step:457/800 train_loss:4.0705 train_time:187251ms step_avg:418.91ms
step:458/800 train_loss:4.1542 train_time:187667ms step_avg:418.90ms
step:459/800 train_loss:4.1836 train_time:188085ms step_avg:418.90ms
step:460/800 train_loss:3.9886 train_time:188502ms step_avg:418.89ms
step:461/800 train_loss:4.1599 train_time:188919ms step_avg:418.89ms
step:462/800 train_loss:4.0532 train_time:189337ms step_avg:418.89ms
step:463/800 train_loss:4.0583 train_time:189753ms step_avg:418.88ms
step:464/800 train_loss:4.1310 train_time:190171ms step_avg:418.88ms
step:465/800 train_loss:4.0728 train_time:190588ms step_avg:418.88ms
step:466/800 train_loss:4.0740 train_time:191005ms step_avg:418.87ms
step:467/800 train_loss:4.1835 train_time:191423ms step_avg:418.87ms
step:468/800 train_loss:4.1848 train_time:191841ms step_avg:418.87ms
step:469/800 train_loss:4.1545 train_time:192257ms step_avg:418.86ms
step:470/800 train_loss:4.0548 train_time:192674ms step_avg:418.86ms
step:471/800 train_loss:4.1315 train_time:193091ms step_avg:418.85ms
step:472/800 train_loss:4.1828 train_time:193506ms step_avg:418.84ms
step:473/800 train_loss:4.1112 train_time:193922ms step_avg:418.84ms
step:474/800 train_loss:4.0752 train_time:194338ms step_avg:418.83ms
step:475/800 train_loss:3.9331 train_time:194756ms step_avg:418.83ms
step:476/800 train_loss:4.3655 train_time:195174ms step_avg:418.83ms
step:477/800 train_loss:4.1243 train_time:195591ms step_avg:418.82ms
step:478/800 train_loss:3.9289 train_time:196010ms step_avg:418.82ms
step:479/800 train_loss:4.1497 train_time:196428ms step_avg:418.82ms
step:480/800 train_loss:4.1209 train_time:196845ms step_avg:418.82ms
step:481/800 train_loss:4.2628 train_time:197266ms step_avg:418.82ms
step:482/800 train_loss:4.0692 train_time:197684ms step_avg:418.82ms
step:483/800 train_loss:3.8760 train_time:198100ms step_avg:418.82ms
step:484/800 train_loss:4.1572 train_time:198517ms step_avg:418.81ms
step:485/800 train_loss:4.0072 train_time:198936ms step_avg:418.81ms
step:486/800 train_loss:4.0191 train_time:199353ms step_avg:418.81ms
step:487/800 train_loss:3.9549 train_time:199769ms step_avg:418.80ms
step:488/800 train_loss:4.0161 train_time:200187ms step_avg:418.80ms
step:489/800 train_loss:4.2127 train_time:200603ms step_avg:418.80ms
step:490/800 train_loss:4.0559 train_time:201021ms step_avg:418.79ms
step:491/800 train_loss:3.9527 train_time:201437ms step_avg:418.79ms
step:492/800 train_loss:3.9609 train_time:201855ms step_avg:418.79ms
step:493/800 train_loss:4.0752 train_time:202272ms step_avg:418.78ms
step:494/800 train_loss:3.9216 train_time:202688ms step_avg:418.78ms
step:495/800 train_loss:4.0606 train_time:203106ms step_avg:418.78ms
step:496/800 train_loss:3.9894 train_time:203524ms step_avg:418.77ms
step:497/800 train_loss:3.8893 train_time:203940ms step_avg:418.77ms
step:498/800 train_loss:4.0699 train_time:204357ms step_avg:418.76ms
step:499/800 train_loss:4.1542 train_time:205405ms step_avg:420.05ms
step:500/800 train_loss:4.1884 train_time:205820ms step_avg:420.04ms
step:500/800 val_loss:4.0510 train_time:205834ms step_avg:420.07ms
step:501/800 train_loss:4.0866 train_time:206243ms step_avg:420.05ms
step:502/800 train_loss:4.1380 train_time:206661ms step_avg:420.04ms
step:503/800 train_loss:4.0821 train_time:207076ms step_avg:420.03ms
step:504/800 train_loss:4.1147 train_time:207494ms step_avg:420.03ms
step:505/800 train_loss:4.0774 train_time:207910ms step_avg:420.02ms
step:506/800 train_loss:4.1669 train_time:208330ms step_avg:420.02ms
step:507/800 train_loss:3.9699 train_time:208747ms step_avg:420.01ms
step:508/800 train_loss:4.0989 train_time:209165ms step_avg:420.01ms
step:509/800 train_loss:4.1762 train_time:209582ms step_avg:420.00ms
step:510/800 train_loss:4.1129 train_time:209997ms step_avg:419.99ms
step:511/800 train_loss:3.9249 train_time:210415ms step_avg:419.99ms
step:512/800 train_loss:4.1257 train_time:210832ms step_avg:419.98ms
step:513/800 train_loss:4.0554 train_time:211250ms step_avg:419.98ms
step:514/800 train_loss:4.0196 train_time:211666ms step_avg:419.97ms
step:515/800 train_loss:4.0848 train_time:212084ms step_avg:419.97ms
step:516/800 train_loss:4.0864 train_time:212502ms step_avg:419.96ms
step:517/800 train_loss:4.4167 train_time:212918ms step_avg:419.96ms
step:518/800 train_loss:4.0041 train_time:213335ms step_avg:419.95ms
step:519/800 train_loss:4.1277 train_time:213754ms step_avg:419.95ms
step:520/800 train_loss:4.0498 train_time:214169ms step_avg:419.94ms
step:521/800 train_loss:4.0242 train_time:214587ms step_avg:419.93ms
step:522/800 train_loss:3.9684 train_time:215003ms step_avg:419.93ms
step:523/800 train_loss:3.9861 train_time:215421ms step_avg:419.92ms
step:524/800 train_loss:4.6160 train_time:215838ms step_avg:419.92ms
step:525/800 train_loss:4.0827 train_time:216257ms step_avg:419.92ms
step:526/800 train_loss:4.0314 train_time:216674ms step_avg:419.91ms
step:527/800 train_loss:4.0260 train_time:217091ms step_avg:419.91ms
step:528/800 train_loss:3.9841 train_time:217508ms step_avg:419.90ms
step:529/800 train_loss:3.9606 train_time:217928ms step_avg:419.90ms
step:530/800 train_loss:4.1685 train_time:218347ms step_avg:419.90ms
step:531/800 train_loss:3.9712 train_time:218764ms step_avg:419.89ms
step:532/800 train_loss:4.2549 train_time:219182ms step_avg:419.89ms
step:533/800 train_loss:4.0596 train_time:219601ms step_avg:419.89ms
step:534/800 train_loss:3.9909 train_time:220018ms step_avg:419.88ms
step:535/800 train_loss:4.0116 train_time:220435ms step_avg:419.88ms
step:536/800 train_loss:3.9454 train_time:220852ms step_avg:419.87ms
step:537/800 train_loss:4.0664 train_time:221268ms step_avg:419.86ms
step:538/800 train_loss:4.0564 train_time:221686ms step_avg:419.86ms
step:539/800 train_loss:3.9639 train_time:222102ms step_avg:419.85ms
step:540/800 train_loss:4.4550 train_time:222518ms step_avg:419.85ms
step:541/800 train_loss:3.9969 train_time:222940ms step_avg:419.85ms
step:542/800 train_loss:4.1135 train_time:223357ms step_avg:419.84ms
step:543/800 train_loss:3.9407 train_time:223774ms step_avg:419.84ms
step:544/800 train_loss:3.9251 train_time:224192ms step_avg:419.83ms
step:545/800 train_loss:4.0095 train_time:224609ms step_avg:419.83ms
step:546/800 train_loss:3.9315 train_time:225030ms step_avg:419.83ms
step:547/800 train_loss:3.9791 train_time:225448ms step_avg:419.83ms
step:548/800 train_loss:3.9888 train_time:225865ms step_avg:419.82ms
step:549/800 train_loss:3.9567 train_time:226283ms step_avg:419.82ms
step:550/800 train_loss:4.0517 train_time:226699ms step_avg:419.81ms
step:551/800 train_loss:3.9298 train_time:227116ms step_avg:419.81ms
step:552/800 train_loss:3.9536 train_time:227535ms step_avg:419.81ms
step:553/800 train_loss:4.2780 train_time:227954ms step_avg:419.80ms
step:554/800 train_loss:4.0707 train_time:228372ms step_avg:419.80ms
step:555/800 train_loss:4.0365 train_time:228791ms step_avg:419.80ms
step:556/800 train_loss:3.9986 train_time:229208ms step_avg:419.80ms
step:557/800 train_loss:4.0145 train_time:229629ms step_avg:419.80ms
step:558/800 train_loss:3.6801 train_time:230047ms step_avg:419.79ms
step:559/800 train_loss:3.9346 train_time:230463ms step_avg:419.79ms
step:560/800 train_loss:3.9752 train_time:230882ms step_avg:419.79ms
step:561/800 train_loss:4.0165 train_time:231297ms step_avg:419.78ms
step:562/800 train_loss:3.9330 train_time:231713ms step_avg:419.77ms
step:563/800 train_loss:3.8757 train_time:232132ms step_avg:419.77ms
step:564/800 train_loss:4.0803 train_time:232552ms step_avg:419.77ms
step:565/800 train_loss:3.8918 train_time:232968ms step_avg:419.76ms
step:566/800 train_loss:4.0184 train_time:233384ms step_avg:419.76ms
step:567/800 train_loss:3.9651 train_time:233909ms step_avg:419.94ms
step:568/800 train_loss:3.9132 train_time:234330ms step_avg:419.95ms
step:569/800 train_loss:4.0089 train_time:234747ms step_avg:419.94ms
step:570/800 train_loss:3.9767 train_time:235294ms step_avg:420.17ms
step:571/800 train_loss:4.0043 train_time:235709ms step_avg:420.16ms
step:572/800 train_loss:4.1005 train_time:236131ms step_avg:420.16ms
step:573/800 train_loss:4.0285 train_time:236547ms step_avg:420.15ms
step:574/800 train_loss:4.0346 train_time:236963ms step_avg:420.15ms
step:575/800 train_loss:4.0931 train_time:237380ms step_avg:420.14ms
step:576/800 train_loss:4.0555 train_time:237797ms step_avg:420.14ms
step:577/800 train_loss:4.0637 train_time:238214ms step_avg:420.13ms
step:578/800 train_loss:4.0120 train_time:238631ms step_avg:420.12ms
step:579/800 train_loss:3.9825 train_time:239048ms step_avg:420.12ms
step:580/800 train_loss:3.9724 train_time:239465ms step_avg:420.11ms
step:581/800 train_loss:3.9254 train_time:239882ms step_avg:420.11ms
step:582/800 train_loss:3.9524 train_time:240302ms step_avg:420.11ms
step:583/800 train_loss:4.1784 train_time:240719ms step_avg:420.10ms
step:584/800 train_loss:3.9450 train_time:241135ms step_avg:420.10ms
step:585/800 train_loss:3.9084 train_time:241562ms step_avg:420.11ms
step:586/800 train_loss:4.0919 train_time:241980ms step_avg:420.10ms
step:587/800 train_loss:3.8456 train_time:242398ms step_avg:420.10ms
step:588/800 train_loss:3.9812 train_time:242815ms step_avg:420.10ms
step:589/800 train_loss:3.9839 train_time:243233ms step_avg:420.09ms
step:590/800 train_loss:4.3248 train_time:243650ms step_avg:420.09ms
step:591/800 train_loss:4.0983 train_time:244068ms step_avg:420.08ms
step:592/800 train_loss:3.8431 train_time:244485ms step_avg:420.08ms
step:593/800 train_loss:3.8523 train_time:244901ms step_avg:420.07ms
step:594/800 train_loss:3.8561 train_time:245318ms step_avg:420.07ms
step:595/800 train_loss:3.8849 train_time:245737ms step_avg:420.06ms
step:596/800 train_loss:4.2593 train_time:246154ms step_avg:420.06ms
step:597/800 train_loss:3.9712 train_time:246571ms step_avg:420.05ms
step:598/800 train_loss:3.9075 train_time:246987ms step_avg:420.05ms
step:599/800 train_loss:3.9776 train_time:247404ms step_avg:420.04ms
step:600/800 train_loss:3.7945 train_time:247821ms step_avg:420.04ms
step:601/800 train_loss:3.9191 train_time:248239ms step_avg:420.03ms
step:602/800 train_loss:3.9490 train_time:248655ms step_avg:420.02ms
step:603/800 train_loss:3.9619 train_time:249073ms step_avg:420.02ms
step:604/800 train_loss:4.0986 train_time:249490ms step_avg:420.02ms
step:605/800 train_loss:3.9640 train_time:249907ms step_avg:420.01ms
step:606/800 train_loss:3.9354 train_time:250324ms step_avg:420.01ms
step:607/800 train_loss:3.8693 train_time:250740ms step_avg:420.00ms
step:608/800 train_loss:4.1137 train_time:251156ms step_avg:419.99ms
step:609/800 train_loss:3.9580 train_time:251574ms step_avg:419.99ms
step:610/800 train_loss:3.9346 train_time:251990ms step_avg:419.98ms
step:611/800 train_loss:4.0396 train_time:252408ms step_avg:419.98ms
step:612/800 train_loss:3.9438 train_time:252828ms step_avg:419.98ms
step:613/800 train_loss:3.9149 train_time:253243ms step_avg:419.97ms
step:614/800 train_loss:4.0837 train_time:253661ms step_avg:419.97ms
step:615/800 train_loss:4.0428 train_time:254078ms step_avg:419.96ms
step:616/800 train_loss:4.0128 train_time:254494ms step_avg:419.96ms
step:617/800 train_loss:3.9290 train_time:254912ms step_avg:419.95ms
step:618/800 train_loss:3.8866 train_time:255331ms step_avg:419.95ms
step:619/800 train_loss:3.9913 train_time:255748ms step_avg:419.95ms
step:620/800 train_loss:3.8888 train_time:256165ms step_avg:419.94ms
step:621/800 train_loss:3.9079 train_time:256582ms step_avg:419.94ms
step:622/800 train_loss:4.2033 train_time:256999ms step_avg:419.93ms
step:623/800 train_loss:3.9104 train_time:257418ms step_avg:419.93ms
step:624/800 train_loss:3.9426 train_time:257834ms step_avg:419.93ms
step:625/800 train_loss:4.0198 train_time:258252ms step_avg:419.92ms
step:625/800 val_loss:3.9437 train_time:258265ms step_avg:419.94ms
step:626/800 train_loss:4.0427 train_time:258674ms step_avg:419.92ms
step:627/800 train_loss:4.0622 train_time:259091ms step_avg:419.92ms
step:628/800 train_loss:4.0399 train_time:259508ms step_avg:419.92ms
step:629/800 train_loss:4.0895 train_time:259929ms step_avg:419.92ms
step:630/800 train_loss:3.9018 train_time:260345ms step_avg:419.91ms
step:631/800 train_loss:4.0384 train_time:260761ms step_avg:419.91ms
step:632/800 train_loss:4.0713 train_time:261179ms step_avg:419.90ms
step:633/800 train_loss:3.9708 train_time:261599ms step_avg:419.90ms
step:634/800 train_loss:3.8974 train_time:262016ms step_avg:419.90ms
step:635/800 train_loss:3.9955 train_time:262433ms step_avg:419.89ms
step:636/800 train_loss:4.2509 train_time:262850ms step_avg:419.89ms
step:637/800 train_loss:3.8474 train_time:263267ms step_avg:419.88ms
step:638/800 train_loss:3.6680 train_time:263684ms step_avg:419.88ms
step:639/800 train_loss:3.8957 train_time:264101ms step_avg:419.88ms
step:640/800 train_loss:3.9287 train_time:264518ms step_avg:419.87ms
step:641/800 train_loss:3.8968 train_time:264934ms step_avg:419.86ms
step:642/800 train_loss:3.8960 train_time:265352ms step_avg:419.86ms
step:643/800 train_loss:3.9352 train_time:265771ms step_avg:419.86ms
step:644/800 train_loss:3.9569 train_time:266188ms step_avg:419.85ms
step:645/800 train_loss:3.8769 train_time:266604ms step_avg:419.85ms
step:646/800 train_loss:4.0933 train_time:267020ms step_avg:419.84ms
step:647/800 train_loss:3.9799 train_time:267437ms step_avg:419.84ms
step:648/800 train_loss:3.9870 train_time:267853ms step_avg:419.83ms
step:649/800 train_loss:4.0024 train_time:268271ms step_avg:419.83ms
step:650/800 train_loss:4.0693 train_time:268687ms step_avg:419.82ms
step:651/800 train_loss:3.9350 train_time:269104ms step_avg:419.82ms
step:652/800 train_loss:4.0762 train_time:269522ms step_avg:419.82ms
step:653/800 train_loss:3.8992 train_time:269940ms step_avg:419.81ms
step:654/800 train_loss:3.9770 train_time:270357ms step_avg:419.81ms
step:655/800 train_loss:3.7409 train_time:270775ms step_avg:419.81ms
step:656/800 train_loss:3.8900 train_time:271195ms step_avg:419.81ms
step:657/800 train_loss:3.8961 train_time:271613ms step_avg:419.80ms
step:658/800 train_loss:3.8327 train_time:272030ms step_avg:419.80ms
step:659/800 train_loss:4.0100 train_time:272447ms step_avg:419.79ms
step:660/800 train_loss:3.9060 train_time:272864ms step_avg:419.79ms
step:661/800 train_loss:3.9901 train_time:273280ms step_avg:419.79ms
step:662/800 train_loss:4.0650 train_time:273700ms step_avg:419.79ms
step:663/800 train_loss:3.9754 train_time:274117ms step_avg:419.78ms
step:664/800 train_loss:3.8607 train_time:274533ms step_avg:419.78ms
step:665/800 train_loss:3.9389 train_time:274950ms step_avg:419.77ms
step:666/800 train_loss:3.8082 train_time:275367ms step_avg:419.77ms
step:667/800 train_loss:4.1052 train_time:275784ms step_avg:419.76ms
step:668/800 train_loss:3.9352 train_time:276201ms step_avg:419.76ms
step:669/800 train_loss:3.9322 train_time:276618ms step_avg:419.75ms
step:670/800 train_loss:3.7944 train_time:277036ms step_avg:419.75ms
step:671/800 train_loss:3.9051 train_time:277453ms step_avg:419.75ms
step:672/800 train_loss:3.8617 train_time:277869ms step_avg:419.74ms
step:673/800 train_loss:3.8885 train_time:278285ms step_avg:419.74ms
step:674/800 train_loss:4.1752 train_time:278714ms step_avg:419.75ms
step:675/800 train_loss:3.9567 train_time:279131ms step_avg:419.75ms
step:676/800 train_loss:4.0285 train_time:279548ms step_avg:419.74ms
step:677/800 train_loss:3.7997 train_time:279966ms step_avg:419.74ms
step:678/800 train_loss:3.9043 train_time:280381ms step_avg:419.73ms
step:679/800 train_loss:3.8508 train_time:280800ms step_avg:419.73ms
step:680/800 train_loss:3.9927 train_time:281218ms step_avg:419.73ms
step:681/800 train_loss:3.8987 train_time:281635ms step_avg:419.72ms
step:682/800 train_loss:3.9272 train_time:282053ms step_avg:419.72ms
step:683/800 train_loss:3.9985 train_time:282468ms step_avg:419.71ms
step:684/800 train_loss:4.0505 train_time:282885ms step_avg:419.71ms
step:685/800 train_loss:3.9438 train_time:283304ms step_avg:419.71ms
step:686/800 train_loss:4.0171 train_time:283720ms step_avg:419.70ms
step:687/800 train_loss:3.9436 train_time:284139ms step_avg:419.70ms
step:688/800 train_loss:3.9948 train_time:284558ms step_avg:419.70ms
step:689/800 train_loss:3.6288 train_time:284976ms step_avg:419.70ms
step:690/800 train_loss:3.7348 train_time:285396ms step_avg:419.70ms
step:691/800 train_loss:3.8623 train_time:285813ms step_avg:419.70ms
step:692/800 train_loss:3.7493 train_time:286230ms step_avg:419.69ms
step:693/800 train_loss:3.9670 train_time:286647ms step_avg:419.69ms
step:694/800 train_loss:3.9821 train_time:287065ms step_avg:419.68ms
step:695/800 train_loss:3.8744 train_time:287482ms step_avg:419.68ms
step:696/800 train_loss:3.8546 train_time:287901ms step_avg:419.68ms
step:697/800 train_loss:4.1537 train_time:288318ms step_avg:419.68ms
step:698/800 train_loss:3.9199 train_time:288735ms step_avg:419.67ms
step:699/800 train_loss:3.9502 train_time:289153ms step_avg:419.67ms
step:700/800 train_loss:4.1214 train_time:289569ms step_avg:419.66ms
step:701/800 train_loss:3.8899 train_time:290007ms step_avg:419.69ms
step:702/800 train_loss:3.8407 train_time:290424ms step_avg:419.69ms
step:703/800 train_loss:3.8343 train_time:290841ms step_avg:419.68ms
step:704/800 train_loss:3.7850 train_time:291260ms step_avg:419.68ms
step:705/800 train_loss:3.8793 train_time:291685ms step_avg:419.69ms
step:706/800 train_loss:3.8684 train_time:292102ms step_avg:419.69ms
step:707/800 train_loss:3.8914 train_time:292520ms step_avg:419.68ms
step:708/800 train_loss:3.9580 train_time:292935ms step_avg:419.68ms
step:709/800 train_loss:3.9007 train_time:293353ms step_avg:419.67ms
step:710/800 train_loss:3.8843 train_time:293770ms step_avg:419.67ms
step:711/800 train_loss:3.8560 train_time:294185ms step_avg:419.67ms
step:712/800 train_loss:3.8992 train_time:294603ms step_avg:419.66ms
step:713/800 train_loss:3.9574 train_time:295019ms step_avg:419.66ms
step:714/800 train_loss:3.9649 train_time:295436ms step_avg:419.65ms
step:715/800 train_loss:3.8791 train_time:295854ms step_avg:419.65ms
step:716/800 train_loss:3.8848 train_time:296272ms step_avg:419.65ms
step:717/800 train_loss:3.9049 train_time:296689ms step_avg:419.64ms
step:718/800 train_loss:4.0391 train_time:297106ms step_avg:419.64ms
step:719/800 train_loss:3.9060 train_time:297524ms step_avg:419.64ms
step:720/800 train_loss:3.9730 train_time:297941ms step_avg:419.63ms
step:721/800 train_loss:4.1525 train_time:298357ms step_avg:419.63ms
step:722/800 train_loss:3.7763 train_time:298773ms step_avg:419.63ms
step:723/800 train_loss:4.0298 train_time:299190ms step_avg:419.62ms
step:724/800 train_loss:4.0912 train_time:299606ms step_avg:419.62ms
step:725/800 train_loss:3.8638 train_time:300024ms step_avg:419.61ms
step:726/800 train_loss:3.9557 train_time:300442ms step_avg:419.61ms
step:727/800 train_loss:3.8612 train_time:300859ms step_avg:419.61ms
step:728/800 train_loss:3.8630 train_time:301275ms step_avg:419.60ms
step:729/800 train_loss:4.0352 train_time:301692ms step_avg:419.60ms
step:730/800 train_loss:3.9940 train_time:302109ms step_avg:419.60ms
step:731/800 train_loss:3.9951 train_time:302527ms step_avg:419.59ms
step:732/800 train_loss:3.8772 train_time:302944ms step_avg:419.59ms
step:733/800 train_loss:3.9005 train_time:303360ms step_avg:419.59ms
step:734/800 train_loss:4.1359 train_time:303777ms step_avg:419.58ms
step:735/800 train_loss:3.8581 train_time:304197ms step_avg:419.58ms
step:736/800 train_loss:3.9354 train_time:304614ms step_avg:419.58ms
step:737/800 train_loss:4.0616 train_time:305029ms step_avg:419.57ms
step:738/800 train_loss:3.9650 train_time:305445ms step_avg:419.57ms
step:739/800 train_loss:3.9107 train_time:305863ms step_avg:419.56ms
step:740/800 train_loss:3.8128 train_time:306280ms step_avg:419.56ms
step:741/800 train_loss:4.4654 train_time:306698ms step_avg:419.56ms
step:742/800 train_loss:3.8212 train_time:307113ms step_avg:419.55ms
step:743/800 train_loss:3.8950 train_time:307530ms step_avg:419.55ms
step:744/800 train_loss:3.8928 train_time:307948ms step_avg:419.55ms
step:745/800 train_loss:3.9527 train_time:308366ms step_avg:419.55ms
step:746/800 train_loss:3.9348 train_time:308783ms step_avg:419.54ms
step:747/800 train_loss:3.9089 train_time:309201ms step_avg:419.54ms
step:748/800 train_loss:3.9483 train_time:309619ms step_avg:419.54ms
step:749/800 train_loss:3.8651 train_time:310035ms step_avg:419.53ms
step:750/800 train_loss:3.8790 train_time:310452ms step_avg:419.53ms
step:750/800 val_loss:3.8850 train_time:310465ms step_avg:419.55ms
step:751/800 train_loss:3.9236 train_time:310873ms step_avg:419.53ms
step:752/800 train_loss:3.8726 train_time:311290ms step_avg:419.53ms
step:753/800 train_loss:3.9098 train_time:311707ms step_avg:419.53ms
step:754/800 train_loss:3.9308 train_time:312126ms step_avg:419.52ms
step:755/800 train_loss:3.8996 train_time:312542ms step_avg:419.52ms
step:756/800 train_loss:3.9872 train_time:313832ms step_avg:420.69ms
step:757/800 train_loss:3.8114 train_time:314258ms step_avg:420.69ms
step:758/800 train_loss:4.0375 train_time:314679ms step_avg:420.69ms
step:759/800 train_loss:3.9526 train_time:315102ms step_avg:420.70ms
step:760/800 train_loss:3.8827 train_time:315697ms step_avg:420.93ms
step:761/800 train_loss:3.9900 train_time:316122ms step_avg:420.93ms
step:762/800 train_loss:3.7099 train_time:316544ms step_avg:420.94ms
step:763/800 train_loss:3.8842 train_time:316968ms step_avg:420.94ms
step:764/800 train_loss:3.9811 train_time:317390ms step_avg:420.94ms
step:765/800 train_loss:3.6216 train_time:317813ms step_avg:420.94ms
step:766/800 train_loss:4.0687 train_time:318236ms step_avg:420.95ms
step:767/800 train_loss:3.9126 train_time:318659ms step_avg:420.95ms
step:768/800 train_loss:3.8683 train_time:319080ms step_avg:420.95ms
step:769/800 train_loss:3.8947 train_time:319501ms step_avg:420.95ms
step:770/800 train_loss:3.9131 train_time:319922ms step_avg:420.95ms
step:771/800 train_loss:3.9753 train_time:320344ms step_avg:420.95ms
step:772/800 train_loss:4.1952 train_time:320767ms step_avg:420.95ms
step:773/800 train_loss:3.7667 train_time:321190ms step_avg:420.96ms
step:774/800 train_loss:3.9781 train_time:321612ms step_avg:420.96ms
step:775/800 train_loss:3.9553 train_time:322033ms step_avg:420.96ms
step:776/800 train_loss:3.9170 train_time:322456ms step_avg:420.96ms
step:777/800 train_loss:3.7274 train_time:322878ms step_avg:420.96ms
step:778/800 train_loss:3.7241 train_time:323300ms step_avg:420.96ms
step:779/800 train_loss:3.7888 train_time:323722ms step_avg:420.96ms
step:780/800 train_loss:3.8806 train_time:324145ms step_avg:420.97ms
step:781/800 train_loss:3.9150 train_time:324570ms step_avg:420.97ms
step:782/800 train_loss:3.9792 train_time:324992ms step_avg:420.97ms
step:783/800 train_loss:3.8762 train_time:325415ms step_avg:420.98ms
step:784/800 train_loss:3.9034 train_time:325837ms step_avg:420.98ms
step:785/800 train_loss:3.8856 train_time:326261ms step_avg:420.98ms
step:786/800 train_loss:3.8747 train_time:326684ms step_avg:420.98ms
step:787/800 train_loss:3.7748 train_time:327108ms step_avg:420.99ms
step:788/800 train_loss:4.0284 train_time:327532ms step_avg:420.99ms
step:789/800 train_loss:3.8259 train_time:327956ms step_avg:421.00ms
step:790/800 train_loss:3.8868 train_time:328378ms step_avg:421.00ms
step:791/800 train_loss:3.9523 train_time:328799ms step_avg:421.00ms
step:792/800 train_loss:4.0806 train_time:329220ms step_avg:421.00ms
step:793/800 train_loss:4.0883 train_time:329643ms step_avg:421.00ms
step:794/800 train_loss:3.8191 train_time:330066ms step_avg:421.00ms
step:795/800 train_loss:3.9244 train_time:330488ms step_avg:421.00ms
step:796/800 train_loss:3.9710 train_time:330909ms step_avg:421.00ms
step:797/800 train_loss:4.0677 train_time:331329ms step_avg:421.00ms
step:798/800 train_loss:3.8359 train_time:331751ms step_avg:421.00ms
step:799/800 train_loss:3.9840 train_time:332174ms step_avg:421.01ms
step:800/800 train_loss:3.8876 train_time:332597ms step_avg:421.01ms
step:800/800 val_loss:3.8762 train_time:332611ms step_avg:421.03ms
