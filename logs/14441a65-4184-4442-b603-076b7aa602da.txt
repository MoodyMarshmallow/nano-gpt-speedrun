====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import json
import dataclasses
import subprocess
import csv
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

def _apply_gate_act(logits: torch.Tensor, kind: str) -> torch.Tensor:
    if kind == "sigmoid":
        return torch.sigmoid(logits)
    if kind == "ns_sigmoid":
        return 0.5 + 0.5 * torch.sigmoid(logits)
    raise ValueError(f"unknown gate_act={kind}")

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.attn_gate = getattr(config, "attn_gate", "none")
        self.gate_pos = getattr(config, "gate_pos", "sdpa")
        self.gate_act = getattr(config, "gate_act", "sigmoid")
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.attn_gate == "headwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_head, bias=False)
            self.gate_param = None
        elif self.attn_gate == "elementwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_embd, bias=False)
            self.gate_param = None
        elif self.attn_gate == "const":
            self.c_gate = None
            self.gate_param = nn.Parameter(torch.zeros(self.n_head, self.head_dim))
        else:
            self.c_gate = None
            self.gate_param = None

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if self.attn_gate != "none" and self.gate_pos == "value":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            v = v * gate
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2) # (B, T, n_head, head_dim)
        if self.attn_gate != "none" and self.gate_pos == "sdpa":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            y = y * gate
        y = y.contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attn_gate : str = "none"
    gate_pos : str = "sdpa"
    gate_act : str = "sigmoid"

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    seed : int = 1337
    attn_gate : str = "none" # none|headwise|elementwise|const
    gate_pos : str = "sdpa" # sdpa|value
    gate_act : str = "sigmoid" # sigmoid|ns_sigmoid
    early_stop_patience : int = 0 # 0 disables early stopping
    early_stop_min_delta : float = 0.0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

def apply_env_overrides():
    # environment-variable overrides allow quick sweeps without editing code
    args.learning_rate = float(os.environ.get("LR", args.learning_rate))
    args.seed = int(os.environ.get("SEED", args.seed))
    args.warmup_iters = int(os.environ.get("WARMUP_ITERS", args.warmup_iters))
    args.num_iterations = int(os.environ.get("NUM_ITERATIONS", args.num_iterations))
    args.early_stop_patience = int(os.environ.get("EARLY_STOP_PATIENCE", args.early_stop_patience))
    args.early_stop_min_delta = float(os.environ.get("EARLY_STOP_MIN_DELTA", args.early_stop_min_delta))
    args.attn_gate = os.environ.get("ATTNGATE", args.attn_gate)
    args.gate_pos = os.environ.get("GATEPOS", args.gate_pos)
    args.gate_act = os.environ.get("GATEACT", args.gate_act)
    args.num_iterations = int(os.environ.get("NUM_ITER", args.num_iterations))
    args.val_loss_every = int(os.environ.get("VAL_EVERY", args.val_loss_every))

def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

apply_env_overrides()
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
np.random.seed(args.seed)

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
git_commit = get_git_commit() if master_process else "unknown"

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    attn_gate=args.attn_gate,
    gate_pos=args.gate_pos,
    gate_act=args.gate_act,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        f.write(f"git_commit: {git_commit}\n")
        f.write(f"seed: {args.seed}\n")
        f.write("hyperparameters:\n")
        f.write(json.dumps(dataclasses.asdict(args), indent=2))
        f.write("\n")
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
best_val_loss = float("inf")
final_val_loss = None
no_improve_count = 0
early_stop_reason = None
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        if not torch.isfinite(val_loss):
            early_stop_reason = "non-finite val_loss"
            final_val_loss = float("nan")
            val_loss_item = float("nan")
        else:
            val_loss_item = val_loss.item()
            final_val_loss = val_loss_item
            if val_loss_item < best_val_loss - args.early_stop_min_delta:
                best_val_loss = val_loss_item
                no_improve_count = 0
            else:
                no_improve_count += 1
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if early_stop_reason is None and args.early_stop_patience > 0 and no_improve_count >= args.early_stop_patience:
                early_stop_reason = f"early_stop patience={args.early_stop_patience} min_delta={args.early_stop_min_delta}"
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process in a subdir to keep logs tidy
        ckpt_dir = os.path.join(logdir, "checkpoints")
        os.makedirs(ckpt_dir, exist_ok=True)
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, os.path.join(ckpt_dir, f"state_step{step:06d}.pt"))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")
    if early_stop_reason is not None:
        break

if master_process:
    if early_stop_reason is not None:
        print(f"stopped early: {early_stop_reason}")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    timed_steps_final = max(args.num_iterations - 9, 1)
    ms_per_step = training_time_ms / timed_steps_final
    os.makedirs("experiments", exist_ok=True)
    results_path = os.path.join("experiments", "results.csv")
    fieldnames = [
        "run_id",
        "date",
        "git_commit",
        "seed",
        "attn_gate",
        "gate_pos",
        "gate_act",
        "learning_rate",
        "batch_size",
        "device_batch_size",
        "sequence_length",
        "num_iterations",
        "warmdown_iters",
        "final_val_loss",
        "best_val_loss",
        "train_time_ms",
        "ms_per_step",
        "gpu_name",
        "n_gpus",
        "runpod_instance",
        "notes",
    ]
    final_loss_value = final_val_loss if final_val_loss is not None else float("nan")
    best_loss_value = best_val_loss if best_val_loss < float("inf") else float("nan")
    row = {
        "run_id": run_id,
        "date": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        "git_commit": git_commit,
        "seed": args.seed,
        "attn_gate": args.attn_gate,
        "gate_pos": args.gate_pos,
        "gate_act": args.gate_act,
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "device_batch_size": args.device_batch_size,
        "sequence_length": args.sequence_length,
        "num_iterations": args.num_iterations,
        "warmdown_iters": args.warmdown_iters,
        "final_val_loss": final_loss_value,
        "best_val_loss": best_loss_value,
        "train_time_ms": training_time_ms,
        "ms_per_step": ms_per_step,
        "gpu_name": torch.cuda.get_device_name(ddp_local_rank),
        "n_gpus": ddp_world_size,
        "runpod_instance": os.environ.get("RUNPOD_INSTANCE_TYPE", "unknown"),
        "notes": "",
    }
    write_header = not os.path.exists(results_path)
    with open(results_path, "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerow(row)
====================================================================================================
git_commit: bf6d1ff4b5e4ce7280e822ab33e90c4fb250e604
seed: 2337
hyperparameters:
{
  "input_bin": "data/fineweb10B/fineweb_train_*.bin",
  "input_val_bin": "data/fineweb10B/fineweb_val_*.bin",
  "batch_size": 512,
  "device_batch_size": 64,
  "sequence_length": 1024,
  "num_iterations": 800,
  "learning_rate": 0.00396,
  "warmup_iters": 150,
  "warmdown_iters": 1450,
  "weight_decay": 0,
  "seed": 2337,
  "attn_gate": "elementwise",
  "gate_pos": "sdpa",
  "gate_act": "sigmoid",
  "early_stop_patience": 2,
  "early_stop_min_delta": 0.0,
  "val_loss_every": 125,
  "val_tokens": 10485760,
  "save_every": 0
}
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Sun Dec  7 18:03:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:00:07.0 Off |                    0 |
| N/A   46C    P0            126W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:00:08.0 Off |                    0 |
| N/A   48C    P0            121W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0            118W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000000:00:0A.0 Off |                    0 |
| N/A   45C    P0            145W /  300W |    2276MiB /  81920MiB |     20%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  |   00000000:00:0B.0 Off |                    0 |
| N/A   47C    P0            114W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  |   00000000:00:0C.0 Off |                    0 |
| N/A   45C    P0            109W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  |   00000000:00:0D.0 Off |                    0 |
| N/A   47C    P0            118W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  |   00000000:00:0E.0 Off |                    0 |
| N/A   47C    P0            129W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/800 val_loss:16.0092 train_time:234ms step_avg:nanms
step:1/800 train_loss:16.0082 train_time:45379ms step_avg:nanms
step:2/800 train_loss:15.8833 train_time:46002ms step_avg:nanms
step:3/800 train_loss:15.5583 train_time:46419ms step_avg:nanms
step:4/800 train_loss:14.8993 train_time:46834ms step_avg:nanms
step:5/800 train_loss:13.6300 train_time:47250ms step_avg:nanms
step:6/800 train_loss:11.9568 train_time:47663ms step_avg:nanms
step:7/800 train_loss:10.4335 train_time:48079ms step_avg:nanms
step:8/800 train_loss:9.8003 train_time:48495ms step_avg:nanms
step:9/800 train_loss:9.5792 train_time:48910ms step_avg:nanms
step:10/800 train_loss:9.4118 train_time:49326ms step_avg:nanms
step:11/800 train_loss:9.2238 train_time:402ms step_avg:nanms
step:12/800 train_loss:9.0564 train_time:818ms step_avg:nanms
step:13/800 train_loss:8.7924 train_time:1234ms step_avg:411.28ms
step:14/800 train_loss:8.6512 train_time:1648ms step_avg:412.08ms
step:15/800 train_loss:8.4679 train_time:2063ms step_avg:412.69ms
step:16/800 train_loss:8.2665 train_time:2482ms step_avg:413.74ms
step:17/800 train_loss:8.1034 train_time:2896ms step_avg:413.73ms
step:18/800 train_loss:7.9734 train_time:3310ms step_avg:413.80ms
step:19/800 train_loss:7.7295 train_time:3725ms step_avg:413.87ms
step:20/800 train_loss:7.6281 train_time:4138ms step_avg:413.82ms
step:21/800 train_loss:7.2511 train_time:4552ms step_avg:413.78ms
step:22/800 train_loss:7.5025 train_time:4966ms step_avg:413.85ms
step:23/800 train_loss:7.6466 train_time:5383ms step_avg:414.09ms
step:24/800 train_loss:7.3023 train_time:5798ms step_avg:414.12ms
step:25/800 train_loss:7.3748 train_time:6213ms step_avg:414.19ms
step:26/800 train_loss:7.1362 train_time:6632ms step_avg:414.52ms
step:27/800 train_loss:7.0284 train_time:7046ms step_avg:414.47ms
step:28/800 train_loss:7.1762 train_time:7462ms step_avg:414.58ms
step:29/800 train_loss:6.8396 train_time:7881ms step_avg:414.81ms
step:30/800 train_loss:7.0620 train_time:8298ms step_avg:414.91ms
step:31/800 train_loss:6.9163 train_time:8715ms step_avg:414.98ms
step:32/800 train_loss:6.8345 train_time:9130ms step_avg:415.01ms
step:33/800 train_loss:6.6136 train_time:9545ms step_avg:415.02ms
step:34/800 train_loss:7.0347 train_time:9960ms step_avg:415.01ms
step:35/800 train_loss:6.8029 train_time:10376ms step_avg:415.04ms
step:36/800 train_loss:6.9956 train_time:10794ms step_avg:415.14ms
step:37/800 train_loss:6.8752 train_time:11210ms step_avg:415.20ms
step:38/800 train_loss:6.7453 train_time:11627ms step_avg:415.24ms
step:39/800 train_loss:6.6021 train_time:12044ms step_avg:415.31ms
step:40/800 train_loss:6.6992 train_time:12461ms step_avg:415.36ms
step:41/800 train_loss:6.5655 train_time:12877ms step_avg:415.37ms
step:42/800 train_loss:6.6015 train_time:13293ms step_avg:415.40ms
step:43/800 train_loss:6.4268 train_time:13709ms step_avg:415.44ms
step:44/800 train_loss:6.5222 train_time:14127ms step_avg:415.49ms
step:45/800 train_loss:6.5021 train_time:14543ms step_avg:415.53ms
step:46/800 train_loss:6.7111 train_time:14959ms step_avg:415.52ms
step:47/800 train_loss:6.5081 train_time:15375ms step_avg:415.55ms
step:48/800 train_loss:6.3421 train_time:15793ms step_avg:415.61ms
step:49/800 train_loss:6.5871 train_time:16208ms step_avg:415.59ms
step:50/800 train_loss:6.4421 train_time:16625ms step_avg:415.63ms
step:51/800 train_loss:6.5946 train_time:17040ms step_avg:415.62ms
step:52/800 train_loss:6.4422 train_time:17456ms step_avg:415.62ms
step:53/800 train_loss:6.2731 train_time:17871ms step_avg:415.62ms
step:54/800 train_loss:6.4039 train_time:18287ms step_avg:415.62ms
step:55/800 train_loss:6.3314 train_time:18704ms step_avg:415.64ms
step:56/800 train_loss:6.6403 train_time:19120ms step_avg:415.66ms
step:57/800 train_loss:6.3142 train_time:19538ms step_avg:415.70ms
step:58/800 train_loss:6.1883 train_time:19956ms step_avg:415.76ms
step:59/800 train_loss:6.3706 train_time:20373ms step_avg:415.78ms
step:60/800 train_loss:6.2775 train_time:20790ms step_avg:415.80ms
step:61/800 train_loss:6.3893 train_time:21208ms step_avg:415.85ms
step:62/800 train_loss:6.1904 train_time:21625ms step_avg:415.86ms
step:63/800 train_loss:6.2712 train_time:22041ms step_avg:415.87ms
step:64/800 train_loss:6.2249 train_time:22461ms step_avg:415.94ms
step:65/800 train_loss:6.6801 train_time:22876ms step_avg:415.92ms
step:66/800 train_loss:6.0593 train_time:23293ms step_avg:415.95ms
step:67/800 train_loss:6.2192 train_time:23711ms step_avg:415.98ms
step:68/800 train_loss:6.0813 train_time:24128ms step_avg:416.00ms
step:69/800 train_loss:6.4090 train_time:24546ms step_avg:416.03ms
step:70/800 train_loss:6.0067 train_time:24963ms step_avg:416.05ms
step:71/800 train_loss:6.0566 train_time:25383ms step_avg:416.11ms
step:72/800 train_loss:6.2631 train_time:25799ms step_avg:416.12ms
step:73/800 train_loss:6.1739 train_time:26218ms step_avg:416.16ms
step:74/800 train_loss:6.0741 train_time:26635ms step_avg:416.17ms
step:75/800 train_loss:6.1729 train_time:27052ms step_avg:416.19ms
step:76/800 train_loss:6.1155 train_time:27472ms step_avg:416.24ms
step:77/800 train_loss:6.1173 train_time:27889ms step_avg:416.25ms
step:78/800 train_loss:6.1806 train_time:28306ms step_avg:416.27ms
step:79/800 train_loss:6.2752 train_time:28724ms step_avg:416.30ms
step:80/800 train_loss:6.0855 train_time:29143ms step_avg:416.33ms
step:81/800 train_loss:6.1815 train_time:29560ms step_avg:416.34ms
step:82/800 train_loss:5.9025 train_time:29978ms step_avg:416.36ms
step:83/800 train_loss:6.0977 train_time:30395ms step_avg:416.37ms
step:84/800 train_loss:6.0788 train_time:30811ms step_avg:416.37ms
step:85/800 train_loss:6.0006 train_time:31230ms step_avg:416.40ms
step:86/800 train_loss:5.8651 train_time:31647ms step_avg:416.40ms
step:87/800 train_loss:6.0769 train_time:32064ms step_avg:416.42ms
step:88/800 train_loss:5.9818 train_time:32486ms step_avg:416.48ms
step:89/800 train_loss:6.0671 train_time:32904ms step_avg:416.51ms
step:90/800 train_loss:6.0522 train_time:33325ms step_avg:416.56ms
step:91/800 train_loss:5.9466 train_time:33742ms step_avg:416.57ms
step:92/800 train_loss:5.9400 train_time:34160ms step_avg:416.58ms
step:93/800 train_loss:6.0378 train_time:34577ms step_avg:416.59ms
step:94/800 train_loss:5.9015 train_time:34994ms step_avg:416.60ms
step:95/800 train_loss:5.8737 train_time:35412ms step_avg:416.61ms
step:96/800 train_loss:5.8809 train_time:35831ms step_avg:416.64ms
step:97/800 train_loss:5.7940 train_time:36246ms step_avg:416.62ms
step:98/800 train_loss:5.8868 train_time:36663ms step_avg:416.62ms
step:99/800 train_loss:5.7858 train_time:37084ms step_avg:416.68ms
step:100/800 train_loss:5.9231 train_time:37501ms step_avg:416.67ms
step:101/800 train_loss:5.8655 train_time:37918ms step_avg:416.68ms
step:102/800 train_loss:5.7486 train_time:38335ms step_avg:416.69ms
step:103/800 train_loss:5.8702 train_time:38753ms step_avg:416.70ms
step:104/800 train_loss:5.8398 train_time:39171ms step_avg:416.71ms
step:105/800 train_loss:5.6609 train_time:39588ms step_avg:416.72ms
step:106/800 train_loss:5.7885 train_time:40006ms step_avg:416.73ms
step:107/800 train_loss:5.9962 train_time:40424ms step_avg:416.74ms
step:108/800 train_loss:5.7747 train_time:40841ms step_avg:416.74ms
step:109/800 train_loss:5.4957 train_time:41257ms step_avg:416.74ms
step:110/800 train_loss:5.7362 train_time:41683ms step_avg:416.83ms
step:111/800 train_loss:5.6833 train_time:42102ms step_avg:416.85ms
step:112/800 train_loss:5.6565 train_time:42520ms step_avg:416.86ms
step:113/800 train_loss:5.7573 train_time:42937ms step_avg:416.86ms
step:114/800 train_loss:5.6801 train_time:43355ms step_avg:416.88ms
step:115/800 train_loss:5.5229 train_time:43773ms step_avg:416.89ms
step:116/800 train_loss:5.7139 train_time:44190ms step_avg:416.89ms
step:117/800 train_loss:5.5319 train_time:44608ms step_avg:416.90ms
step:118/800 train_loss:5.5365 train_time:45029ms step_avg:416.93ms
step:119/800 train_loss:5.6369 train_time:45446ms step_avg:416.94ms
step:120/800 train_loss:5.6759 train_time:45864ms step_avg:416.95ms
step:121/800 train_loss:5.5705 train_time:46286ms step_avg:416.99ms
step:122/800 train_loss:5.4472 train_time:46703ms step_avg:416.99ms
step:123/800 train_loss:5.5416 train_time:47121ms step_avg:417.00ms
step:124/800 train_loss:5.3916 train_time:47538ms step_avg:417.00ms
step:125/800 train_loss:5.7193 train_time:47956ms step_avg:417.01ms
step:125/800 val_loss:5.5487 train_time:47969ms step_avg:417.12ms
step:126/800 train_loss:5.5682 train_time:48377ms step_avg:417.04ms
step:127/800 train_loss:5.5375 train_time:48795ms step_avg:417.05ms
step:128/800 train_loss:5.5977 train_time:49212ms step_avg:417.05ms
step:129/800 train_loss:5.4331 train_time:49629ms step_avg:417.05ms
step:130/800 train_loss:5.7082 train_time:50047ms step_avg:417.06ms
step:131/800 train_loss:5.4963 train_time:50467ms step_avg:417.08ms
step:132/800 train_loss:5.5023 train_time:50897ms step_avg:417.19ms
step:133/800 train_loss:5.4214 train_time:51315ms step_avg:417.19ms
step:134/800 train_loss:5.4673 train_time:51730ms step_avg:417.18ms
step:135/800 train_loss:5.4140 train_time:52148ms step_avg:417.19ms
step:136/800 train_loss:5.4639 train_time:52565ms step_avg:417.18ms
step:137/800 train_loss:5.2696 train_time:52983ms step_avg:417.19ms
step:138/800 train_loss:5.4223 train_time:53401ms step_avg:417.20ms
step:139/800 train_loss:5.4022 train_time:53816ms step_avg:417.18ms
step:140/800 train_loss:5.3963 train_time:54234ms step_avg:417.18ms
step:141/800 train_loss:5.4150 train_time:54657ms step_avg:417.23ms
step:142/800 train_loss:5.3203 train_time:55074ms step_avg:417.22ms
step:143/800 train_loss:5.4207 train_time:55491ms step_avg:417.22ms
step:144/800 train_loss:5.2083 train_time:55909ms step_avg:417.23ms
step:145/800 train_loss:5.3776 train_time:56325ms step_avg:417.22ms
step:146/800 train_loss:5.3049 train_time:56742ms step_avg:417.22ms
step:147/800 train_loss:5.2101 train_time:57161ms step_avg:417.24ms
step:148/800 train_loss:5.3385 train_time:57579ms step_avg:417.24ms
step:149/800 train_loss:5.2926 train_time:57995ms step_avg:417.23ms
step:150/800 train_loss:5.3766 train_time:58414ms step_avg:417.24ms
step:151/800 train_loss:5.3749 train_time:58830ms step_avg:417.23ms
step:152/800 train_loss:5.2580 train_time:59247ms step_avg:417.23ms
step:153/800 train_loss:5.2305 train_time:59667ms step_avg:417.25ms
step:154/800 train_loss:5.3003 train_time:60084ms step_avg:417.25ms
step:155/800 train_loss:5.2283 train_time:60503ms step_avg:417.26ms
step:156/800 train_loss:5.2043 train_time:60919ms step_avg:417.26ms
step:157/800 train_loss:5.2100 train_time:61339ms step_avg:417.27ms
step:158/800 train_loss:5.3446 train_time:61757ms step_avg:417.28ms
step:159/800 train_loss:5.1162 train_time:62175ms step_avg:417.28ms
step:160/800 train_loss:5.1719 train_time:62592ms step_avg:417.28ms
step:161/800 train_loss:5.0283 train_time:63008ms step_avg:417.27ms
step:162/800 train_loss:5.1782 train_time:63426ms step_avg:417.28ms
step:163/800 train_loss:5.2126 train_time:63843ms step_avg:417.27ms
step:164/800 train_loss:5.2020 train_time:64261ms step_avg:417.28ms
step:165/800 train_loss:5.0126 train_time:64680ms step_avg:417.29ms
step:166/800 train_loss:5.1326 train_time:65099ms step_avg:417.30ms
step:167/800 train_loss:5.2933 train_time:65517ms step_avg:417.30ms
step:168/800 train_loss:5.0686 train_time:65934ms step_avg:417.30ms
step:169/800 train_loss:5.1598 train_time:66355ms step_avg:417.32ms
step:170/800 train_loss:5.0102 train_time:66772ms step_avg:417.33ms
step:171/800 train_loss:4.9448 train_time:67190ms step_avg:417.33ms
step:172/800 train_loss:5.0552 train_time:67607ms step_avg:417.33ms
step:173/800 train_loss:5.0223 train_time:68024ms step_avg:417.33ms
step:174/800 train_loss:5.0863 train_time:68443ms step_avg:417.33ms
step:175/800 train_loss:5.2269 train_time:68860ms step_avg:417.33ms
step:176/800 train_loss:5.1082 train_time:69277ms step_avg:417.33ms
step:177/800 train_loss:4.9441 train_time:69696ms step_avg:417.34ms
step:178/800 train_loss:4.9181 train_time:70113ms step_avg:417.34ms
step:179/800 train_loss:4.9541 train_time:70531ms step_avg:417.34ms
step:180/800 train_loss:5.0033 train_time:70950ms step_avg:417.35ms
step:181/800 train_loss:4.9790 train_time:71366ms step_avg:417.34ms
step:182/800 train_loss:5.1013 train_time:71783ms step_avg:417.35ms
step:183/800 train_loss:4.9790 train_time:72201ms step_avg:417.35ms
step:184/800 train_loss:4.9135 train_time:72619ms step_avg:417.35ms
step:185/800 train_loss:4.9382 train_time:73036ms step_avg:417.35ms
step:186/800 train_loss:5.0605 train_time:73457ms step_avg:417.37ms
step:187/800 train_loss:4.9419 train_time:73875ms step_avg:417.37ms
step:188/800 train_loss:5.1976 train_time:74293ms step_avg:417.38ms
step:189/800 train_loss:4.9827 train_time:74826ms step_avg:418.02ms
step:190/800 train_loss:4.8956 train_time:75368ms step_avg:418.71ms
step:191/800 train_loss:5.0510 train_time:75785ms step_avg:418.70ms
step:192/800 train_loss:4.8905 train_time:76203ms step_avg:418.70ms
step:193/800 train_loss:4.8083 train_time:76621ms step_avg:418.69ms
step:194/800 train_loss:5.0161 train_time:77037ms step_avg:418.68ms
step:195/800 train_loss:4.9516 train_time:77457ms step_avg:418.68ms
step:196/800 train_loss:5.1432 train_time:77872ms step_avg:418.67ms
step:197/800 train_loss:5.0253 train_time:78290ms step_avg:418.66ms
step:198/800 train_loss:4.8564 train_time:78708ms step_avg:418.66ms
step:199/800 train_loss:4.8999 train_time:79125ms step_avg:418.65ms
step:200/800 train_loss:4.7945 train_time:79543ms step_avg:418.65ms
step:201/800 train_loss:4.8832 train_time:79960ms step_avg:418.64ms
step:202/800 train_loss:4.8045 train_time:80378ms step_avg:418.63ms
step:203/800 train_loss:5.0380 train_time:80798ms step_avg:418.64ms
step:204/800 train_loss:4.9269 train_time:81217ms step_avg:418.64ms
step:205/800 train_loss:4.8934 train_time:81635ms step_avg:418.64ms
step:206/800 train_loss:5.0487 train_time:82058ms step_avg:418.67ms
step:207/800 train_loss:4.7234 train_time:82476ms step_avg:418.66ms
step:208/800 train_loss:4.8700 train_time:82894ms step_avg:418.66ms
step:209/800 train_loss:4.8259 train_time:83313ms step_avg:418.66ms
step:210/800 train_loss:4.9950 train_time:83731ms step_avg:418.66ms
step:211/800 train_loss:4.9043 train_time:84149ms step_avg:418.65ms
step:212/800 train_loss:4.7914 train_time:84568ms step_avg:418.65ms
step:213/800 train_loss:4.9516 train_time:84986ms step_avg:418.65ms
step:214/800 train_loss:4.7625 train_time:85405ms step_avg:418.65ms
step:215/800 train_loss:4.8522 train_time:85822ms step_avg:418.64ms
step:216/800 train_loss:4.7069 train_time:86237ms step_avg:418.63ms
step:217/800 train_loss:4.8503 train_time:86657ms step_avg:418.63ms
step:218/800 train_loss:4.8204 train_time:87074ms step_avg:418.62ms
step:219/800 train_loss:4.7864 train_time:87495ms step_avg:418.64ms
step:220/800 train_loss:4.7933 train_time:87914ms step_avg:418.64ms
step:221/800 train_loss:4.8247 train_time:88331ms step_avg:418.63ms
step:222/800 train_loss:4.8718 train_time:88752ms step_avg:418.64ms
step:223/800 train_loss:4.7988 train_time:89169ms step_avg:418.63ms
step:224/800 train_loss:4.8108 train_time:89587ms step_avg:418.63ms
step:225/800 train_loss:4.9344 train_time:90004ms step_avg:418.62ms
step:226/800 train_loss:4.6747 train_time:90422ms step_avg:418.62ms
step:227/800 train_loss:4.7033 train_time:90839ms step_avg:418.61ms
step:228/800 train_loss:4.6890 train_time:91257ms step_avg:418.61ms
step:229/800 train_loss:4.8491 train_time:91675ms step_avg:418.61ms
step:230/800 train_loss:4.6856 train_time:92093ms step_avg:418.60ms
step:231/800 train_loss:4.8345 train_time:92512ms step_avg:418.60ms
step:232/800 train_loss:4.6955 train_time:92930ms step_avg:418.60ms
step:233/800 train_loss:4.6573 train_time:93347ms step_avg:418.60ms
step:234/800 train_loss:4.8645 train_time:93765ms step_avg:418.59ms
step:235/800 train_loss:4.6963 train_time:94182ms step_avg:418.59ms
step:236/800 train_loss:4.6317 train_time:94598ms step_avg:418.58ms
step:237/800 train_loss:4.8888 train_time:95016ms step_avg:418.57ms
step:238/800 train_loss:4.7726 train_time:95433ms step_avg:418.57ms
step:239/800 train_loss:4.6826 train_time:95854ms step_avg:418.58ms
step:240/800 train_loss:4.8214 train_time:96271ms step_avg:418.57ms
step:241/800 train_loss:4.8082 train_time:96690ms step_avg:418.57ms
step:242/800 train_loss:4.7098 train_time:97109ms step_avg:418.57ms
step:243/800 train_loss:4.8787 train_time:97526ms step_avg:418.57ms
step:244/800 train_loss:4.6957 train_time:97945ms step_avg:418.57ms
step:245/800 train_loss:4.7079 train_time:98363ms step_avg:418.56ms
step:246/800 train_loss:4.7785 train_time:98780ms step_avg:418.56ms
step:247/800 train_loss:4.7331 train_time:99198ms step_avg:418.56ms
step:248/800 train_loss:4.6917 train_time:99618ms step_avg:418.56ms
step:249/800 train_loss:4.8588 train_time:100036ms step_avg:418.56ms
step:250/800 train_loss:4.5878 train_time:100457ms step_avg:418.57ms
step:250/800 val_loss:4.7013 train_time:100471ms step_avg:418.63ms
step:251/800 train_loss:4.6321 train_time:100877ms step_avg:418.58ms
step:252/800 train_loss:4.7608 train_time:101296ms step_avg:418.58ms
step:253/800 train_loss:4.7608 train_time:101718ms step_avg:418.59ms
step:254/800 train_loss:4.6299 train_time:102137ms step_avg:418.59ms
step:255/800 train_loss:4.6549 train_time:102555ms step_avg:418.59ms
step:256/800 train_loss:4.7939 train_time:102974ms step_avg:418.59ms
step:257/800 train_loss:4.7350 train_time:103392ms step_avg:418.59ms
step:258/800 train_loss:4.6965 train_time:103811ms step_avg:418.59ms
step:259/800 train_loss:4.6232 train_time:104229ms step_avg:418.59ms
step:260/800 train_loss:4.6393 train_time:104650ms step_avg:418.60ms
step:261/800 train_loss:4.7112 train_time:105069ms step_avg:418.60ms
step:262/800 train_loss:4.7271 train_time:105487ms step_avg:418.60ms
step:263/800 train_loss:4.6270 train_time:105905ms step_avg:418.60ms
step:264/800 train_loss:4.5729 train_time:106324ms step_avg:418.60ms
step:265/800 train_loss:4.6226 train_time:106742ms step_avg:418.60ms
step:266/800 train_loss:4.4869 train_time:107161ms step_avg:418.60ms
step:267/800 train_loss:4.5397 train_time:107580ms step_avg:418.60ms
step:268/800 train_loss:4.5845 train_time:107998ms step_avg:418.60ms
step:269/800 train_loss:4.5344 train_time:108417ms step_avg:418.60ms
step:270/800 train_loss:4.4970 train_time:108834ms step_avg:418.59ms
step:271/800 train_loss:4.7292 train_time:109252ms step_avg:418.59ms
step:272/800 train_loss:4.6567 train_time:109672ms step_avg:418.59ms
step:273/800 train_loss:4.5149 train_time:110089ms step_avg:418.59ms
step:274/800 train_loss:4.5621 train_time:110508ms step_avg:418.59ms
step:275/800 train_loss:4.6843 train_time:110926ms step_avg:418.59ms
step:276/800 train_loss:4.7004 train_time:111343ms step_avg:418.58ms
step:277/800 train_loss:4.8991 train_time:111761ms step_avg:418.58ms
step:278/800 train_loss:4.6391 train_time:112179ms step_avg:418.58ms
step:279/800 train_loss:4.7729 train_time:112599ms step_avg:418.58ms
step:280/800 train_loss:4.6190 train_time:113019ms step_avg:418.59ms
step:281/800 train_loss:4.6783 train_time:113436ms step_avg:418.58ms
step:282/800 train_loss:4.5832 train_time:113856ms step_avg:418.59ms
step:283/800 train_loss:4.6843 train_time:114274ms step_avg:418.58ms
step:284/800 train_loss:4.5111 train_time:114692ms step_avg:418.58ms
step:285/800 train_loss:4.6781 train_time:115110ms step_avg:418.58ms
step:286/800 train_loss:4.6703 train_time:115528ms step_avg:418.58ms
step:287/800 train_loss:4.7096 train_time:115946ms step_avg:418.58ms
step:288/800 train_loss:4.5581 train_time:116364ms step_avg:418.58ms
step:289/800 train_loss:4.6258 train_time:116782ms step_avg:418.57ms
step:290/800 train_loss:4.4843 train_time:117200ms step_avg:418.57ms
step:291/800 train_loss:4.4821 train_time:117620ms step_avg:418.58ms
step:292/800 train_loss:4.5861 train_time:118038ms step_avg:418.57ms
step:293/800 train_loss:4.4871 train_time:118458ms step_avg:418.58ms
step:294/800 train_loss:4.5455 train_time:118876ms step_avg:418.58ms
step:295/800 train_loss:4.5517 train_time:119296ms step_avg:418.58ms
step:296/800 train_loss:4.4276 train_time:119717ms step_avg:418.59ms
step:297/800 train_loss:4.4071 train_time:120135ms step_avg:418.59ms
step:298/800 train_loss:4.4439 train_time:120551ms step_avg:418.58ms
step:299/800 train_loss:4.5511 train_time:120968ms step_avg:418.57ms
step:300/800 train_loss:4.4255 train_time:121385ms step_avg:418.57ms
step:301/800 train_loss:4.6161 train_time:121804ms step_avg:418.57ms
step:302/800 train_loss:4.5866 train_time:122222ms step_avg:418.57ms
step:303/800 train_loss:4.4979 train_time:122640ms step_avg:418.56ms
step:304/800 train_loss:4.5766 train_time:123058ms step_avg:418.57ms
step:305/800 train_loss:4.5530 train_time:123476ms step_avg:418.56ms
step:306/800 train_loss:5.0397 train_time:123894ms step_avg:418.56ms
step:307/800 train_loss:4.5085 train_time:124313ms step_avg:418.56ms
step:308/800 train_loss:4.4120 train_time:124731ms step_avg:418.56ms
step:309/800 train_loss:4.5963 train_time:125148ms step_avg:418.55ms
step:310/800 train_loss:4.3942 train_time:125565ms step_avg:418.55ms
step:311/800 train_loss:4.6403 train_time:125984ms step_avg:418.55ms
step:312/800 train_loss:4.5379 train_time:126399ms step_avg:418.54ms
step:313/800 train_loss:4.4486 train_time:126817ms step_avg:418.54ms
step:314/800 train_loss:4.5815 train_time:127235ms step_avg:418.54ms
step:315/800 train_loss:4.7189 train_time:127652ms step_avg:418.53ms
step:316/800 train_loss:4.5449 train_time:128071ms step_avg:418.53ms
step:317/800 train_loss:4.4338 train_time:128489ms step_avg:418.53ms
step:318/800 train_loss:4.4358 train_time:128906ms step_avg:418.53ms
step:319/800 train_loss:4.4627 train_time:129325ms step_avg:418.53ms
step:320/800 train_loss:4.4128 train_time:129742ms step_avg:418.52ms
step:321/800 train_loss:4.5034 train_time:130160ms step_avg:418.52ms
step:322/800 train_loss:4.5152 train_time:130578ms step_avg:418.52ms
step:323/800 train_loss:4.4742 train_time:130995ms step_avg:418.51ms
step:324/800 train_loss:4.5530 train_time:131414ms step_avg:418.52ms
step:325/800 train_loss:4.5311 train_time:131833ms step_avg:418.52ms
step:326/800 train_loss:4.6030 train_time:132252ms step_avg:418.52ms
step:327/800 train_loss:4.4536 train_time:132668ms step_avg:418.51ms
step:328/800 train_loss:4.9267 train_time:133086ms step_avg:418.51ms
step:329/800 train_loss:4.6130 train_time:133503ms step_avg:418.51ms
step:330/800 train_loss:4.3878 train_time:133920ms step_avg:418.50ms
step:331/800 train_loss:4.3584 train_time:134337ms step_avg:418.50ms
step:332/800 train_loss:4.5202 train_time:134754ms step_avg:418.49ms
step:333/800 train_loss:4.4350 train_time:135173ms step_avg:418.49ms
step:334/800 train_loss:4.4228 train_time:135590ms step_avg:418.49ms
step:335/800 train_loss:4.3764 train_time:136007ms step_avg:418.48ms
step:336/800 train_loss:4.5701 train_time:136425ms step_avg:418.48ms
step:337/800 train_loss:4.4953 train_time:136843ms step_avg:418.48ms
step:338/800 train_loss:5.0515 train_time:137262ms step_avg:418.48ms
step:339/800 train_loss:4.4773 train_time:137679ms step_avg:418.48ms
step:340/800 train_loss:4.4422 train_time:138097ms step_avg:418.47ms
step:341/800 train_loss:4.4300 train_time:138517ms step_avg:418.48ms
step:342/800 train_loss:4.3661 train_time:138934ms step_avg:418.48ms
step:343/800 train_loss:4.3391 train_time:139352ms step_avg:418.48ms
step:344/800 train_loss:4.4028 train_time:139769ms step_avg:418.47ms
step:345/800 train_loss:4.5017 train_time:140186ms step_avg:418.46ms
step:346/800 train_loss:4.3772 train_time:140604ms step_avg:418.46ms
step:347/800 train_loss:4.3196 train_time:141023ms step_avg:418.46ms
step:348/800 train_loss:4.3649 train_time:141440ms step_avg:418.46ms
step:349/800 train_loss:4.3698 train_time:141858ms step_avg:418.46ms
step:350/800 train_loss:4.3100 train_time:142275ms step_avg:418.46ms
step:351/800 train_loss:3.9998 train_time:142692ms step_avg:418.45ms
step:352/800 train_loss:4.2835 train_time:143110ms step_avg:418.45ms
step:353/800 train_loss:4.6483 train_time:143527ms step_avg:418.45ms
step:354/800 train_loss:4.1632 train_time:143945ms step_avg:418.44ms
step:355/800 train_loss:4.4131 train_time:144364ms step_avg:418.45ms
step:356/800 train_loss:4.3156 train_time:144783ms step_avg:418.45ms
step:357/800 train_loss:4.4056 train_time:145199ms step_avg:418.44ms
step:358/800 train_loss:4.4174 train_time:145617ms step_avg:418.44ms
step:359/800 train_loss:4.3288 train_time:146034ms step_avg:418.44ms
step:360/800 train_loss:4.6157 train_time:146452ms step_avg:418.43ms
step:361/800 train_loss:4.0303 train_time:146870ms step_avg:418.43ms
step:362/800 train_loss:4.5247 train_time:147289ms step_avg:418.44ms
step:363/800 train_loss:4.4254 train_time:147706ms step_avg:418.43ms
step:364/800 train_loss:4.3110 train_time:148123ms step_avg:418.43ms
step:365/800 train_loss:4.2442 train_time:148540ms step_avg:418.42ms
step:366/800 train_loss:4.4120 train_time:148959ms step_avg:418.42ms
step:367/800 train_loss:4.3425 train_time:149377ms step_avg:418.42ms
step:368/800 train_loss:4.3247 train_time:149796ms step_avg:418.42ms
step:369/800 train_loss:4.3230 train_time:150216ms step_avg:418.43ms
step:370/800 train_loss:4.2151 train_time:150633ms step_avg:418.42ms
step:371/800 train_loss:4.3572 train_time:151052ms step_avg:418.43ms
step:372/800 train_loss:4.2836 train_time:151470ms step_avg:418.42ms
step:373/800 train_loss:4.1693 train_time:151887ms step_avg:418.42ms
step:374/800 train_loss:4.3658 train_time:152305ms step_avg:418.42ms
step:375/800 train_loss:4.2998 train_time:152722ms step_avg:418.42ms
step:375/800 val_loss:4.3081 train_time:152735ms step_avg:418.45ms
step:376/800 train_loss:4.2822 train_time:153142ms step_avg:418.42ms
step:377/800 train_loss:4.3443 train_time:153562ms step_avg:418.42ms
step:378/800 train_loss:4.2415 train_time:154090ms step_avg:418.72ms
step:379/800 train_loss:4.2989 train_time:154509ms step_avg:418.72ms
step:380/800 train_loss:4.3538 train_time:155054ms step_avg:419.07ms
step:381/800 train_loss:4.3998 train_time:155471ms step_avg:419.06ms
step:382/800 train_loss:4.3254 train_time:155893ms step_avg:419.07ms
step:383/800 train_loss:4.3066 train_time:156309ms step_avg:419.06ms
step:384/800 train_loss:4.2233 train_time:156727ms step_avg:419.05ms
step:385/800 train_loss:4.3168 train_time:157145ms step_avg:419.05ms
step:386/800 train_loss:4.2352 train_time:157563ms step_avg:419.05ms
step:387/800 train_loss:4.3569 train_time:157980ms step_avg:419.05ms
step:388/800 train_loss:4.5553 train_time:158398ms step_avg:419.04ms
step:389/800 train_loss:4.2521 train_time:158816ms step_avg:419.04ms
step:390/800 train_loss:4.2228 train_time:159233ms step_avg:419.03ms
step:391/800 train_loss:4.3406 train_time:159652ms step_avg:419.03ms
step:392/800 train_loss:4.2545 train_time:160068ms step_avg:419.03ms
step:393/800 train_loss:4.3627 train_time:160486ms step_avg:419.02ms
step:394/800 train_loss:4.1863 train_time:160903ms step_avg:419.02ms
step:395/800 train_loss:4.3266 train_time:161319ms step_avg:419.01ms
step:396/800 train_loss:4.0937 train_time:161736ms step_avg:419.01ms
step:397/800 train_loss:4.2679 train_time:162154ms step_avg:419.00ms
step:398/800 train_loss:4.3472 train_time:162571ms step_avg:419.00ms
step:399/800 train_loss:4.3093 train_time:162991ms step_avg:419.00ms
step:400/800 train_loss:4.2229 train_time:163408ms step_avg:418.99ms
step:401/800 train_loss:4.2917 train_time:163825ms step_avg:418.99ms
step:402/800 train_loss:4.3286 train_time:164243ms step_avg:418.99ms
step:403/800 train_loss:4.2893 train_time:164662ms step_avg:418.99ms
step:404/800 train_loss:4.3870 train_time:165080ms step_avg:418.98ms
step:405/800 train_loss:4.1568 train_time:165498ms step_avg:418.98ms
step:406/800 train_loss:4.2165 train_time:165916ms step_avg:418.98ms
step:407/800 train_loss:4.5003 train_time:166334ms step_avg:418.98ms
step:408/800 train_loss:4.2491 train_time:166751ms step_avg:418.97ms
step:409/800 train_loss:4.2438 train_time:167168ms step_avg:418.97ms
step:410/800 train_loss:4.2917 train_time:167589ms step_avg:418.97ms
step:411/800 train_loss:4.1721 train_time:168007ms step_avg:418.97ms
step:412/800 train_loss:4.1934 train_time:168423ms step_avg:418.96ms
step:413/800 train_loss:4.6104 train_time:168840ms step_avg:418.96ms
step:414/800 train_loss:4.0627 train_time:169258ms step_avg:418.96ms
step:415/800 train_loss:4.4352 train_time:169674ms step_avg:418.95ms
step:416/800 train_loss:4.1892 train_time:170095ms step_avg:418.95ms
step:417/800 train_loss:4.1928 train_time:170513ms step_avg:418.95ms
step:418/800 train_loss:4.3806 train_time:170931ms step_avg:418.95ms
step:419/800 train_loss:4.1122 train_time:171349ms step_avg:418.95ms
step:420/800 train_loss:4.2052 train_time:171765ms step_avg:418.94ms
step:421/800 train_loss:4.1631 train_time:172182ms step_avg:418.93ms
step:422/800 train_loss:4.0634 train_time:172601ms step_avg:418.93ms
step:423/800 train_loss:4.1834 train_time:173019ms step_avg:418.93ms
step:424/800 train_loss:4.2849 train_time:173435ms step_avg:418.93ms
step:425/800 train_loss:4.0645 train_time:173854ms step_avg:418.92ms
step:426/800 train_loss:4.2321 train_time:174271ms step_avg:418.92ms
step:427/800 train_loss:4.1190 train_time:174692ms step_avg:418.93ms
step:428/800 train_loss:4.3169 train_time:175109ms step_avg:418.92ms
step:429/800 train_loss:4.2421 train_time:175528ms step_avg:418.92ms
step:430/800 train_loss:4.1648 train_time:175946ms step_avg:418.92ms
step:431/800 train_loss:4.1426 train_time:176363ms step_avg:418.92ms
step:432/800 train_loss:4.0685 train_time:176780ms step_avg:418.91ms
step:433/800 train_loss:4.1704 train_time:177200ms step_avg:418.91ms
step:434/800 train_loss:4.2433 train_time:177618ms step_avg:418.91ms
step:435/800 train_loss:4.1757 train_time:178033ms step_avg:418.90ms
step:436/800 train_loss:4.2200 train_time:178459ms step_avg:418.92ms
step:437/800 train_loss:4.2353 train_time:178875ms step_avg:418.91ms
step:438/800 train_loss:4.1102 train_time:179294ms step_avg:418.91ms
step:439/800 train_loss:4.1325 train_time:179711ms step_avg:418.91ms
step:440/800 train_loss:4.1079 train_time:180129ms step_avg:418.91ms
step:441/800 train_loss:4.2855 train_time:180547ms step_avg:418.90ms
step:442/800 train_loss:4.1788 train_time:180964ms step_avg:418.90ms
step:443/800 train_loss:4.1619 train_time:181381ms step_avg:418.89ms
step:444/800 train_loss:4.0497 train_time:181799ms step_avg:418.89ms
step:445/800 train_loss:4.3047 train_time:182215ms step_avg:418.89ms
step:446/800 train_loss:4.2387 train_time:182632ms step_avg:418.88ms
step:447/800 train_loss:4.2368 train_time:183048ms step_avg:418.88ms
step:448/800 train_loss:4.1477 train_time:183466ms step_avg:418.87ms
step:449/800 train_loss:4.2427 train_time:183882ms step_avg:418.87ms
step:450/800 train_loss:4.0679 train_time:184299ms step_avg:418.86ms
step:451/800 train_loss:4.1092 train_time:184717ms step_avg:418.86ms
step:452/800 train_loss:3.9901 train_time:185136ms step_avg:418.86ms
step:453/800 train_loss:4.0932 train_time:185552ms step_avg:418.85ms
step:454/800 train_loss:4.0752 train_time:185970ms step_avg:418.85ms
step:455/800 train_loss:4.0342 train_time:186390ms step_avg:418.86ms
step:456/800 train_loss:4.2473 train_time:186807ms step_avg:418.85ms
step:457/800 train_loss:4.1092 train_time:187225ms step_avg:418.85ms
step:458/800 train_loss:4.1912 train_time:187643ms step_avg:418.85ms
step:459/800 train_loss:4.2218 train_time:188058ms step_avg:418.84ms
step:460/800 train_loss:4.0265 train_time:188476ms step_avg:418.83ms
step:461/800 train_loss:4.1980 train_time:188892ms step_avg:418.83ms
step:462/800 train_loss:4.0957 train_time:189309ms step_avg:418.82ms
step:463/800 train_loss:4.0936 train_time:189727ms step_avg:418.82ms
step:464/800 train_loss:4.1686 train_time:190146ms step_avg:418.82ms
step:465/800 train_loss:4.1121 train_time:190563ms step_avg:418.82ms
step:466/800 train_loss:4.1088 train_time:190981ms step_avg:418.82ms
step:467/800 train_loss:4.2236 train_time:191398ms step_avg:418.82ms
step:468/800 train_loss:4.2227 train_time:191815ms step_avg:418.81ms
step:469/800 train_loss:4.1958 train_time:192233ms step_avg:418.81ms
step:470/800 train_loss:4.0962 train_time:192651ms step_avg:418.81ms
step:471/800 train_loss:4.1723 train_time:193067ms step_avg:418.80ms
step:472/800 train_loss:4.2255 train_time:193485ms step_avg:418.80ms
step:473/800 train_loss:4.1478 train_time:193902ms step_avg:418.80ms
step:474/800 train_loss:4.1139 train_time:194319ms step_avg:418.79ms
step:475/800 train_loss:3.9720 train_time:194736ms step_avg:418.79ms
step:476/800 train_loss:4.4066 train_time:195154ms step_avg:418.79ms
step:477/800 train_loss:4.1667 train_time:195571ms step_avg:418.78ms
step:478/800 train_loss:3.9682 train_time:195991ms step_avg:418.78ms
step:479/800 train_loss:4.1852 train_time:196407ms step_avg:418.78ms
step:480/800 train_loss:4.1600 train_time:196823ms step_avg:418.77ms
step:481/800 train_loss:4.2959 train_time:197240ms step_avg:418.77ms
step:482/800 train_loss:4.1043 train_time:197656ms step_avg:418.76ms
step:483/800 train_loss:3.9111 train_time:198073ms step_avg:418.76ms
step:484/800 train_loss:4.1958 train_time:198492ms step_avg:418.76ms
step:485/800 train_loss:4.0423 train_time:198909ms step_avg:418.76ms
step:486/800 train_loss:4.0585 train_time:199327ms step_avg:418.75ms
step:487/800 train_loss:3.9946 train_time:199743ms step_avg:418.75ms
step:488/800 train_loss:4.0487 train_time:200162ms step_avg:418.75ms
step:489/800 train_loss:4.2480 train_time:200578ms step_avg:418.74ms
step:490/800 train_loss:4.0939 train_time:200997ms step_avg:418.74ms
step:491/800 train_loss:3.9890 train_time:201414ms step_avg:418.74ms
step:492/800 train_loss:3.9980 train_time:201830ms step_avg:418.73ms
step:493/800 train_loss:4.1141 train_time:202246ms step_avg:418.73ms
step:494/800 train_loss:3.9579 train_time:202661ms step_avg:418.72ms
step:495/800 train_loss:4.1030 train_time:203078ms step_avg:418.72ms
step:496/800 train_loss:4.0275 train_time:203496ms step_avg:418.72ms
step:497/800 train_loss:3.9337 train_time:203912ms step_avg:418.71ms
step:498/800 train_loss:4.1055 train_time:204330ms step_avg:418.71ms
step:499/800 train_loss:4.1947 train_time:204747ms step_avg:418.71ms
step:500/800 train_loss:4.2253 train_time:205165ms step_avg:418.70ms
step:500/800 val_loss:4.0889 train_time:205178ms step_avg:418.73ms
step:501/800 train_loss:4.1240 train_time:205585ms step_avg:418.71ms
step:502/800 train_loss:4.1720 train_time:206002ms step_avg:418.70ms
step:503/800 train_loss:4.1188 train_time:206419ms step_avg:418.70ms
step:504/800 train_loss:4.1528 train_time:206839ms step_avg:418.70ms
step:505/800 train_loss:4.1139 train_time:207257ms step_avg:418.70ms
step:506/800 train_loss:4.2071 train_time:207673ms step_avg:418.70ms
step:507/800 train_loss:4.0018 train_time:208090ms step_avg:418.69ms
step:508/800 train_loss:4.1350 train_time:208506ms step_avg:418.69ms
step:509/800 train_loss:4.2147 train_time:208923ms step_avg:418.68ms
step:510/800 train_loss:4.1535 train_time:209342ms step_avg:418.68ms
step:511/800 train_loss:3.9654 train_time:209761ms step_avg:418.69ms
step:512/800 train_loss:4.1653 train_time:210177ms step_avg:418.68ms
step:513/800 train_loss:4.0875 train_time:210595ms step_avg:418.68ms
step:514/800 train_loss:4.0593 train_time:211012ms step_avg:418.67ms
step:515/800 train_loss:4.1201 train_time:211428ms step_avg:418.67ms
step:516/800 train_loss:4.1198 train_time:211845ms step_avg:418.67ms
step:517/800 train_loss:4.4510 train_time:212260ms step_avg:418.66ms
step:518/800 train_loss:4.0382 train_time:212678ms step_avg:418.66ms
step:519/800 train_loss:4.1693 train_time:213096ms step_avg:418.66ms
step:520/800 train_loss:4.0951 train_time:213513ms step_avg:418.65ms
step:521/800 train_loss:4.0626 train_time:213931ms step_avg:418.65ms
step:522/800 train_loss:4.0007 train_time:214347ms step_avg:418.65ms
step:523/800 train_loss:4.0242 train_time:214764ms step_avg:418.64ms
step:524/800 train_loss:4.6431 train_time:215180ms step_avg:418.64ms
step:525/800 train_loss:4.1215 train_time:215597ms step_avg:418.63ms
step:526/800 train_loss:4.0660 train_time:216013ms step_avg:418.63ms
step:527/800 train_loss:4.0643 train_time:216430ms step_avg:418.63ms
step:528/800 train_loss:4.0255 train_time:216847ms step_avg:418.62ms
step:529/800 train_loss:3.9967 train_time:217264ms step_avg:418.62ms
step:530/800 train_loss:4.2007 train_time:217684ms step_avg:418.62ms
step:531/800 train_loss:4.0111 train_time:218102ms step_avg:418.62ms
step:532/800 train_loss:4.2891 train_time:218518ms step_avg:418.62ms
step:533/800 train_loss:4.0914 train_time:218935ms step_avg:418.61ms
step:534/800 train_loss:4.0227 train_time:219351ms step_avg:418.61ms
step:535/800 train_loss:4.0535 train_time:219768ms step_avg:418.60ms
step:536/800 train_loss:3.9841 train_time:220187ms step_avg:418.61ms
step:537/800 train_loss:4.0963 train_time:220604ms step_avg:418.60ms
step:538/800 train_loss:4.0963 train_time:221021ms step_avg:418.60ms
step:539/800 train_loss:4.0021 train_time:221439ms step_avg:418.60ms
step:540/800 train_loss:4.4917 train_time:221861ms step_avg:418.60ms
step:541/800 train_loss:4.0326 train_time:222279ms step_avg:418.60ms
step:542/800 train_loss:4.1483 train_time:222696ms step_avg:418.60ms
step:543/800 train_loss:3.9802 train_time:223114ms step_avg:418.60ms
step:544/800 train_loss:3.9638 train_time:223531ms step_avg:418.60ms
step:545/800 train_loss:4.0494 train_time:223949ms step_avg:418.60ms
step:546/800 train_loss:3.9694 train_time:224366ms step_avg:418.59ms
step:547/800 train_loss:4.0159 train_time:224782ms step_avg:418.59ms
step:548/800 train_loss:4.0235 train_time:225200ms step_avg:418.59ms
step:549/800 train_loss:3.9962 train_time:225619ms step_avg:418.59ms
step:550/800 train_loss:4.0859 train_time:226035ms step_avg:418.58ms
step:551/800 train_loss:3.9605 train_time:226453ms step_avg:418.58ms
step:552/800 train_loss:3.9894 train_time:226869ms step_avg:418.58ms
step:553/800 train_loss:4.3088 train_time:227286ms step_avg:418.57ms
step:554/800 train_loss:4.1095 train_time:227703ms step_avg:418.57ms
step:555/800 train_loss:4.0734 train_time:228122ms step_avg:418.57ms
step:556/800 train_loss:4.0390 train_time:228539ms step_avg:418.57ms
step:557/800 train_loss:4.0515 train_time:228958ms step_avg:418.57ms
step:558/800 train_loss:3.7254 train_time:229376ms step_avg:418.57ms
step:559/800 train_loss:3.9685 train_time:229804ms step_avg:418.59ms
step:560/800 train_loss:4.0103 train_time:230221ms step_avg:418.58ms
step:561/800 train_loss:4.0537 train_time:230639ms step_avg:418.58ms
step:562/800 train_loss:3.9678 train_time:231059ms step_avg:418.59ms
step:563/800 train_loss:3.9125 train_time:231477ms step_avg:418.58ms
step:564/800 train_loss:4.1161 train_time:231892ms step_avg:418.58ms
step:565/800 train_loss:3.9330 train_time:232309ms step_avg:418.57ms
step:566/800 train_loss:4.0514 train_time:232725ms step_avg:418.57ms
step:567/800 train_loss:4.0075 train_time:233796ms step_avg:419.74ms
step:568/800 train_loss:3.9493 train_time:234215ms step_avg:419.74ms
step:569/800 train_loss:4.0440 train_time:234634ms step_avg:419.74ms
step:570/800 train_loss:4.0163 train_time:235182ms step_avg:419.97ms
step:571/800 train_loss:4.0393 train_time:235599ms step_avg:419.96ms
step:572/800 train_loss:4.1352 train_time:236016ms step_avg:419.96ms
step:573/800 train_loss:4.0585 train_time:236432ms step_avg:419.95ms
step:574/800 train_loss:4.0660 train_time:236848ms step_avg:419.94ms
step:575/800 train_loss:4.1303 train_time:237266ms step_avg:419.94ms
step:576/800 train_loss:4.0891 train_time:237683ms step_avg:419.93ms
step:577/800 train_loss:4.0984 train_time:238100ms step_avg:419.93ms
step:578/800 train_loss:4.0498 train_time:238517ms step_avg:419.92ms
step:579/800 train_loss:4.0144 train_time:238935ms step_avg:419.92ms
step:580/800 train_loss:4.0049 train_time:239352ms step_avg:419.92ms
step:581/800 train_loss:3.9586 train_time:239769ms step_avg:419.91ms
step:582/800 train_loss:3.9899 train_time:240185ms step_avg:419.90ms
step:583/800 train_loss:4.2135 train_time:240603ms step_avg:419.90ms
step:584/800 train_loss:3.9826 train_time:241022ms step_avg:419.90ms
step:585/800 train_loss:3.9411 train_time:241438ms step_avg:419.89ms
step:586/800 train_loss:4.1229 train_time:241859ms step_avg:419.89ms
step:587/800 train_loss:3.8845 train_time:242278ms step_avg:419.89ms
step:588/800 train_loss:4.0129 train_time:242694ms step_avg:419.89ms
step:589/800 train_loss:4.0220 train_time:243111ms step_avg:419.88ms
step:590/800 train_loss:4.3625 train_time:243528ms step_avg:419.88ms
step:591/800 train_loss:4.1326 train_time:243944ms step_avg:419.87ms
step:592/800 train_loss:3.8791 train_time:244362ms step_avg:419.87ms
step:593/800 train_loss:3.8867 train_time:244779ms step_avg:419.86ms
step:594/800 train_loss:3.8942 train_time:245197ms step_avg:419.86ms
step:595/800 train_loss:3.9214 train_time:245616ms step_avg:419.86ms
step:596/800 train_loss:4.2929 train_time:246032ms step_avg:419.85ms
step:597/800 train_loss:4.0040 train_time:246448ms step_avg:419.84ms
step:598/800 train_loss:3.9452 train_time:246867ms step_avg:419.84ms
step:599/800 train_loss:4.0076 train_time:247282ms step_avg:419.83ms
step:600/800 train_loss:3.8301 train_time:247699ms step_avg:419.83ms
step:601/800 train_loss:3.9510 train_time:248116ms step_avg:419.82ms
step:602/800 train_loss:3.9799 train_time:248532ms step_avg:419.82ms
step:603/800 train_loss:3.9949 train_time:248949ms step_avg:419.81ms
step:604/800 train_loss:4.1293 train_time:249366ms step_avg:419.81ms
step:605/800 train_loss:4.0039 train_time:249782ms step_avg:419.80ms
step:606/800 train_loss:3.9720 train_time:250197ms step_avg:419.79ms
step:607/800 train_loss:3.8984 train_time:250615ms step_avg:419.79ms
step:608/800 train_loss:4.1437 train_time:251034ms step_avg:419.79ms
step:609/800 train_loss:3.9935 train_time:251450ms step_avg:419.78ms
step:610/800 train_loss:3.9698 train_time:251868ms step_avg:419.78ms
step:611/800 train_loss:4.0756 train_time:252285ms step_avg:419.77ms
step:612/800 train_loss:3.9785 train_time:252701ms step_avg:419.77ms
step:613/800 train_loss:3.9489 train_time:253118ms step_avg:419.76ms
step:614/800 train_loss:4.1172 train_time:253533ms step_avg:419.76ms
step:615/800 train_loss:4.0840 train_time:253949ms step_avg:419.75ms
step:616/800 train_loss:4.0474 train_time:254365ms step_avg:419.74ms
step:617/800 train_loss:3.9603 train_time:254782ms step_avg:419.74ms
step:618/800 train_loss:3.9216 train_time:255200ms step_avg:419.74ms
step:619/800 train_loss:4.0246 train_time:255616ms step_avg:419.73ms
step:620/800 train_loss:3.9293 train_time:256033ms step_avg:419.73ms
step:621/800 train_loss:3.9395 train_time:256450ms step_avg:419.72ms
step:622/800 train_loss:4.2349 train_time:256865ms step_avg:419.71ms
step:623/800 train_loss:3.9464 train_time:257284ms step_avg:419.71ms
step:624/800 train_loss:3.9780 train_time:257700ms step_avg:419.71ms
step:625/800 train_loss:4.0519 train_time:258116ms step_avg:419.70ms
step:625/800 val_loss:3.9790 train_time:258129ms step_avg:419.72ms
step:626/800 train_loss:4.0810 train_time:258535ms step_avg:419.70ms
step:627/800 train_loss:4.0983 train_time:258951ms step_avg:419.69ms
step:628/800 train_loss:4.0752 train_time:259366ms step_avg:419.69ms
step:629/800 train_loss:4.1262 train_time:259785ms step_avg:419.68ms
step:630/800 train_loss:3.9354 train_time:260203ms step_avg:419.68ms
step:631/800 train_loss:4.0738 train_time:260619ms step_avg:419.68ms
step:632/800 train_loss:4.1086 train_time:261037ms step_avg:419.67ms
step:633/800 train_loss:4.0065 train_time:261455ms step_avg:419.67ms
step:634/800 train_loss:3.9296 train_time:261871ms step_avg:419.66ms
step:635/800 train_loss:4.0270 train_time:262288ms step_avg:419.66ms
step:636/800 train_loss:4.2863 train_time:262705ms step_avg:419.66ms
step:637/800 train_loss:3.8764 train_time:263122ms step_avg:419.65ms
step:638/800 train_loss:3.7035 train_time:263539ms step_avg:419.65ms
step:639/800 train_loss:3.9310 train_time:263956ms step_avg:419.64ms
step:640/800 train_loss:3.9592 train_time:264373ms step_avg:419.64ms
step:641/800 train_loss:3.9312 train_time:264789ms step_avg:419.63ms
step:642/800 train_loss:3.9304 train_time:265207ms step_avg:419.63ms
step:643/800 train_loss:3.9699 train_time:265623ms step_avg:419.63ms
step:644/800 train_loss:3.9965 train_time:266039ms step_avg:419.62ms
step:645/800 train_loss:3.9133 train_time:266457ms step_avg:419.62ms
step:646/800 train_loss:4.1315 train_time:266874ms step_avg:419.61ms
step:647/800 train_loss:4.0158 train_time:267291ms step_avg:419.61ms
step:648/800 train_loss:4.0214 train_time:267708ms step_avg:419.60ms
step:649/800 train_loss:4.0330 train_time:268124ms step_avg:419.60ms
step:650/800 train_loss:4.1025 train_time:268542ms step_avg:419.60ms
step:651/800 train_loss:3.9685 train_time:268957ms step_avg:419.59ms
step:652/800 train_loss:4.1061 train_time:269374ms step_avg:419.59ms
step:653/800 train_loss:3.9350 train_time:269791ms step_avg:419.58ms
step:654/800 train_loss:4.0071 train_time:270208ms step_avg:419.58ms
step:655/800 train_loss:3.7758 train_time:270626ms step_avg:419.58ms
step:656/800 train_loss:3.9261 train_time:271043ms step_avg:419.57ms
step:657/800 train_loss:3.9312 train_time:271460ms step_avg:419.57ms
step:658/800 train_loss:3.8733 train_time:271878ms step_avg:419.56ms
step:659/800 train_loss:4.0487 train_time:272295ms step_avg:419.56ms
step:660/800 train_loss:3.9416 train_time:272710ms step_avg:419.55ms
step:661/800 train_loss:4.0268 train_time:273127ms step_avg:419.55ms
step:662/800 train_loss:4.1044 train_time:273544ms step_avg:419.55ms
step:663/800 train_loss:4.0094 train_time:273960ms step_avg:419.54ms
step:664/800 train_loss:3.8973 train_time:274377ms step_avg:419.54ms
step:665/800 train_loss:3.9725 train_time:274794ms step_avg:419.53ms
step:666/800 train_loss:3.8440 train_time:275211ms step_avg:419.53ms
step:667/800 train_loss:4.1376 train_time:275627ms step_avg:419.52ms
step:668/800 train_loss:3.9708 train_time:276044ms step_avg:419.52ms
step:669/800 train_loss:3.9676 train_time:276464ms step_avg:419.52ms
step:670/800 train_loss:3.8313 train_time:276880ms step_avg:419.52ms
step:671/800 train_loss:3.9378 train_time:277297ms step_avg:419.51ms
step:672/800 train_loss:3.8975 train_time:277718ms step_avg:419.51ms
step:673/800 train_loss:3.9259 train_time:278135ms step_avg:419.51ms
step:674/800 train_loss:4.2093 train_time:278552ms step_avg:419.51ms
step:675/800 train_loss:3.9951 train_time:278969ms step_avg:419.50ms
step:676/800 train_loss:4.0603 train_time:279388ms step_avg:419.50ms
step:677/800 train_loss:3.8331 train_time:279803ms step_avg:419.50ms
step:678/800 train_loss:3.9363 train_time:280221ms step_avg:419.49ms
step:679/800 train_loss:3.8853 train_time:280639ms step_avg:419.49ms
step:680/800 train_loss:4.0312 train_time:281056ms step_avg:419.49ms
step:681/800 train_loss:3.9309 train_time:281473ms step_avg:419.48ms
step:682/800 train_loss:3.9617 train_time:281892ms step_avg:419.48ms
step:683/800 train_loss:4.0346 train_time:282309ms step_avg:419.48ms
step:684/800 train_loss:4.0805 train_time:282726ms step_avg:419.47ms
step:685/800 train_loss:3.9741 train_time:283143ms step_avg:419.47ms
step:686/800 train_loss:4.0547 train_time:283560ms step_avg:419.47ms
step:687/800 train_loss:3.9777 train_time:283977ms step_avg:419.46ms
step:688/800 train_loss:4.0292 train_time:284394ms step_avg:419.46ms
step:689/800 train_loss:3.6550 train_time:284812ms step_avg:419.46ms
step:690/800 train_loss:3.7644 train_time:285229ms step_avg:419.45ms
step:691/800 train_loss:3.8992 train_time:285645ms step_avg:419.45ms
step:692/800 train_loss:3.7854 train_time:286061ms step_avg:419.44ms
step:693/800 train_loss:4.0010 train_time:286479ms step_avg:419.44ms
step:694/800 train_loss:4.0163 train_time:286896ms step_avg:419.44ms
step:695/800 train_loss:3.9071 train_time:287317ms step_avg:419.44ms
step:696/800 train_loss:3.8899 train_time:287734ms step_avg:419.44ms
step:697/800 train_loss:4.1870 train_time:288149ms step_avg:419.43ms
step:698/800 train_loss:3.9571 train_time:288566ms step_avg:419.43ms
step:699/800 train_loss:3.9845 train_time:288984ms step_avg:419.43ms
step:700/800 train_loss:4.1546 train_time:289400ms step_avg:419.42ms
step:701/800 train_loss:3.9277 train_time:289819ms step_avg:419.42ms
step:702/800 train_loss:3.8739 train_time:290236ms step_avg:419.42ms
step:703/800 train_loss:3.8698 train_time:290651ms step_avg:419.41ms
step:704/800 train_loss:3.8180 train_time:291069ms step_avg:419.41ms
step:705/800 train_loss:3.9146 train_time:291487ms step_avg:419.41ms
step:706/800 train_loss:3.9004 train_time:291902ms step_avg:419.40ms
step:707/800 train_loss:3.9240 train_time:292320ms step_avg:419.40ms
step:708/800 train_loss:3.9924 train_time:292737ms step_avg:419.39ms
step:709/800 train_loss:3.9293 train_time:293154ms step_avg:419.39ms
step:710/800 train_loss:3.9167 train_time:293572ms step_avg:419.39ms
step:711/800 train_loss:3.8920 train_time:293989ms step_avg:419.39ms
step:712/800 train_loss:3.9350 train_time:294406ms step_avg:419.38ms
step:713/800 train_loss:3.9929 train_time:294822ms step_avg:419.38ms
step:714/800 train_loss:4.0032 train_time:295240ms step_avg:419.37ms
step:715/800 train_loss:3.9138 train_time:295656ms step_avg:419.37ms
step:716/800 train_loss:3.9202 train_time:296073ms step_avg:419.37ms
step:717/800 train_loss:3.9432 train_time:296491ms step_avg:419.37ms
step:718/800 train_loss:4.0725 train_time:296908ms step_avg:419.36ms
step:719/800 train_loss:3.9435 train_time:297324ms step_avg:419.36ms
step:720/800 train_loss:4.0101 train_time:297740ms step_avg:419.35ms
step:721/800 train_loss:4.1804 train_time:298157ms step_avg:419.35ms
step:722/800 train_loss:3.8072 train_time:298573ms step_avg:419.34ms
step:723/800 train_loss:4.0649 train_time:298991ms step_avg:419.34ms
step:724/800 train_loss:4.1254 train_time:299409ms step_avg:419.34ms
step:725/800 train_loss:3.8986 train_time:299826ms step_avg:419.34ms
step:726/800 train_loss:3.9900 train_time:300243ms step_avg:419.33ms
step:727/800 train_loss:3.8950 train_time:300660ms step_avg:419.33ms
step:728/800 train_loss:3.8930 train_time:301079ms step_avg:419.33ms
step:729/800 train_loss:4.0712 train_time:301496ms step_avg:419.33ms
step:730/800 train_loss:4.0304 train_time:301915ms step_avg:419.33ms
step:731/800 train_loss:4.0313 train_time:302332ms step_avg:419.32ms
step:732/800 train_loss:3.9123 train_time:302749ms step_avg:419.32ms
step:733/800 train_loss:3.9375 train_time:303166ms step_avg:419.32ms
step:734/800 train_loss:4.1716 train_time:303582ms step_avg:419.31ms
step:735/800 train_loss:3.8904 train_time:304000ms step_avg:419.31ms
step:736/800 train_loss:3.9711 train_time:304418ms step_avg:419.31ms
step:737/800 train_loss:4.0952 train_time:304835ms step_avg:419.31ms
step:738/800 train_loss:3.9954 train_time:305252ms step_avg:419.30ms
step:739/800 train_loss:3.9444 train_time:305669ms step_avg:419.30ms
step:740/800 train_loss:3.8464 train_time:306086ms step_avg:419.30ms
step:741/800 train_loss:4.4974 train_time:306503ms step_avg:419.29ms
step:742/800 train_loss:3.8518 train_time:306920ms step_avg:419.29ms
step:743/800 train_loss:3.9336 train_time:307343ms step_avg:419.30ms
step:744/800 train_loss:3.9287 train_time:307760ms step_avg:419.29ms
step:745/800 train_loss:3.9851 train_time:308178ms step_avg:419.29ms
step:746/800 train_loss:3.9718 train_time:308595ms step_avg:419.29ms
step:747/800 train_loss:3.9433 train_time:309011ms step_avg:419.28ms
step:748/800 train_loss:3.9769 train_time:309427ms step_avg:419.28ms
step:749/800 train_loss:3.9049 train_time:309843ms step_avg:419.27ms
step:750/800 train_loss:3.9140 train_time:310259ms step_avg:419.27ms
step:750/800 val_loss:3.9210 train_time:310273ms step_avg:419.29ms
step:751/800 train_loss:3.9574 train_time:310675ms step_avg:419.27ms
step:752/800 train_loss:3.9062 train_time:311091ms step_avg:419.26ms
step:753/800 train_loss:3.9466 train_time:311507ms step_avg:419.26ms
step:754/800 train_loss:3.9620 train_time:311929ms step_avg:419.26ms
step:755/800 train_loss:3.9308 train_time:312345ms step_avg:419.25ms
step:756/800 train_loss:4.0222 train_time:313379ms step_avg:420.08ms
step:757/800 train_loss:3.8525 train_time:313796ms step_avg:420.08ms
step:758/800 train_loss:4.0688 train_time:314212ms step_avg:420.07ms
step:759/800 train_loss:3.9894 train_time:314629ms step_avg:420.07ms
step:760/800 train_loss:3.9197 train_time:315168ms step_avg:420.22ms
step:761/800 train_loss:4.0220 train_time:315586ms step_avg:420.22ms
step:762/800 train_loss:3.7460 train_time:316001ms step_avg:420.21ms
step:763/800 train_loss:3.9143 train_time:316420ms step_avg:420.21ms
step:764/800 train_loss:4.0180 train_time:316837ms step_avg:420.21ms
step:765/800 train_loss:3.6616 train_time:317252ms step_avg:420.20ms
step:766/800 train_loss:4.1061 train_time:317671ms step_avg:420.20ms
step:767/800 train_loss:3.9564 train_time:318086ms step_avg:420.19ms
step:768/800 train_loss:3.9007 train_time:318503ms step_avg:420.19ms
step:769/800 train_loss:3.9250 train_time:318922ms step_avg:420.19ms
step:770/800 train_loss:3.9509 train_time:319337ms step_avg:420.18ms
step:771/800 train_loss:4.0078 train_time:319755ms step_avg:420.18ms
step:772/800 train_loss:4.2313 train_time:320176ms step_avg:420.18ms
step:773/800 train_loss:3.8015 train_time:320592ms step_avg:420.17ms
step:774/800 train_loss:4.0153 train_time:321009ms step_avg:420.17ms
step:775/800 train_loss:3.9912 train_time:321428ms step_avg:420.17ms
step:776/800 train_loss:3.9489 train_time:321844ms step_avg:420.16ms
step:777/800 train_loss:3.7624 train_time:322262ms step_avg:420.16ms
step:778/800 train_loss:3.7637 train_time:322679ms step_avg:420.16ms
step:779/800 train_loss:3.8250 train_time:323095ms step_avg:420.15ms
step:780/800 train_loss:3.9119 train_time:323512ms step_avg:420.15ms
step:781/800 train_loss:3.9524 train_time:323926ms step_avg:420.14ms
step:782/800 train_loss:4.0141 train_time:324343ms step_avg:420.13ms
step:783/800 train_loss:3.9105 train_time:324759ms step_avg:420.13ms
step:784/800 train_loss:3.9387 train_time:325177ms step_avg:420.12ms
step:785/800 train_loss:3.9165 train_time:325600ms step_avg:420.13ms
step:786/800 train_loss:3.9068 train_time:326017ms step_avg:420.12ms
step:787/800 train_loss:3.8138 train_time:326434ms step_avg:420.12ms
step:788/800 train_loss:4.0675 train_time:326851ms step_avg:420.12ms
step:789/800 train_loss:3.8593 train_time:327272ms step_avg:420.12ms
step:790/800 train_loss:3.9250 train_time:327689ms step_avg:420.11ms
step:791/800 train_loss:3.9838 train_time:328105ms step_avg:420.11ms
step:792/800 train_loss:4.1171 train_time:328522ms step_avg:420.11ms
step:793/800 train_loss:4.1240 train_time:328938ms step_avg:420.10ms
step:794/800 train_loss:3.8622 train_time:329355ms step_avg:420.10ms
step:795/800 train_loss:3.9605 train_time:329773ms step_avg:420.09ms
step:796/800 train_loss:4.0031 train_time:330189ms step_avg:420.09ms
step:797/800 train_loss:4.1051 train_time:330606ms step_avg:420.08ms
step:798/800 train_loss:3.8724 train_time:331022ms step_avg:420.08ms
step:799/800 train_loss:4.0194 train_time:331439ms step_avg:420.07ms
step:800/800 train_loss:3.9252 train_time:331855ms step_avg:420.07ms
step:800/800 val_loss:3.9126 train_time:331873ms step_avg:420.09ms
