====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import json
import dataclasses
import subprocess
import csv
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

def _apply_gate_act(logits: torch.Tensor, kind: str) -> torch.Tensor:
    if kind == "sigmoid":
        return torch.sigmoid(logits)
    if kind == "ns_sigmoid":
        return 0.5 + 0.5 * torch.sigmoid(logits)
    raise ValueError(f"unknown gate_act={kind}")

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.attn_gate = getattr(config, "attn_gate", "none")
        self.gate_pos = getattr(config, "gate_pos", "sdpa")
        self.gate_act = getattr(config, "gate_act", "sigmoid")
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.attn_gate == "headwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_head, bias=False)
            self.gate_param = None
        elif self.attn_gate == "elementwise":
            self.c_gate = nn.Linear(self.n_embd, self.n_embd, bias=False)
            self.gate_param = None
        elif self.attn_gate == "const":
            self.c_gate = None
            self.gate_param = nn.Parameter(torch.zeros(self.n_head, self.head_dim))
        else:
            self.c_gate = None
            self.gate_param = None

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if self.attn_gate != "none" and self.gate_pos == "value":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            v = v * gate
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2) # (B, T, n_head, head_dim)
        if self.attn_gate != "none" and self.gate_pos == "sdpa":
            if self.attn_gate == "const":
                gate = _apply_gate_act(self.gate_param, self.gate_act)[None, None, :, :]
            else:
                gate_logits = self.c_gate(x)
                gate = _apply_gate_act(gate_logits, self.gate_act)
                if self.attn_gate == "headwise":
                    gate = gate.view(B, T, self.n_head, 1)
                else:
                    gate = gate.view(B, T, self.n_head, self.head_dim)
            y = y * gate
        y = y.contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attn_gate : str = "none"
    gate_pos : str = "sdpa"
    gate_act : str = "sigmoid"

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    seed : int = 1337
    attn_gate : str = "none" # none|headwise|elementwise|const
    gate_pos : str = "sdpa" # sdpa|value
    gate_act : str = "sigmoid" # sigmoid|ns_sigmoid
    early_stop_patience : int = 0 # 0 disables early stopping
    early_stop_min_delta : float = 0.0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

def apply_env_overrides():
    # environment-variable overrides allow quick sweeps without editing code
    args.learning_rate = float(os.environ.get("LR", args.learning_rate))
    args.seed = int(os.environ.get("SEED", args.seed))
    args.warmup_iters = int(os.environ.get("WARMUP_ITERS", args.warmup_iters))
    args.num_iterations = int(os.environ.get("NUM_ITERATIONS", args.num_iterations))
    args.early_stop_patience = int(os.environ.get("EARLY_STOP_PATIENCE", args.early_stop_patience))
    args.early_stop_min_delta = float(os.environ.get("EARLY_STOP_MIN_DELTA", args.early_stop_min_delta))
    args.attn_gate = os.environ.get("ATTNGATE", args.attn_gate)
    args.gate_pos = os.environ.get("GATEPOS", args.gate_pos)
    args.gate_act = os.environ.get("GATEACT", args.gate_act)
    args.num_iterations = int(os.environ.get("NUM_ITER", args.num_iterations))
    args.val_loss_every = int(os.environ.get("VAL_EVERY", args.val_loss_every))

def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

apply_env_overrides()
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
np.random.seed(args.seed)

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
git_commit = get_git_commit() if master_process else "unknown"

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    attn_gate=args.attn_gate,
    gate_pos=args.gate_pos,
    gate_act=args.gate_act,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        f.write(f"git_commit: {git_commit}\n")
        f.write(f"seed: {args.seed}\n")
        f.write("hyperparameters:\n")
        f.write(json.dumps(dataclasses.asdict(args), indent=2))
        f.write("\n")
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
best_val_loss = float("inf")
final_val_loss = None
no_improve_count = 0
early_stop_reason = None
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        if not torch.isfinite(val_loss):
            early_stop_reason = "non-finite val_loss"
            final_val_loss = float("nan")
            val_loss_item = float("nan")
        else:
            val_loss_item = val_loss.item()
            final_val_loss = val_loss_item
            if val_loss_item < best_val_loss - args.early_stop_min_delta:
                best_val_loss = val_loss_item
                no_improve_count = 0
            else:
                no_improve_count += 1
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss_item:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
            if early_stop_reason is None and args.early_stop_patience > 0 and no_improve_count >= args.early_stop_patience:
                early_stop_reason = f"early_stop patience={args.early_stop_patience} min_delta={args.early_stop_min_delta}"
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process in a subdir to keep logs tidy
        ckpt_dir = os.path.join(logdir, "checkpoints")
        os.makedirs(ckpt_dir, exist_ok=True)
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, os.path.join(ckpt_dir, f"state_step{step:06d}.pt"))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")
    if early_stop_reason is not None:
        break

if master_process:
    if early_stop_reason is not None:
        print(f"stopped early: {early_stop_reason}")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    timed_steps_final = max(args.num_iterations - 9, 1)
    ms_per_step = training_time_ms / timed_steps_final
    os.makedirs("experiments", exist_ok=True)
    results_path = os.path.join("experiments", "results.csv")
    fieldnames = [
        "run_id",
        "date",
        "git_commit",
        "seed",
        "attn_gate",
        "gate_pos",
        "gate_act",
        "learning_rate",
        "batch_size",
        "device_batch_size",
        "sequence_length",
        "num_iterations",
        "warmdown_iters",
        "final_val_loss",
        "best_val_loss",
        "train_time_ms",
        "ms_per_step",
        "gpu_name",
        "n_gpus",
        "runpod_instance",
        "notes",
    ]
    final_loss_value = final_val_loss if final_val_loss is not None else float("nan")
    best_loss_value = best_val_loss if best_val_loss < float("inf") else float("nan")
    row = {
        "run_id": run_id,
        "date": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        "git_commit": git_commit,
        "seed": args.seed,
        "attn_gate": args.attn_gate,
        "gate_pos": args.gate_pos,
        "gate_act": args.gate_act,
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "device_batch_size": args.device_batch_size,
        "sequence_length": args.sequence_length,
        "num_iterations": args.num_iterations,
        "warmdown_iters": args.warmdown_iters,
        "final_val_loss": final_loss_value,
        "best_val_loss": best_loss_value,
        "train_time_ms": training_time_ms,
        "ms_per_step": ms_per_step,
        "gpu_name": torch.cuda.get_device_name(ddp_local_rank),
        "n_gpus": ddp_world_size,
        "runpod_instance": os.environ.get("RUNPOD_INSTANCE_TYPE", "unknown"),
        "notes": "",
    }
    write_header = not os.path.exists(results_path)
    with open(results_path, "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerow(row)
====================================================================================================
git_commit: bf6d1ff4b5e4ce7280e822ab33e90c4fb250e604
seed: 1337
hyperparameters:
{
  "input_bin": "data/fineweb10B/fineweb_train_*.bin",
  "input_val_bin": "data/fineweb10B/fineweb_val_*.bin",
  "batch_size": 512,
  "device_batch_size": 64,
  "sequence_length": 1024,
  "num_iterations": 800,
  "learning_rate": 0.00396,
  "warmup_iters": 150,
  "warmdown_iters": 1450,
  "weight_decay": 0,
  "seed": 1337,
  "attn_gate": "elementwise",
  "gate_pos": "sdpa",
  "gate_act": "sigmoid",
  "early_stop_patience": 2,
  "early_stop_min_delta": 0.0,
  "val_loss_every": 125,
  "val_tokens": 10485760,
  "save_every": 0
}
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Sun Dec  7 17:54:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:00:07.0 Off |                    0 |
| N/A   46C    P0            116W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:00:08.0 Off |                    0 |
| N/A   48C    P0            146W /  300W |    2276MiB /  81920MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0            139W /  300W |    2276MiB /  81920MiB |     17%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          On  |   00000000:00:0A.0 Off |                    0 |
| N/A   46C    P0            110W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          On  |   00000000:00:0B.0 Off |                    0 |
| N/A   47C    P0            131W /  300W |    2276MiB /  81920MiB |     12%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100 80GB PCIe          On  |   00000000:00:0C.0 Off |                    0 |
| N/A   46C    P0            109W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100 80GB PCIe          On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0            119W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100 80GB PCIe          On  |   00000000:00:0E.0 Off |                    0 |
| N/A   47C    P0            118W /  300W |    2276MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/800 val_loss:16.0015 train_time:257ms step_avg:nanms
step:1/800 train_loss:16.0007 train_time:43922ms step_avg:nanms
step:2/800 train_loss:15.8758 train_time:44952ms step_avg:nanms
step:3/800 train_loss:15.5559 train_time:45385ms step_avg:nanms
step:4/800 train_loss:14.8941 train_time:45806ms step_avg:nanms
step:5/800 train_loss:13.6105 train_time:46226ms step_avg:nanms
step:6/800 train_loss:11.9345 train_time:46646ms step_avg:nanms
step:7/800 train_loss:10.4025 train_time:47067ms step_avg:nanms
step:8/800 train_loss:9.7871 train_time:47492ms step_avg:nanms
step:9/800 train_loss:9.5492 train_time:47912ms step_avg:nanms
step:10/800 train_loss:9.3880 train_time:48333ms step_avg:nanms
step:11/800 train_loss:9.1944 train_time:416ms step_avg:nanms
step:12/800 train_loss:9.0365 train_time:838ms step_avg:nanms
step:13/800 train_loss:8.7686 train_time:1259ms step_avg:419.77ms
step:14/800 train_loss:8.6227 train_time:1683ms step_avg:420.73ms
step:15/800 train_loss:8.4382 train_time:2103ms step_avg:420.66ms
step:16/800 train_loss:8.2403 train_time:2522ms step_avg:420.40ms
step:17/800 train_loss:8.0670 train_time:2942ms step_avg:420.28ms
step:18/800 train_loss:7.9533 train_time:3361ms step_avg:420.12ms
step:19/800 train_loss:7.7099 train_time:3781ms step_avg:420.11ms
step:20/800 train_loss:7.6035 train_time:4201ms step_avg:420.07ms
step:21/800 train_loss:7.2359 train_time:4622ms step_avg:420.20ms
step:22/800 train_loss:7.4897 train_time:5042ms step_avg:420.17ms
step:23/800 train_loss:7.6246 train_time:5461ms step_avg:420.09ms
step:24/800 train_loss:7.2965 train_time:5883ms step_avg:420.20ms
step:25/800 train_loss:7.3637 train_time:6304ms step_avg:420.24ms
step:26/800 train_loss:7.1317 train_time:6723ms step_avg:420.19ms
step:27/800 train_loss:7.0249 train_time:7144ms step_avg:420.25ms
step:28/800 train_loss:7.1672 train_time:7564ms step_avg:420.21ms
step:29/800 train_loss:6.8289 train_time:7986ms step_avg:420.31ms
step:30/800 train_loss:7.0515 train_time:8407ms step_avg:420.33ms
step:31/800 train_loss:6.9036 train_time:8828ms step_avg:420.40ms
step:32/800 train_loss:6.8235 train_time:9250ms step_avg:420.46ms
step:33/800 train_loss:6.6032 train_time:9673ms step_avg:420.57ms
step:34/800 train_loss:7.0251 train_time:10095ms step_avg:420.62ms
step:35/800 train_loss:6.7959 train_time:10517ms step_avg:420.68ms
step:36/800 train_loss:6.9902 train_time:10938ms step_avg:420.70ms
step:37/800 train_loss:6.8731 train_time:11360ms step_avg:420.76ms
step:38/800 train_loss:6.7381 train_time:11784ms step_avg:420.86ms
step:39/800 train_loss:6.6021 train_time:12206ms step_avg:420.90ms
step:40/800 train_loss:6.6980 train_time:12628ms step_avg:420.94ms
step:41/800 train_loss:6.5644 train_time:13050ms step_avg:420.97ms
step:42/800 train_loss:6.6058 train_time:13473ms step_avg:421.03ms
step:43/800 train_loss:6.4268 train_time:13895ms step_avg:421.08ms
step:44/800 train_loss:6.5252 train_time:14319ms step_avg:421.15ms
step:45/800 train_loss:6.5010 train_time:14743ms step_avg:421.23ms
step:46/800 train_loss:6.7021 train_time:15164ms step_avg:421.22ms
step:47/800 train_loss:6.5035 train_time:15587ms step_avg:421.26ms
step:48/800 train_loss:6.3351 train_time:16009ms step_avg:421.28ms
step:49/800 train_loss:6.5830 train_time:16428ms step_avg:421.23ms
step:50/800 train_loss:6.4429 train_time:16850ms step_avg:421.25ms
step:51/800 train_loss:6.5963 train_time:17271ms step_avg:421.24ms
step:52/800 train_loss:6.4417 train_time:17692ms step_avg:421.24ms
step:53/800 train_loss:6.2795 train_time:18114ms step_avg:421.26ms
step:54/800 train_loss:6.4082 train_time:18536ms step_avg:421.28ms
step:55/800 train_loss:6.3362 train_time:18958ms step_avg:421.28ms
step:56/800 train_loss:6.6435 train_time:19384ms step_avg:421.38ms
step:57/800 train_loss:6.3168 train_time:19805ms step_avg:421.38ms
step:58/800 train_loss:6.1902 train_time:20226ms step_avg:421.37ms
step:59/800 train_loss:6.3647 train_time:20647ms step_avg:421.36ms
step:60/800 train_loss:6.2742 train_time:21069ms step_avg:421.37ms
step:61/800 train_loss:6.3870 train_time:21490ms step_avg:421.37ms
step:62/800 train_loss:6.1851 train_time:21912ms step_avg:421.38ms
step:63/800 train_loss:6.2672 train_time:22332ms step_avg:421.37ms
step:64/800 train_loss:6.2220 train_time:22754ms step_avg:421.37ms
step:65/800 train_loss:6.7562 train_time:23176ms step_avg:421.38ms
step:66/800 train_loss:6.0621 train_time:23600ms step_avg:421.43ms
step:67/800 train_loss:6.2278 train_time:24023ms step_avg:421.45ms
step:68/800 train_loss:6.0837 train_time:24445ms step_avg:421.47ms
step:69/800 train_loss:6.4068 train_time:24868ms step_avg:421.49ms
step:70/800 train_loss:6.0092 train_time:25290ms step_avg:421.50ms
step:71/800 train_loss:6.0602 train_time:25713ms step_avg:421.52ms
step:72/800 train_loss:6.2679 train_time:26134ms step_avg:421.52ms
step:73/800 train_loss:6.1800 train_time:26556ms step_avg:421.52ms
step:74/800 train_loss:6.0833 train_time:26980ms step_avg:421.56ms
step:75/800 train_loss:6.1784 train_time:27404ms step_avg:421.60ms
step:76/800 train_loss:6.1293 train_time:27827ms step_avg:421.62ms
step:77/800 train_loss:6.1252 train_time:28249ms step_avg:421.62ms
step:78/800 train_loss:6.1818 train_time:28672ms step_avg:421.65ms
step:79/800 train_loss:6.2758 train_time:29097ms step_avg:421.69ms
step:80/800 train_loss:6.0859 train_time:29521ms step_avg:421.73ms
step:81/800 train_loss:6.1857 train_time:29943ms step_avg:421.73ms
step:82/800 train_loss:5.9140 train_time:30367ms step_avg:421.77ms
step:83/800 train_loss:6.1000 train_time:30790ms step_avg:421.78ms
step:84/800 train_loss:6.0786 train_time:31213ms step_avg:421.79ms
step:85/800 train_loss:6.0066 train_time:31635ms step_avg:421.80ms
step:86/800 train_loss:5.8699 train_time:32058ms step_avg:421.82ms
step:87/800 train_loss:6.0842 train_time:32484ms step_avg:421.87ms
step:88/800 train_loss:5.9849 train_time:32906ms step_avg:421.87ms
step:89/800 train_loss:6.0703 train_time:33329ms step_avg:421.89ms
step:90/800 train_loss:6.0587 train_time:33753ms step_avg:421.91ms
step:91/800 train_loss:5.9551 train_time:34176ms step_avg:421.92ms
step:92/800 train_loss:5.9372 train_time:34598ms step_avg:421.93ms
step:93/800 train_loss:6.0304 train_time:35022ms step_avg:421.96ms
step:94/800 train_loss:5.9099 train_time:35446ms step_avg:421.97ms
step:95/800 train_loss:5.8797 train_time:35869ms step_avg:421.99ms
step:96/800 train_loss:5.8870 train_time:36292ms step_avg:422.00ms
step:97/800 train_loss:5.7925 train_time:36714ms step_avg:422.00ms
step:98/800 train_loss:5.8947 train_time:37136ms step_avg:421.99ms
step:99/800 train_loss:5.7901 train_time:37557ms step_avg:421.99ms
step:100/800 train_loss:5.9350 train_time:37984ms step_avg:422.05ms
step:101/800 train_loss:5.8711 train_time:38407ms step_avg:422.05ms
step:102/800 train_loss:5.7526 train_time:38832ms step_avg:422.09ms
step:103/800 train_loss:5.8805 train_time:39254ms step_avg:422.09ms
step:104/800 train_loss:5.8455 train_time:39681ms step_avg:422.14ms
step:105/800 train_loss:5.6563 train_time:40103ms step_avg:422.14ms
step:106/800 train_loss:5.7907 train_time:40527ms step_avg:422.16ms
step:107/800 train_loss:6.0117 train_time:40949ms step_avg:422.16ms
step:108/800 train_loss:5.7682 train_time:41383ms step_avg:422.27ms
step:109/800 train_loss:5.4918 train_time:41805ms step_avg:422.27ms
step:110/800 train_loss:5.7367 train_time:42229ms step_avg:422.29ms
step:111/800 train_loss:5.6814 train_time:42654ms step_avg:422.32ms
step:112/800 train_loss:5.6659 train_time:43078ms step_avg:422.34ms
step:113/800 train_loss:5.7526 train_time:43501ms step_avg:422.34ms
step:114/800 train_loss:5.6822 train_time:43925ms step_avg:422.35ms
step:115/800 train_loss:5.5288 train_time:44348ms step_avg:422.36ms
step:116/800 train_loss:5.7006 train_time:44771ms step_avg:422.37ms
step:117/800 train_loss:5.5344 train_time:45196ms step_avg:422.39ms
step:118/800 train_loss:5.5361 train_time:45619ms step_avg:422.40ms
step:119/800 train_loss:5.6370 train_time:46043ms step_avg:422.42ms
step:120/800 train_loss:5.6667 train_time:46467ms step_avg:422.43ms
step:121/800 train_loss:5.5791 train_time:46891ms step_avg:422.44ms
step:122/800 train_loss:5.4552 train_time:47314ms step_avg:422.44ms
step:123/800 train_loss:5.5370 train_time:47735ms step_avg:422.43ms
step:124/800 train_loss:5.4052 train_time:48157ms step_avg:422.43ms
step:125/800 train_loss:5.7290 train_time:48584ms step_avg:422.47ms
step:125/800 val_loss:5.5388 train_time:48588ms step_avg:422.50ms
step:126/800 train_loss:5.5445 train_time:49011ms step_avg:422.51ms
step:127/800 train_loss:5.5220 train_time:49434ms step_avg:422.51ms
step:128/800 train_loss:5.6074 train_time:49857ms step_avg:422.51ms
step:129/800 train_loss:5.4425 train_time:50279ms step_avg:422.51ms
step:130/800 train_loss:5.6982 train_time:50702ms step_avg:422.52ms
step:131/800 train_loss:5.4914 train_time:51125ms step_avg:422.52ms
step:132/800 train_loss:5.5041 train_time:51553ms step_avg:422.56ms
step:133/800 train_loss:5.4300 train_time:51974ms step_avg:422.56ms
step:134/800 train_loss:5.4618 train_time:52397ms step_avg:422.56ms
step:135/800 train_loss:5.4189 train_time:52820ms step_avg:422.56ms
step:136/800 train_loss:5.4692 train_time:53243ms step_avg:422.57ms
step:137/800 train_loss:5.2642 train_time:53666ms step_avg:422.57ms
step:138/800 train_loss:5.4218 train_time:54088ms step_avg:422.56ms
step:139/800 train_loss:5.3848 train_time:54511ms step_avg:422.57ms
step:140/800 train_loss:5.3883 train_time:54935ms step_avg:422.58ms
step:141/800 train_loss:5.4306 train_time:55359ms step_avg:422.59ms
step:142/800 train_loss:5.3301 train_time:55782ms step_avg:422.59ms
step:143/800 train_loss:5.4214 train_time:56206ms step_avg:422.60ms
step:144/800 train_loss:5.2011 train_time:56629ms step_avg:422.61ms
step:145/800 train_loss:5.3729 train_time:57055ms step_avg:422.63ms
step:146/800 train_loss:5.3022 train_time:57477ms step_avg:422.62ms
step:147/800 train_loss:5.2197 train_time:57900ms step_avg:422.63ms
step:148/800 train_loss:5.3323 train_time:58325ms step_avg:422.64ms
step:149/800 train_loss:5.2959 train_time:58750ms step_avg:422.66ms
step:150/800 train_loss:5.3737 train_time:59174ms step_avg:422.67ms
step:151/800 train_loss:5.3796 train_time:59598ms step_avg:422.68ms
step:152/800 train_loss:5.2558 train_time:60022ms step_avg:422.69ms
step:153/800 train_loss:5.2303 train_time:60447ms step_avg:422.71ms
step:154/800 train_loss:5.2957 train_time:60870ms step_avg:422.71ms
step:155/800 train_loss:5.2285 train_time:61293ms step_avg:422.71ms
step:156/800 train_loss:5.2008 train_time:61714ms step_avg:422.70ms
step:157/800 train_loss:5.2037 train_time:62136ms step_avg:422.70ms
step:158/800 train_loss:5.3414 train_time:62559ms step_avg:422.69ms
step:159/800 train_loss:5.1103 train_time:62967ms step_avg:422.59ms
step:160/800 train_loss:5.1710 train_time:63384ms step_avg:422.56ms
step:161/800 train_loss:5.0304 train_time:63801ms step_avg:422.53ms
step:162/800 train_loss:5.1756 train_time:64220ms step_avg:422.50ms
step:163/800 train_loss:5.2084 train_time:64642ms step_avg:422.50ms
step:164/800 train_loss:5.2016 train_time:65062ms step_avg:422.48ms
step:165/800 train_loss:5.0118 train_time:65479ms step_avg:422.44ms
step:166/800 train_loss:5.1244 train_time:65896ms step_avg:422.41ms
step:167/800 train_loss:5.2842 train_time:66315ms step_avg:422.39ms
step:168/800 train_loss:5.0580 train_time:66738ms step_avg:422.39ms
step:169/800 train_loss:5.1405 train_time:67156ms step_avg:422.36ms
step:170/800 train_loss:5.0072 train_time:67574ms step_avg:422.34ms
step:171/800 train_loss:4.9466 train_time:67991ms step_avg:422.31ms
step:172/800 train_loss:5.0527 train_time:68410ms step_avg:422.29ms
step:173/800 train_loss:5.0158 train_time:68829ms step_avg:422.26ms
step:174/800 train_loss:5.0855 train_time:69247ms step_avg:422.24ms
step:175/800 train_loss:5.2236 train_time:69664ms step_avg:422.21ms
step:176/800 train_loss:5.1071 train_time:70083ms step_avg:422.19ms
step:177/800 train_loss:4.9357 train_time:70503ms step_avg:422.18ms
step:178/800 train_loss:4.9154 train_time:70921ms step_avg:422.15ms
step:179/800 train_loss:4.9509 train_time:71340ms step_avg:422.13ms
step:180/800 train_loss:5.0001 train_time:71758ms step_avg:422.10ms
step:181/800 train_loss:4.9731 train_time:72176ms step_avg:422.08ms
step:182/800 train_loss:5.0937 train_time:72594ms step_avg:422.06ms
step:183/800 train_loss:4.9747 train_time:73012ms step_avg:422.04ms
step:184/800 train_loss:4.9140 train_time:73431ms step_avg:422.02ms
step:185/800 train_loss:4.9344 train_time:73847ms step_avg:421.98ms
step:186/800 train_loss:5.0562 train_time:74264ms step_avg:421.96ms
step:187/800 train_loss:4.9410 train_time:74703ms step_avg:422.05ms
step:188/800 train_loss:5.1920 train_time:75128ms step_avg:422.07ms
step:189/800 train_loss:4.9734 train_time:75657ms step_avg:422.66ms
step:190/800 train_loss:4.8924 train_time:76202ms step_avg:423.35ms
step:191/800 train_loss:5.0550 train_time:76620ms step_avg:423.31ms
step:192/800 train_loss:4.8825 train_time:77040ms step_avg:423.30ms
step:193/800 train_loss:4.8059 train_time:77459ms step_avg:423.28ms
step:194/800 train_loss:5.0188 train_time:77877ms step_avg:423.24ms
step:195/800 train_loss:4.9445 train_time:78299ms step_avg:423.24ms
step:196/800 train_loss:5.1447 train_time:78717ms step_avg:423.21ms
step:197/800 train_loss:5.0236 train_time:79140ms step_avg:423.21ms
step:198/800 train_loss:4.8580 train_time:79562ms step_avg:423.20ms
step:199/800 train_loss:4.9000 train_time:79981ms step_avg:423.18ms
step:200/800 train_loss:4.7895 train_time:80401ms step_avg:423.16ms
step:201/800 train_loss:4.8791 train_time:80819ms step_avg:423.14ms
step:202/800 train_loss:4.8006 train_time:81240ms step_avg:423.13ms
step:203/800 train_loss:5.0319 train_time:81660ms step_avg:423.11ms
step:204/800 train_loss:4.9242 train_time:82079ms step_avg:423.09ms
step:205/800 train_loss:4.8933 train_time:82497ms step_avg:423.06ms
step:206/800 train_loss:5.0544 train_time:82916ms step_avg:423.04ms
step:207/800 train_loss:4.7220 train_time:83337ms step_avg:423.03ms
step:208/800 train_loss:4.8678 train_time:83756ms step_avg:423.01ms
step:209/800 train_loss:4.8220 train_time:84174ms step_avg:422.98ms
step:210/800 train_loss:4.9911 train_time:84593ms step_avg:422.96ms
step:211/800 train_loss:4.9036 train_time:85013ms step_avg:422.95ms
step:212/800 train_loss:4.7863 train_time:85433ms step_avg:422.94ms
step:213/800 train_loss:4.9390 train_time:85852ms step_avg:422.91ms
step:214/800 train_loss:4.7656 train_time:86270ms step_avg:422.89ms
step:215/800 train_loss:4.8528 train_time:86690ms step_avg:422.88ms
step:216/800 train_loss:4.7102 train_time:87111ms step_avg:422.87ms
step:217/800 train_loss:4.8417 train_time:87530ms step_avg:422.85ms
step:218/800 train_loss:4.8163 train_time:87949ms step_avg:422.83ms
step:219/800 train_loss:4.7810 train_time:88368ms step_avg:422.81ms
step:220/800 train_loss:4.7927 train_time:88788ms step_avg:422.80ms
step:221/800 train_loss:4.8194 train_time:89207ms step_avg:422.78ms
step:222/800 train_loss:4.8631 train_time:89626ms step_avg:422.76ms
step:223/800 train_loss:4.8059 train_time:90045ms step_avg:422.75ms
step:224/800 train_loss:4.7963 train_time:90464ms step_avg:422.73ms
step:225/800 train_loss:4.9358 train_time:90883ms step_avg:422.71ms
step:226/800 train_loss:4.6645 train_time:91301ms step_avg:422.69ms
step:227/800 train_loss:4.6944 train_time:91720ms step_avg:422.67ms
step:228/800 train_loss:4.6848 train_time:92140ms step_avg:422.66ms
step:229/800 train_loss:4.8459 train_time:92557ms step_avg:422.63ms
step:230/800 train_loss:4.6847 train_time:92976ms step_avg:422.62ms
step:231/800 train_loss:4.8299 train_time:93396ms step_avg:422.60ms
step:232/800 train_loss:4.6976 train_time:93813ms step_avg:422.58ms
step:233/800 train_loss:4.6527 train_time:94231ms step_avg:422.56ms
step:234/800 train_loss:4.8568 train_time:94649ms step_avg:422.54ms
step:235/800 train_loss:4.6901 train_time:95067ms step_avg:422.52ms
step:236/800 train_loss:4.6233 train_time:95486ms step_avg:422.51ms
step:237/800 train_loss:4.8819 train_time:95905ms step_avg:422.49ms
step:238/800 train_loss:4.7583 train_time:96322ms step_avg:422.47ms
step:239/800 train_loss:4.6711 train_time:96742ms step_avg:422.45ms
step:240/800 train_loss:4.8197 train_time:97160ms step_avg:422.43ms
step:241/800 train_loss:4.7997 train_time:97577ms step_avg:422.41ms
step:242/800 train_loss:4.7061 train_time:97994ms step_avg:422.39ms
step:243/800 train_loss:4.8793 train_time:98413ms step_avg:422.37ms
step:244/800 train_loss:4.6889 train_time:98832ms step_avg:422.36ms
step:245/800 train_loss:4.7061 train_time:99250ms step_avg:422.34ms
step:246/800 train_loss:4.7768 train_time:99668ms step_avg:422.32ms
step:247/800 train_loss:4.7317 train_time:100088ms step_avg:422.31ms
step:248/800 train_loss:4.6872 train_time:100506ms step_avg:422.30ms
step:249/800 train_loss:4.8545 train_time:100925ms step_avg:422.28ms
step:250/800 train_loss:4.5818 train_time:101345ms step_avg:422.27ms
step:250/800 val_loss:4.6958 train_time:101358ms step_avg:422.33ms
step:251/800 train_loss:4.6309 train_time:101767ms step_avg:422.27ms
step:252/800 train_loss:4.7592 train_time:102184ms step_avg:422.25ms
step:253/800 train_loss:4.7585 train_time:102602ms step_avg:422.23ms
step:254/800 train_loss:4.6233 train_time:103021ms step_avg:422.22ms
step:255/800 train_loss:4.6419 train_time:103440ms step_avg:422.20ms
step:256/800 train_loss:4.7923 train_time:103858ms step_avg:422.19ms
step:257/800 train_loss:4.7306 train_time:104276ms step_avg:422.17ms
step:258/800 train_loss:4.6922 train_time:104694ms step_avg:422.15ms
step:259/800 train_loss:4.6198 train_time:105112ms step_avg:422.14ms
step:260/800 train_loss:4.6366 train_time:105530ms step_avg:422.12ms
step:261/800 train_loss:4.7084 train_time:105948ms step_avg:422.10ms
step:262/800 train_loss:4.7184 train_time:106369ms step_avg:422.10ms
step:263/800 train_loss:4.6228 train_time:106787ms step_avg:422.08ms
step:264/800 train_loss:4.5668 train_time:107208ms step_avg:422.08ms
step:265/800 train_loss:4.6169 train_time:107626ms step_avg:422.06ms
step:266/800 train_loss:4.4800 train_time:108046ms step_avg:422.05ms
step:267/800 train_loss:4.5302 train_time:108464ms step_avg:422.04ms
step:268/800 train_loss:4.5767 train_time:108882ms step_avg:422.02ms
step:269/800 train_loss:4.5307 train_time:109301ms step_avg:422.01ms
step:270/800 train_loss:4.4920 train_time:109717ms step_avg:421.99ms
step:271/800 train_loss:4.7289 train_time:110136ms step_avg:421.98ms
step:272/800 train_loss:4.6485 train_time:110557ms step_avg:421.97ms
step:273/800 train_loss:4.5092 train_time:110974ms step_avg:421.96ms
step:274/800 train_loss:4.5535 train_time:111392ms step_avg:421.94ms
step:275/800 train_loss:4.6851 train_time:111811ms step_avg:421.93ms
step:276/800 train_loss:4.6911 train_time:112231ms step_avg:421.92ms
step:277/800 train_loss:4.9022 train_time:112648ms step_avg:421.90ms
step:278/800 train_loss:4.6325 train_time:113068ms step_avg:421.90ms
step:279/800 train_loss:4.7604 train_time:113489ms step_avg:421.89ms
step:280/800 train_loss:4.6131 train_time:113912ms step_avg:421.90ms
step:281/800 train_loss:4.6739 train_time:114329ms step_avg:421.88ms
step:282/800 train_loss:4.5755 train_time:114749ms step_avg:421.87ms
step:283/800 train_loss:4.6741 train_time:115168ms step_avg:421.86ms
step:284/800 train_loss:4.5023 train_time:115586ms step_avg:421.85ms
step:285/800 train_loss:4.6720 train_time:116008ms step_avg:421.85ms
step:286/800 train_loss:4.6532 train_time:116429ms step_avg:421.84ms
step:287/800 train_loss:4.7028 train_time:116847ms step_avg:421.83ms
step:288/800 train_loss:4.5520 train_time:117266ms step_avg:421.82ms
step:289/800 train_loss:4.6092 train_time:117685ms step_avg:421.81ms
step:290/800 train_loss:4.4791 train_time:118105ms step_avg:421.80ms
step:291/800 train_loss:4.4743 train_time:118523ms step_avg:421.79ms
step:292/800 train_loss:4.5838 train_time:118941ms step_avg:421.78ms
step:293/800 train_loss:4.4832 train_time:119361ms step_avg:421.77ms
step:294/800 train_loss:4.5397 train_time:119777ms step_avg:421.75ms
step:295/800 train_loss:4.5452 train_time:120196ms step_avg:421.74ms
step:296/800 train_loss:4.4244 train_time:120615ms step_avg:421.73ms
step:297/800 train_loss:4.4028 train_time:121030ms step_avg:421.71ms
step:298/800 train_loss:4.4304 train_time:121449ms step_avg:421.70ms
step:299/800 train_loss:4.5412 train_time:121868ms step_avg:421.69ms
step:300/800 train_loss:4.4199 train_time:122286ms step_avg:421.67ms
step:301/800 train_loss:4.6053 train_time:122704ms step_avg:421.66ms
step:302/800 train_loss:4.5707 train_time:123123ms step_avg:421.65ms
step:303/800 train_loss:4.4899 train_time:123540ms step_avg:421.64ms
step:304/800 train_loss:4.5626 train_time:123957ms step_avg:421.62ms
step:305/800 train_loss:4.5424 train_time:124377ms step_avg:421.62ms
step:306/800 train_loss:5.0329 train_time:124794ms step_avg:421.60ms
step:307/800 train_loss:4.4970 train_time:125213ms step_avg:421.59ms
step:308/800 train_loss:4.4029 train_time:125633ms step_avg:421.59ms
step:309/800 train_loss:4.5939 train_time:126051ms step_avg:421.58ms
step:310/800 train_loss:4.3859 train_time:126469ms step_avg:421.56ms
step:311/800 train_loss:4.6272 train_time:126888ms step_avg:421.55ms
step:312/800 train_loss:4.5258 train_time:127310ms step_avg:421.56ms
step:313/800 train_loss:4.4379 train_time:127728ms step_avg:421.55ms
step:314/800 train_loss:4.5629 train_time:128147ms step_avg:421.54ms
step:315/800 train_loss:4.6960 train_time:128566ms step_avg:421.53ms
step:316/800 train_loss:4.5321 train_time:128983ms step_avg:421.51ms
step:317/800 train_loss:4.4065 train_time:129401ms step_avg:421.50ms
step:318/800 train_loss:4.4336 train_time:129819ms step_avg:421.49ms
step:319/800 train_loss:4.4538 train_time:130238ms step_avg:421.48ms
step:320/800 train_loss:4.4037 train_time:130655ms step_avg:421.47ms
step:321/800 train_loss:4.4900 train_time:131073ms step_avg:421.46ms
step:322/800 train_loss:4.5022 train_time:131496ms step_avg:421.46ms
step:323/800 train_loss:4.4643 train_time:131915ms step_avg:421.45ms
step:324/800 train_loss:4.5349 train_time:132335ms step_avg:421.45ms
step:325/800 train_loss:4.5236 train_time:132752ms step_avg:421.44ms
step:326/800 train_loss:4.5851 train_time:133171ms step_avg:421.43ms
step:327/800 train_loss:4.4349 train_time:133588ms step_avg:421.41ms
step:328/800 train_loss:4.9050 train_time:134010ms step_avg:421.42ms
step:329/800 train_loss:4.6025 train_time:134430ms step_avg:421.41ms
step:330/800 train_loss:4.3653 train_time:134848ms step_avg:421.40ms
step:331/800 train_loss:4.3335 train_time:135266ms step_avg:421.39ms
step:332/800 train_loss:4.5039 train_time:135684ms step_avg:421.38ms
step:333/800 train_loss:4.4190 train_time:136102ms step_avg:421.37ms
step:334/800 train_loss:4.4111 train_time:136521ms step_avg:421.36ms
step:335/800 train_loss:4.3665 train_time:136937ms step_avg:421.35ms
step:336/800 train_loss:4.5496 train_time:137357ms step_avg:421.34ms
step:337/800 train_loss:4.4826 train_time:137776ms step_avg:421.33ms
step:338/800 train_loss:5.0302 train_time:138193ms step_avg:421.32ms
step:339/800 train_loss:4.4587 train_time:138612ms step_avg:421.31ms
step:340/800 train_loss:4.4306 train_time:139031ms step_avg:421.31ms
step:341/800 train_loss:4.4110 train_time:139448ms step_avg:421.29ms
step:342/800 train_loss:4.3459 train_time:139867ms step_avg:421.28ms
step:343/800 train_loss:4.3201 train_time:140285ms step_avg:421.28ms
step:344/800 train_loss:4.3872 train_time:140703ms step_avg:421.27ms
step:345/800 train_loss:4.4864 train_time:141121ms step_avg:421.26ms
step:346/800 train_loss:4.3657 train_time:141539ms step_avg:421.25ms
step:347/800 train_loss:4.3012 train_time:141956ms step_avg:421.23ms
step:348/800 train_loss:4.3468 train_time:142373ms step_avg:421.22ms
step:349/800 train_loss:4.3549 train_time:142793ms step_avg:421.22ms
step:350/800 train_loss:4.2928 train_time:143211ms step_avg:421.21ms
step:351/800 train_loss:3.9843 train_time:143630ms step_avg:421.20ms
step:352/800 train_loss:4.2705 train_time:144048ms step_avg:421.19ms
step:353/800 train_loss:4.6322 train_time:144467ms step_avg:421.19ms
step:354/800 train_loss:4.1461 train_time:144885ms step_avg:421.18ms
step:355/800 train_loss:4.3967 train_time:145304ms step_avg:421.17ms
step:356/800 train_loss:4.2908 train_time:145720ms step_avg:421.16ms
step:357/800 train_loss:4.3862 train_time:146139ms step_avg:421.15ms
step:358/800 train_loss:4.3856 train_time:146557ms step_avg:421.14ms
step:359/800 train_loss:4.3204 train_time:146974ms step_avg:421.13ms
step:360/800 train_loss:4.5472 train_time:147392ms step_avg:421.12ms
step:361/800 train_loss:4.0090 train_time:147811ms step_avg:421.11ms
step:362/800 train_loss:4.5039 train_time:148228ms step_avg:421.10ms
step:363/800 train_loss:4.4101 train_time:148646ms step_avg:421.09ms
step:364/800 train_loss:4.3039 train_time:149064ms step_avg:421.08ms
step:365/800 train_loss:4.2272 train_time:149481ms step_avg:421.07ms
step:366/800 train_loss:4.3970 train_time:149899ms step_avg:421.07ms
step:367/800 train_loss:4.3267 train_time:150319ms step_avg:421.06ms
step:368/800 train_loss:4.3075 train_time:150737ms step_avg:421.05ms
step:369/800 train_loss:4.3070 train_time:151153ms step_avg:421.04ms
step:370/800 train_loss:4.1991 train_time:151572ms step_avg:421.03ms
step:371/800 train_loss:4.3527 train_time:151989ms step_avg:421.02ms
step:372/800 train_loss:4.2666 train_time:152409ms step_avg:421.02ms
step:373/800 train_loss:4.1576 train_time:152827ms step_avg:421.01ms
step:374/800 train_loss:4.3564 train_time:153247ms step_avg:421.01ms
step:375/800 train_loss:4.2910 train_time:153669ms step_avg:421.01ms
step:375/800 val_loss:4.2949 train_time:153682ms step_avg:421.05ms
step:376/800 train_loss:4.2715 train_time:154091ms step_avg:421.01ms
step:377/800 train_loss:4.3322 train_time:154508ms step_avg:421.00ms
step:378/800 train_loss:4.2303 train_time:155035ms step_avg:421.29ms
step:379/800 train_loss:4.2829 train_time:155455ms step_avg:421.29ms
step:380/800 train_loss:4.3452 train_time:155999ms step_avg:421.62ms
step:381/800 train_loss:4.3896 train_time:156416ms step_avg:421.61ms
step:382/800 train_loss:4.3124 train_time:156833ms step_avg:421.59ms
step:383/800 train_loss:4.2932 train_time:157251ms step_avg:421.58ms
step:384/800 train_loss:4.2139 train_time:157668ms step_avg:421.57ms
step:385/800 train_loss:4.3061 train_time:158086ms step_avg:421.56ms
step:386/800 train_loss:4.2244 train_time:158504ms step_avg:421.55ms
step:387/800 train_loss:4.3477 train_time:158923ms step_avg:421.55ms
step:388/800 train_loss:4.5389 train_time:159340ms step_avg:421.54ms
step:389/800 train_loss:4.2440 train_time:159758ms step_avg:421.52ms
step:390/800 train_loss:4.2180 train_time:160175ms step_avg:421.51ms
step:391/800 train_loss:4.3351 train_time:160592ms step_avg:421.50ms
step:392/800 train_loss:4.2438 train_time:161009ms step_avg:421.49ms
step:393/800 train_loss:4.3546 train_time:161428ms step_avg:421.48ms
step:394/800 train_loss:4.1815 train_time:161846ms step_avg:421.48ms
step:395/800 train_loss:4.3211 train_time:162263ms step_avg:421.46ms
step:396/800 train_loss:4.0847 train_time:162685ms step_avg:421.46ms
step:397/800 train_loss:4.2618 train_time:163103ms step_avg:421.45ms
step:398/800 train_loss:4.3359 train_time:163521ms step_avg:421.45ms
step:399/800 train_loss:4.3031 train_time:163939ms step_avg:421.44ms
step:400/800 train_loss:4.2151 train_time:164359ms step_avg:421.43ms
step:401/800 train_loss:4.2734 train_time:164777ms step_avg:421.43ms
step:402/800 train_loss:4.3222 train_time:165193ms step_avg:421.41ms
step:403/800 train_loss:4.2763 train_time:165612ms step_avg:421.41ms
step:404/800 train_loss:4.3726 train_time:166032ms step_avg:421.40ms
step:405/800 train_loss:4.1479 train_time:166450ms step_avg:421.39ms
step:406/800 train_loss:4.2143 train_time:166868ms step_avg:421.38ms
step:407/800 train_loss:4.4917 train_time:167287ms step_avg:421.38ms
step:408/800 train_loss:4.2371 train_time:167702ms step_avg:421.36ms
step:409/800 train_loss:4.2342 train_time:168120ms step_avg:421.35ms
step:410/800 train_loss:4.2914 train_time:168538ms step_avg:421.34ms
step:411/800 train_loss:4.1612 train_time:168955ms step_avg:421.33ms
step:412/800 train_loss:4.1870 train_time:169374ms step_avg:421.33ms
step:413/800 train_loss:4.5972 train_time:169791ms step_avg:421.32ms
step:414/800 train_loss:4.0516 train_time:170208ms step_avg:421.31ms
step:415/800 train_loss:4.4261 train_time:170627ms step_avg:421.30ms
step:416/800 train_loss:4.1839 train_time:171043ms step_avg:421.29ms
step:417/800 train_loss:4.1875 train_time:171461ms step_avg:421.28ms
step:418/800 train_loss:4.3703 train_time:171880ms step_avg:421.27ms
step:419/800 train_loss:4.1012 train_time:172298ms step_avg:421.27ms
step:420/800 train_loss:4.2059 train_time:172716ms step_avg:421.26ms
step:421/800 train_loss:4.1512 train_time:173134ms step_avg:421.25ms
step:422/800 train_loss:4.0543 train_time:173551ms step_avg:421.24ms
step:423/800 train_loss:4.1734 train_time:173969ms step_avg:421.23ms
step:424/800 train_loss:4.2734 train_time:174386ms step_avg:421.22ms
step:425/800 train_loss:4.0568 train_time:174803ms step_avg:421.21ms
step:426/800 train_loss:4.2290 train_time:175222ms step_avg:421.21ms
step:427/800 train_loss:4.1060 train_time:175640ms step_avg:421.20ms
step:428/800 train_loss:4.3107 train_time:176058ms step_avg:421.19ms
step:429/800 train_loss:4.2367 train_time:176475ms step_avg:421.18ms
step:430/800 train_loss:4.1587 train_time:176893ms step_avg:421.17ms
step:431/800 train_loss:4.1322 train_time:177311ms step_avg:421.17ms
step:432/800 train_loss:4.0582 train_time:177728ms step_avg:421.16ms
step:433/800 train_loss:4.1672 train_time:178146ms step_avg:421.15ms
step:434/800 train_loss:4.2342 train_time:178565ms step_avg:421.14ms
step:435/800 train_loss:4.1669 train_time:178984ms step_avg:421.14ms
step:436/800 train_loss:4.2157 train_time:179400ms step_avg:421.13ms
step:437/800 train_loss:4.2271 train_time:179818ms step_avg:421.12ms
step:438/800 train_loss:4.1041 train_time:180235ms step_avg:421.11ms
step:439/800 train_loss:4.1279 train_time:180653ms step_avg:421.10ms
step:440/800 train_loss:4.1028 train_time:181071ms step_avg:421.10ms
step:441/800 train_loss:4.2807 train_time:181488ms step_avg:421.09ms
step:442/800 train_loss:4.1683 train_time:181907ms step_avg:421.08ms
step:443/800 train_loss:4.1605 train_time:182324ms step_avg:421.07ms
step:444/800 train_loss:4.0413 train_time:182741ms step_avg:421.06ms
step:445/800 train_loss:4.3051 train_time:183158ms step_avg:421.05ms
step:446/800 train_loss:4.2289 train_time:183574ms step_avg:421.04ms
step:447/800 train_loss:4.2269 train_time:183992ms step_avg:421.03ms
step:448/800 train_loss:4.1448 train_time:184409ms step_avg:421.03ms
step:449/800 train_loss:4.2418 train_time:184826ms step_avg:421.02ms
step:450/800 train_loss:4.0637 train_time:185244ms step_avg:421.01ms
step:451/800 train_loss:4.0986 train_time:185662ms step_avg:421.00ms
step:452/800 train_loss:3.9831 train_time:186082ms step_avg:421.00ms
step:453/800 train_loss:4.0913 train_time:186499ms step_avg:420.99ms
step:454/800 train_loss:4.0655 train_time:186916ms step_avg:420.98ms
step:455/800 train_loss:4.0312 train_time:187333ms step_avg:420.97ms
step:456/800 train_loss:4.2402 train_time:187752ms step_avg:420.97ms
step:457/800 train_loss:4.1056 train_time:188170ms step_avg:420.96ms
step:458/800 train_loss:4.1855 train_time:188587ms step_avg:420.95ms
step:459/800 train_loss:4.2198 train_time:189004ms step_avg:420.95ms
step:460/800 train_loss:4.0204 train_time:189422ms step_avg:420.94ms
step:461/800 train_loss:4.1912 train_time:189839ms step_avg:420.93ms
step:462/800 train_loss:4.0860 train_time:190256ms step_avg:420.92ms
step:463/800 train_loss:4.0891 train_time:190674ms step_avg:420.91ms
step:464/800 train_loss:4.1705 train_time:191092ms step_avg:420.91ms
step:465/800 train_loss:4.1044 train_time:191511ms step_avg:420.90ms
step:466/800 train_loss:4.1066 train_time:191928ms step_avg:420.89ms
step:467/800 train_loss:4.2166 train_time:192345ms step_avg:420.89ms
step:468/800 train_loss:4.2164 train_time:192763ms step_avg:420.88ms
step:469/800 train_loss:4.1918 train_time:193184ms step_avg:420.88ms
step:470/800 train_loss:4.0826 train_time:193601ms step_avg:420.87ms
step:471/800 train_loss:4.1737 train_time:194018ms step_avg:420.86ms
step:472/800 train_loss:4.2202 train_time:194435ms step_avg:420.85ms
step:473/800 train_loss:4.1474 train_time:194851ms step_avg:420.85ms
step:474/800 train_loss:4.1107 train_time:195268ms step_avg:420.84ms
step:475/800 train_loss:3.9699 train_time:195688ms step_avg:420.83ms
step:476/800 train_loss:4.4096 train_time:196105ms step_avg:420.83ms
step:477/800 train_loss:4.1571 train_time:196523ms step_avg:420.82ms
step:478/800 train_loss:3.9607 train_time:196942ms step_avg:420.82ms
step:479/800 train_loss:4.1787 train_time:197360ms step_avg:420.81ms
step:480/800 train_loss:4.1485 train_time:197777ms step_avg:420.80ms
step:481/800 train_loss:4.2884 train_time:198197ms step_avg:420.80ms
step:482/800 train_loss:4.0966 train_time:198615ms step_avg:420.79ms
step:483/800 train_loss:3.9089 train_time:199058ms step_avg:420.84ms
step:484/800 train_loss:4.1931 train_time:199480ms step_avg:420.84ms
step:485/800 train_loss:4.0405 train_time:199903ms step_avg:420.85ms
step:486/800 train_loss:4.0530 train_time:200325ms step_avg:420.85ms
step:487/800 train_loss:3.9907 train_time:200748ms step_avg:420.85ms
step:488/800 train_loss:4.0454 train_time:201170ms step_avg:420.86ms
step:489/800 train_loss:4.2419 train_time:201597ms step_avg:420.87ms
step:490/800 train_loss:4.0928 train_time:202020ms step_avg:420.87ms
step:491/800 train_loss:3.9855 train_time:202441ms step_avg:420.88ms
step:492/800 train_loss:3.9888 train_time:202864ms step_avg:420.88ms
step:493/800 train_loss:4.1120 train_time:203287ms step_avg:420.88ms
step:494/800 train_loss:3.9569 train_time:203710ms step_avg:420.89ms
step:495/800 train_loss:4.0965 train_time:204133ms step_avg:420.89ms
step:496/800 train_loss:4.0251 train_time:204556ms step_avg:420.90ms
step:497/800 train_loss:3.9225 train_time:204962ms step_avg:420.87ms
step:498/800 train_loss:4.1079 train_time:205381ms step_avg:420.86ms
step:499/800 train_loss:4.1852 train_time:205799ms step_avg:420.86ms
step:500/800 train_loss:4.2206 train_time:206216ms step_avg:420.85ms
step:500/800 val_loss:4.0845 train_time:206230ms step_avg:420.88ms
step:501/800 train_loss:4.1138 train_time:206638ms step_avg:420.85ms
step:502/800 train_loss:4.1662 train_time:207061ms step_avg:420.86ms
step:503/800 train_loss:4.1108 train_time:207478ms step_avg:420.85ms
step:504/800 train_loss:4.1548 train_time:207896ms step_avg:420.84ms
step:505/800 train_loss:4.1075 train_time:208314ms step_avg:420.84ms
step:506/800 train_loss:4.2076 train_time:208732ms step_avg:420.83ms
step:507/800 train_loss:3.9930 train_time:209150ms step_avg:420.83ms
step:508/800 train_loss:4.1364 train_time:209569ms step_avg:420.82ms
step:509/800 train_loss:4.2157 train_time:209984ms step_avg:420.81ms
step:510/800 train_loss:4.1457 train_time:210402ms step_avg:420.80ms
step:511/800 train_loss:3.9571 train_time:210820ms step_avg:420.80ms
step:512/800 train_loss:4.1581 train_time:211235ms step_avg:420.79ms
step:513/800 train_loss:4.0908 train_time:211654ms step_avg:420.78ms
step:514/800 train_loss:4.0581 train_time:212073ms step_avg:420.78ms
step:515/800 train_loss:4.1180 train_time:212491ms step_avg:420.77ms
step:516/800 train_loss:4.1245 train_time:212909ms step_avg:420.77ms
step:517/800 train_loss:4.4485 train_time:213327ms step_avg:420.76ms
step:518/800 train_loss:4.0380 train_time:213743ms step_avg:420.75ms
step:519/800 train_loss:4.1636 train_time:214162ms step_avg:420.75ms
step:520/800 train_loss:4.0796 train_time:214581ms step_avg:420.75ms
step:521/800 train_loss:4.0626 train_time:214999ms step_avg:420.74ms
step:522/800 train_loss:3.9959 train_time:215417ms step_avg:420.74ms
step:523/800 train_loss:4.0182 train_time:215833ms step_avg:420.73ms
step:524/800 train_loss:4.6397 train_time:216250ms step_avg:420.72ms
step:525/800 train_loss:4.1182 train_time:216667ms step_avg:420.71ms
step:526/800 train_loss:4.0611 train_time:217085ms step_avg:420.71ms
step:527/800 train_loss:4.0649 train_time:217503ms step_avg:420.70ms
step:528/800 train_loss:4.0130 train_time:217920ms step_avg:420.69ms
step:529/800 train_loss:3.9887 train_time:218338ms step_avg:420.69ms
step:530/800 train_loss:4.1936 train_time:218759ms step_avg:420.69ms
step:531/800 train_loss:4.0108 train_time:219177ms step_avg:420.69ms
step:532/800 train_loss:4.2828 train_time:219595ms step_avg:420.68ms
step:533/800 train_loss:4.0929 train_time:220014ms step_avg:420.68ms
step:534/800 train_loss:4.0218 train_time:220430ms step_avg:420.67ms
step:535/800 train_loss:4.0480 train_time:220848ms step_avg:420.66ms
step:536/800 train_loss:3.9790 train_time:221266ms step_avg:420.66ms
step:537/800 train_loss:4.0924 train_time:221684ms step_avg:420.65ms
step:538/800 train_loss:4.0930 train_time:222100ms step_avg:420.64ms
step:539/800 train_loss:4.0031 train_time:222518ms step_avg:420.64ms
step:540/800 train_loss:4.4925 train_time:222934ms step_avg:420.63ms
step:541/800 train_loss:4.0284 train_time:223352ms step_avg:420.63ms
step:542/800 train_loss:4.1418 train_time:223770ms step_avg:420.62ms
step:543/800 train_loss:3.9766 train_time:224187ms step_avg:420.61ms
step:544/800 train_loss:3.9609 train_time:224606ms step_avg:420.61ms
step:545/800 train_loss:4.0473 train_time:225024ms step_avg:420.60ms
step:546/800 train_loss:3.9646 train_time:225441ms step_avg:420.60ms
step:547/800 train_loss:4.0111 train_time:225860ms step_avg:420.60ms
step:548/800 train_loss:4.0138 train_time:226279ms step_avg:420.59ms
step:549/800 train_loss:3.9943 train_time:226697ms step_avg:420.59ms
step:550/800 train_loss:4.0832 train_time:227115ms step_avg:420.58ms
step:551/800 train_loss:3.9556 train_time:227534ms step_avg:420.58ms
step:552/800 train_loss:3.9846 train_time:227953ms step_avg:420.58ms
step:553/800 train_loss:4.3155 train_time:228369ms step_avg:420.57ms
step:554/800 train_loss:4.1026 train_time:228787ms step_avg:420.56ms
step:555/800 train_loss:4.0738 train_time:229204ms step_avg:420.56ms
step:556/800 train_loss:4.0365 train_time:229621ms step_avg:420.55ms
step:557/800 train_loss:4.0501 train_time:230039ms step_avg:420.55ms
step:558/800 train_loss:3.7215 train_time:230460ms step_avg:420.55ms
step:559/800 train_loss:3.9651 train_time:230878ms step_avg:420.54ms
step:560/800 train_loss:4.0103 train_time:231295ms step_avg:420.54ms
step:561/800 train_loss:4.0491 train_time:231713ms step_avg:420.53ms
step:562/800 train_loss:3.9610 train_time:232130ms step_avg:420.53ms
step:563/800 train_loss:3.9168 train_time:232548ms step_avg:420.52ms
step:564/800 train_loss:4.1118 train_time:232966ms step_avg:420.52ms
step:565/800 train_loss:3.9224 train_time:233384ms step_avg:420.51ms
step:566/800 train_loss:4.0495 train_time:233800ms step_avg:420.50ms
step:567/800 train_loss:3.9985 train_time:234846ms step_avg:421.63ms
step:568/800 train_loss:3.9517 train_time:235265ms step_avg:421.62ms
step:569/800 train_loss:4.0421 train_time:235681ms step_avg:421.61ms
step:570/800 train_loss:4.0115 train_time:236239ms step_avg:421.86ms
step:571/800 train_loss:4.0361 train_time:236659ms step_avg:421.85ms
step:572/800 train_loss:4.1334 train_time:237077ms step_avg:421.85ms
step:573/800 train_loss:4.0522 train_time:237494ms step_avg:421.84ms
step:574/800 train_loss:4.0658 train_time:237912ms step_avg:421.83ms
step:575/800 train_loss:4.1280 train_time:238328ms step_avg:421.82ms
step:576/800 train_loss:4.0867 train_time:238746ms step_avg:421.81ms
step:577/800 train_loss:4.0960 train_time:239164ms step_avg:421.81ms
step:578/800 train_loss:4.0456 train_time:239582ms step_avg:421.80ms
step:579/800 train_loss:4.0117 train_time:240000ms step_avg:421.79ms
step:580/800 train_loss:4.0133 train_time:240419ms step_avg:421.79ms
step:581/800 train_loss:3.9611 train_time:240836ms step_avg:421.78ms
step:582/800 train_loss:3.9860 train_time:241255ms step_avg:421.78ms
step:583/800 train_loss:4.2117 train_time:241673ms step_avg:421.77ms
step:584/800 train_loss:3.9805 train_time:242089ms step_avg:421.76ms
step:585/800 train_loss:3.9396 train_time:242507ms step_avg:421.75ms
step:586/800 train_loss:4.1208 train_time:242926ms step_avg:421.75ms
step:587/800 train_loss:3.8837 train_time:243344ms step_avg:421.74ms
step:588/800 train_loss:4.0163 train_time:243763ms step_avg:421.73ms
step:589/800 train_loss:4.0191 train_time:244180ms step_avg:421.73ms
step:590/800 train_loss:4.3589 train_time:244596ms step_avg:421.72ms
step:591/800 train_loss:4.1361 train_time:245014ms step_avg:421.71ms
step:592/800 train_loss:3.8751 train_time:245432ms step_avg:421.71ms
step:593/800 train_loss:3.8886 train_time:245849ms step_avg:421.70ms
step:594/800 train_loss:3.8847 train_time:246267ms step_avg:421.69ms
step:595/800 train_loss:3.9220 train_time:246685ms step_avg:421.68ms
step:596/800 train_loss:4.2870 train_time:247105ms step_avg:421.68ms
step:597/800 train_loss:4.0020 train_time:247524ms step_avg:421.68ms
step:598/800 train_loss:3.9416 train_time:247941ms step_avg:421.67ms
step:599/800 train_loss:4.0049 train_time:248361ms step_avg:421.67ms
step:600/800 train_loss:3.8306 train_time:248778ms step_avg:421.66ms
step:601/800 train_loss:3.9525 train_time:249196ms step_avg:421.65ms
step:602/800 train_loss:3.9797 train_time:249613ms step_avg:421.64ms
step:603/800 train_loss:3.9917 train_time:250032ms step_avg:421.64ms
step:604/800 train_loss:4.1275 train_time:250448ms step_avg:421.63ms
step:605/800 train_loss:3.9981 train_time:250866ms step_avg:421.62ms
step:606/800 train_loss:3.9691 train_time:251284ms step_avg:421.62ms
step:607/800 train_loss:3.8932 train_time:251701ms step_avg:421.61ms
step:608/800 train_loss:4.1464 train_time:252118ms step_avg:421.60ms
step:609/800 train_loss:3.9854 train_time:252534ms step_avg:421.59ms
step:610/800 train_loss:3.9644 train_time:252953ms step_avg:421.59ms
step:611/800 train_loss:4.0726 train_time:253370ms step_avg:421.58ms
step:612/800 train_loss:3.9739 train_time:253786ms step_avg:421.57ms
step:613/800 train_loss:3.9402 train_time:254204ms step_avg:421.57ms
step:614/800 train_loss:4.1193 train_time:254622ms step_avg:421.56ms
step:615/800 train_loss:4.0825 train_time:255039ms step_avg:421.55ms
step:616/800 train_loss:4.0456 train_time:255460ms step_avg:421.55ms
step:617/800 train_loss:3.9590 train_time:255878ms step_avg:421.54ms
step:618/800 train_loss:3.9167 train_time:256295ms step_avg:421.54ms
step:619/800 train_loss:4.0202 train_time:256712ms step_avg:421.53ms
step:620/800 train_loss:3.9287 train_time:257130ms step_avg:421.52ms
step:621/800 train_loss:3.9390 train_time:257548ms step_avg:421.52ms
step:622/800 train_loss:4.2321 train_time:257968ms step_avg:421.52ms
step:623/800 train_loss:3.9411 train_time:258387ms step_avg:421.51ms
step:624/800 train_loss:3.9753 train_time:258803ms step_avg:421.50ms
step:625/800 train_loss:4.0489 train_time:259220ms step_avg:421.50ms
step:625/800 val_loss:3.9763 train_time:259234ms step_avg:421.52ms
step:626/800 train_loss:4.0793 train_time:259639ms step_avg:421.49ms
step:627/800 train_loss:4.0974 train_time:260057ms step_avg:421.49ms
step:628/800 train_loss:4.0759 train_time:260473ms step_avg:421.48ms
step:629/800 train_loss:4.1250 train_time:260891ms step_avg:421.47ms
step:630/800 train_loss:3.9347 train_time:261308ms step_avg:421.47ms
step:631/800 train_loss:4.0689 train_time:261726ms step_avg:421.46ms
step:632/800 train_loss:4.1074 train_time:262143ms step_avg:421.45ms
step:633/800 train_loss:4.0056 train_time:262560ms step_avg:421.44ms
step:634/800 train_loss:3.9280 train_time:262978ms step_avg:421.44ms
step:635/800 train_loss:4.0281 train_time:263395ms step_avg:421.43ms
step:636/800 train_loss:4.2812 train_time:263812ms step_avg:421.42ms
step:637/800 train_loss:3.8724 train_time:264229ms step_avg:421.42ms
step:638/800 train_loss:3.6954 train_time:264646ms step_avg:421.41ms
step:639/800 train_loss:3.9313 train_time:265066ms step_avg:421.41ms
step:640/800 train_loss:3.9604 train_time:265483ms step_avg:421.40ms
step:641/800 train_loss:3.9241 train_time:265899ms step_avg:421.39ms
step:642/800 train_loss:3.9258 train_time:266318ms step_avg:421.39ms
step:643/800 train_loss:3.9689 train_time:266736ms step_avg:421.38ms
step:644/800 train_loss:3.9915 train_time:267153ms step_avg:421.38ms
step:645/800 train_loss:3.9065 train_time:267571ms step_avg:421.37ms
step:646/800 train_loss:4.1287 train_time:267988ms step_avg:421.37ms
step:647/800 train_loss:4.0115 train_time:268406ms step_avg:421.36ms
step:648/800 train_loss:4.0148 train_time:268825ms step_avg:421.36ms
step:649/800 train_loss:4.0319 train_time:269243ms step_avg:421.35ms
step:650/800 train_loss:4.1017 train_time:269660ms step_avg:421.34ms
step:651/800 train_loss:3.9638 train_time:270077ms step_avg:421.34ms
step:652/800 train_loss:4.1024 train_time:270494ms step_avg:421.33ms
step:653/800 train_loss:3.9300 train_time:270912ms step_avg:421.33ms
step:654/800 train_loss:4.0069 train_time:271329ms step_avg:421.32ms
step:655/800 train_loss:3.7759 train_time:271746ms step_avg:421.31ms
step:656/800 train_loss:3.9223 train_time:272164ms step_avg:421.31ms
step:657/800 train_loss:3.9284 train_time:272582ms step_avg:421.30ms
step:658/800 train_loss:3.8628 train_time:272999ms step_avg:421.29ms
step:659/800 train_loss:4.0428 train_time:273416ms step_avg:421.29ms
step:660/800 train_loss:3.9352 train_time:273838ms step_avg:421.29ms
step:661/800 train_loss:4.0196 train_time:274255ms step_avg:421.28ms
step:662/800 train_loss:4.0970 train_time:274673ms step_avg:421.28ms
step:663/800 train_loss:4.0069 train_time:275091ms step_avg:421.27ms
step:664/800 train_loss:3.8922 train_time:275508ms step_avg:421.27ms
step:665/800 train_loss:3.9760 train_time:275925ms step_avg:421.26ms
step:666/800 train_loss:3.8365 train_time:276344ms step_avg:421.26ms
step:667/800 train_loss:4.1368 train_time:276760ms step_avg:421.25ms
step:668/800 train_loss:3.9745 train_time:277179ms step_avg:421.24ms
step:669/800 train_loss:3.9654 train_time:277596ms step_avg:421.24ms
step:670/800 train_loss:3.8248 train_time:278013ms step_avg:421.23ms
step:671/800 train_loss:3.9328 train_time:278433ms step_avg:421.23ms
step:672/800 train_loss:3.8974 train_time:278851ms step_avg:421.23ms
step:673/800 train_loss:3.9252 train_time:279267ms step_avg:421.22ms
step:674/800 train_loss:4.2058 train_time:279685ms step_avg:421.21ms
step:675/800 train_loss:3.9949 train_time:280102ms step_avg:421.21ms
step:676/800 train_loss:4.0579 train_time:280519ms step_avg:421.20ms
step:677/800 train_loss:3.8328 train_time:280937ms step_avg:421.19ms
step:678/800 train_loss:3.9338 train_time:281354ms step_avg:421.19ms
step:679/800 train_loss:3.8818 train_time:281773ms step_avg:421.19ms
step:680/800 train_loss:4.0265 train_time:282190ms step_avg:421.18ms
step:681/800 train_loss:3.9353 train_time:282609ms step_avg:421.18ms
step:682/800 train_loss:3.9573 train_time:283026ms step_avg:421.17ms
step:683/800 train_loss:4.0314 train_time:283443ms step_avg:421.16ms
step:684/800 train_loss:4.0792 train_time:283861ms step_avg:421.16ms
step:685/800 train_loss:3.9748 train_time:284278ms step_avg:421.15ms
step:686/800 train_loss:4.0536 train_time:284696ms step_avg:421.15ms
step:687/800 train_loss:3.9778 train_time:285114ms step_avg:421.14ms
step:688/800 train_loss:4.0247 train_time:285536ms step_avg:421.14ms
step:689/800 train_loss:3.6429 train_time:285954ms step_avg:421.14ms
step:690/800 train_loss:3.7635 train_time:286370ms step_avg:421.13ms
step:691/800 train_loss:3.9032 train_time:286789ms step_avg:421.13ms
step:692/800 train_loss:3.7799 train_time:287206ms step_avg:421.12ms
step:693/800 train_loss:4.0020 train_time:287624ms step_avg:421.12ms
step:694/800 train_loss:4.0125 train_time:288042ms step_avg:421.11ms
step:695/800 train_loss:3.9020 train_time:288459ms step_avg:421.11ms
step:696/800 train_loss:3.8872 train_time:288876ms step_avg:421.10ms
step:697/800 train_loss:4.1870 train_time:289293ms step_avg:421.10ms
step:698/800 train_loss:3.9584 train_time:289710ms step_avg:421.09ms
step:699/800 train_loss:3.9825 train_time:290129ms step_avg:421.09ms
step:700/800 train_loss:4.1518 train_time:290545ms step_avg:421.08ms
step:701/800 train_loss:3.9193 train_time:291006ms step_avg:421.14ms
step:702/800 train_loss:3.8718 train_time:291423ms step_avg:421.13ms
step:703/800 train_loss:3.8662 train_time:291841ms step_avg:421.13ms
step:704/800 train_loss:3.8152 train_time:292259ms step_avg:421.12ms
step:705/800 train_loss:3.9115 train_time:292677ms step_avg:421.12ms
step:706/800 train_loss:3.9004 train_time:293094ms step_avg:421.11ms
step:707/800 train_loss:3.9219 train_time:293514ms step_avg:421.11ms
step:708/800 train_loss:3.9895 train_time:293937ms step_avg:421.11ms
step:709/800 train_loss:3.9309 train_time:294354ms step_avg:421.11ms
step:710/800 train_loss:3.9120 train_time:294772ms step_avg:421.10ms
step:711/800 train_loss:3.8916 train_time:295189ms step_avg:421.10ms
step:712/800 train_loss:3.9358 train_time:295606ms step_avg:421.09ms
step:713/800 train_loss:3.9924 train_time:296023ms step_avg:421.09ms
step:714/800 train_loss:4.0013 train_time:296442ms step_avg:421.08ms
step:715/800 train_loss:3.9079 train_time:296859ms step_avg:421.08ms
step:716/800 train_loss:3.9163 train_time:297277ms step_avg:421.07ms
step:717/800 train_loss:3.9336 train_time:297695ms step_avg:421.07ms
step:718/800 train_loss:4.0702 train_time:298112ms step_avg:421.06ms
step:719/800 train_loss:3.9382 train_time:298529ms step_avg:421.06ms
step:720/800 train_loss:4.0108 train_time:298947ms step_avg:421.05ms
step:721/800 train_loss:4.1773 train_time:299366ms step_avg:421.05ms
step:722/800 train_loss:3.8123 train_time:299783ms step_avg:421.04ms
step:723/800 train_loss:4.0599 train_time:300200ms step_avg:421.04ms
step:724/800 train_loss:4.1329 train_time:300618ms step_avg:421.03ms
step:725/800 train_loss:3.8960 train_time:301039ms step_avg:421.03ms
step:726/800 train_loss:3.9903 train_time:301455ms step_avg:421.03ms
step:727/800 train_loss:3.8940 train_time:301872ms step_avg:421.02ms
step:728/800 train_loss:3.8922 train_time:302290ms step_avg:421.02ms
step:729/800 train_loss:4.0742 train_time:302707ms step_avg:421.01ms
step:730/800 train_loss:4.0272 train_time:303125ms step_avg:421.01ms
step:731/800 train_loss:4.0318 train_time:303544ms step_avg:421.00ms
step:732/800 train_loss:3.9132 train_time:303962ms step_avg:421.00ms
step:733/800 train_loss:3.9350 train_time:304379ms step_avg:421.00ms
step:734/800 train_loss:4.1712 train_time:304798ms step_avg:420.99ms
step:735/800 train_loss:3.8833 train_time:305216ms step_avg:420.99ms
step:736/800 train_loss:3.9700 train_time:305638ms step_avg:420.99ms
step:737/800 train_loss:4.0917 train_time:306056ms step_avg:420.98ms
step:738/800 train_loss:3.9927 train_time:306474ms step_avg:420.98ms
step:739/800 train_loss:3.9474 train_time:306893ms step_avg:420.98ms
step:740/800 train_loss:3.8467 train_time:307311ms step_avg:420.97ms
step:741/800 train_loss:4.5036 train_time:307730ms step_avg:420.97ms
step:742/800 train_loss:3.8549 train_time:308148ms step_avg:420.97ms
step:743/800 train_loss:3.9303 train_time:308566ms step_avg:420.96ms
step:744/800 train_loss:3.9248 train_time:308983ms step_avg:420.96ms
step:745/800 train_loss:3.9815 train_time:309402ms step_avg:420.95ms
step:746/800 train_loss:3.9629 train_time:309820ms step_avg:420.95ms
step:747/800 train_loss:3.9480 train_time:310237ms step_avg:420.95ms
step:748/800 train_loss:3.9784 train_time:310656ms step_avg:420.94ms
step:749/800 train_loss:3.8993 train_time:311074ms step_avg:420.94ms
step:750/800 train_loss:3.9162 train_time:311492ms step_avg:420.93ms
step:750/800 val_loss:3.9187 train_time:311505ms step_avg:420.95ms
step:751/800 train_loss:3.9550 train_time:311913ms step_avg:420.94ms
step:752/800 train_loss:3.9024 train_time:312331ms step_avg:420.93ms
step:753/800 train_loss:3.9421 train_time:312750ms step_avg:420.93ms
step:754/800 train_loss:3.9651 train_time:313167ms step_avg:420.92ms
step:755/800 train_loss:3.9313 train_time:313585ms step_avg:420.92ms
step:756/800 train_loss:4.0150 train_time:314588ms step_avg:421.70ms
step:757/800 train_loss:3.8503 train_time:315008ms step_avg:421.70ms
step:758/800 train_loss:4.0749 train_time:315425ms step_avg:421.69ms
step:759/800 train_loss:3.9835 train_time:315843ms step_avg:421.69ms
step:760/800 train_loss:3.9158 train_time:316393ms step_avg:421.86ms
step:761/800 train_loss:4.0221 train_time:316809ms step_avg:421.85ms
step:762/800 train_loss:3.7492 train_time:317225ms step_avg:421.84ms
step:763/800 train_loss:3.9128 train_time:317643ms step_avg:421.84ms
step:764/800 train_loss:4.0144 train_time:318059ms step_avg:421.83ms
step:765/800 train_loss:3.6600 train_time:318478ms step_avg:421.82ms
step:766/800 train_loss:4.1035 train_time:318895ms step_avg:421.82ms
step:767/800 train_loss:3.9452 train_time:319312ms step_avg:421.81ms
step:768/800 train_loss:3.8950 train_time:319729ms step_avg:421.81ms
step:769/800 train_loss:3.9308 train_time:320148ms step_avg:421.80ms
step:770/800 train_loss:3.9486 train_time:320565ms step_avg:421.80ms
step:771/800 train_loss:4.0094 train_time:320983ms step_avg:421.79ms
step:772/800 train_loss:4.2281 train_time:321401ms step_avg:421.79ms
step:773/800 train_loss:3.8022 train_time:321820ms step_avg:421.78ms
step:774/800 train_loss:4.0084 train_time:322238ms step_avg:421.78ms
step:775/800 train_loss:3.9906 train_time:322654ms step_avg:421.77ms
step:776/800 train_loss:3.9463 train_time:323070ms step_avg:421.76ms
step:777/800 train_loss:3.7603 train_time:323487ms step_avg:421.76ms
step:778/800 train_loss:3.7544 train_time:323905ms step_avg:421.75ms
step:779/800 train_loss:3.8248 train_time:324322ms step_avg:421.74ms
step:780/800 train_loss:3.9098 train_time:324740ms step_avg:421.74ms
step:781/800 train_loss:3.9525 train_time:325158ms step_avg:421.74ms
step:782/800 train_loss:4.0085 train_time:325575ms step_avg:421.73ms
step:783/800 train_loss:3.9054 train_time:325993ms step_avg:421.72ms
step:784/800 train_loss:3.9332 train_time:326410ms step_avg:421.72ms
step:785/800 train_loss:3.9185 train_time:326827ms step_avg:421.71ms
step:786/800 train_loss:3.9063 train_time:327247ms step_avg:421.71ms
step:787/800 train_loss:3.8162 train_time:327664ms step_avg:421.70ms
step:788/800 train_loss:4.0707 train_time:328082ms step_avg:421.70ms
step:789/800 train_loss:3.8560 train_time:328499ms step_avg:421.69ms
step:790/800 train_loss:3.9233 train_time:328920ms step_avg:421.69ms
step:791/800 train_loss:3.9784 train_time:329338ms step_avg:421.69ms
step:792/800 train_loss:4.1139 train_time:329756ms step_avg:421.68ms
step:793/800 train_loss:4.1201 train_time:330173ms step_avg:421.68ms
step:794/800 train_loss:3.8593 train_time:330590ms step_avg:421.67ms
step:795/800 train_loss:3.9578 train_time:331008ms step_avg:421.67ms
step:796/800 train_loss:4.0022 train_time:331426ms step_avg:421.66ms
step:797/800 train_loss:4.1117 train_time:331843ms step_avg:421.66ms
step:798/800 train_loss:3.8726 train_time:332261ms step_avg:421.65ms
step:799/800 train_loss:4.0207 train_time:332680ms step_avg:421.65ms
step:800/800 train_loss:3.9270 train_time:333097ms step_avg:421.64ms
step:800/800 val_loss:3.9104 train_time:333115ms step_avg:421.66ms
